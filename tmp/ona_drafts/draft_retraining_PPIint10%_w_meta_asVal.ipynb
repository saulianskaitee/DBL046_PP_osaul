{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f838074-e829-40d5-9b15-949a9bfe9708",
   "metadata": {},
   "outputs": [],
   "source": [
    "import uuid, sys, os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import ast\n",
    "import math\n",
    "import random\n",
    "\n",
    "from sklearn import metrics\n",
    "from scipy import stats\n",
    "from collections import Counter\n",
    "\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"1\"\n",
    "os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"expandable_segments:True\"\n",
    "import torch\n",
    "torch.cuda.set_device(0)  # 0 == \"first visible\" -> actually GPU 2 on the node\n",
    "print(torch.cuda.get_device_name(0))\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader, TensorDataset, WeightedRandomSampler\n",
    "import pytorch_lightning as pl\n",
    "from torch.optim import AdamW\n",
    "\n",
    "torch.manual_seed(0)\n",
    "\n",
    "from accelerate import Accelerator\n",
    "torch.cuda.empty_cache()\n",
    "import training_utils.partitioning_utils as pat_utils\n",
    "from tqdm import trange"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2318b4f4-87e8-46d5-9a0b-88e789db1aa9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "requests.get(\"https://api.wandb.ai/status\").status_code\n",
    "\n",
    "import wandb\n",
    "wandb.login(key=\"f8a6d759fe657b095d56bddbdb4d586dfaebd468\", relogin=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a771b897-1647-4594-b648-41d2639d5282",
   "metadata": {},
   "outputs": [],
   "source": [
    "SEED = 0\n",
    "os.environ[\"PYTHONHASHSEED\"] = str(SEED)\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "torch.cuda.manual_seed_all(SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12ceffc5-ad46-4e77-aee2-49d4743bd362",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir(\"/zhome/c9/0/203261/DBL046_PP_osaul/DBL046_PP_osaul/tmp/ona_drafts\")\n",
    "# print(os.getcwd())\n",
    "\n",
    "print(\"PyTorch:\", torch.__version__)\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print('Using device:', device)\n",
    "print(\"Current location:\", os.getcwd())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcfc6c69-e492-4bf1-a23e-e8ee41a27d4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model parameters\n",
    "memory_verbose = False\n",
    "use_wandb = True # Used to track loss in real-time without printing\n",
    "model_save_steps = 1\n",
    "train_frac = 1.0\n",
    "test_frac = 1.0\n",
    "\n",
    "embedding_dimension = 1280 #| 960 | 1152\n",
    "number_of_recycles = 2\n",
    "padding_value = -5000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b152a01f-a72c-4d75-9e1c-6b6f937515eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Output path\n",
    "trained_model_dir = \"/work3/s232958/data/PPint_retrain10%_0.4_Christian/251115\"\n",
    "\n",
    "## Embeddings paths\n",
    "binders_embeddings = \"/work3/s232958/data/PPint_DB/binders_embeddings_esm2\"\n",
    "targets_embeddings = \"/work3/s232958/data/PPint_DB/targets_embeddings_esm2\"\n",
    "\n",
    "# ## Training variables\n",
    "runID = uuid.uuid4()\n",
    "\n",
    "def print_mem_consumption():\n",
    "    # 1. Total memory available on the GPU (device 0)\n",
    "    t = torch.cuda.get_device_properties(0).total_memory\n",
    "    # 2. How much memory PyTorch has *reserved* from CUDA\n",
    "    r = torch.cuda.memory_reserved(0)\n",
    "    # 3. How much of that reserved memory is actually *used* by tensors\n",
    "    a = torch.cuda.memory_allocated(0)\n",
    "    # 4. Reserved but not currently allocated (so “free inside PyTorch’s pool”)\n",
    "    f = r - a\n",
    "\n",
    "    print(\"Total memory: \", t/1e9)      # total VRAM in GB\n",
    "    print(\"Reserved memory: \", r/1e9)   # PyTorch’s reserved pool in GB\n",
    "    print(\"Allocated memory: \", a//1e9) # actually in use (integer division)\n",
    "    print(\"Free memory: \", f/1e9)       # slack in the reserved pool in GB\n",
    "print_mem_consumption()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6a54147-26d3-4e49-b87a-b05209b00a7d",
   "metadata": {},
   "source": [
    "### Loading PPint dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2815ec4-5467-4b95-8d4b-84a3efb3e93b",
   "metadata": {},
   "outputs": [],
   "source": [
    "path_to_mmseqs_clustering = \"/work3/s232958/data/PPint_DB/3_å_dataset5_singlefasta/clusterRes40\"\n",
    "all_seqs, clust, clust_keys = pat_utils.mmseqs_parser(path_to_mmseqs_clustering)\n",
    "\n",
    "path_to_interaction_df = \"/work3/s232958/data/PPint_DB/disordered_interfaces_no_cutoff_filtered_nonredundant80_3å_5.csv.gz\"\n",
    "disordered_interfaces_df = pd.read_csv(path_to_interaction_df,index_col=0).reset_index(drop=True)\n",
    "disordered_interfaces_df[\"PDB_chain_name\"] = (disordered_interfaces_df[\"PDB\"] + \"_\" + disordered_interfaces_df[\"chainname\"]).tolist()\n",
    "disordered_interfaces_df[\"index_num\"] = np.arange(len(disordered_interfaces_df))\n",
    "disordered_interfaces_df[\"chain_name_index\"] = [row[\"PDB_chain_name\"] + \"_\" + str(row[\"index_num\"]) for index, row in disordered_interfaces_df.iterrows()]\n",
    "disordered_interfaces_df = disordered_interfaces_df.set_index(\"PDB_interface_name\")\n",
    "disordered_interfaces_df[\"interface_residues\"] = disordered_interfaces_df[\"interface_residues\"].apply(lambda x: ast.literal_eval(x))\n",
    "# disordered_interfaces_df[\"inter_chain_hamming\"] = [1 - (Ldistance(seq.split(\"-\")[0], seq.split(\"-\")[1]))/np.max([len(seq.split(\"-\")[0]), len(seq.split(\"-\")[1])]) for seq in disordered_interfaces_df[\"protien_interface_sequences\"]]\n",
    "disordered_interfaces_df[\"dimer\"] = disordered_interfaces_df[\"inter_chain_hamming\"] > 0.60\n",
    "disordered_interfaces_df[\"clust_keys\"] = [clust_keys.get(row[\"chain_name_index\"]) for index, row in disordered_interfaces_df.iterrows()] \n",
    "\n",
    "pdb_interface_and_clust_keys = {index:disordered_interfaces_df.loc[index,\"clust_keys\"].values.tolist() for index in tqdm(disordered_interfaces_df.index.drop_duplicates(), total=len(disordered_interfaces_df)/2)}\n",
    "new_clusters, new_clusters_clustkeys = pat_utils.recluster_mmseqs_keys_to_non_overlapping_groups(pdb_interface_and_clust_keys)\n",
    "\n",
    "### Creating train and test datasets based on train and test-idexes\n",
    "train_indexes, test_indexes = pat_utils.run_train_test_partition(interaction_df=disordered_interfaces_df,\n",
    "                                                    clustering=new_clusters, # Clusters from Bidentate-graphs\n",
    "                                                    train_ratio=0.8, \n",
    "                                                    test_ratio=0.2, \n",
    "                                                    v=True, \n",
    "                                                    seed=0)\n",
    "\n",
    "disordered_interfaces_df[\"ID\"] = [row[\"PDB\"]+\"_\"+str(row[\"interface_index\"])+\"_\"+row[\"chainname\"] for __, row in disordered_interfaces_df.iterrows()]\n",
    "disordered_interfaces_df[\"PDB_interface_name\"] = disordered_interfaces_df.index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "836e72c5-0b99-4bd0-9b5a-5573c217d0e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "grouped = {}\n",
    "for _, row in disordered_interfaces_df.iterrows():\n",
    "    iface = row[\"PDB_interface_name\"]\n",
    "    seq = row[\"sequence\"]\n",
    "    rid = row[\"ID\"]\n",
    "    dimer = row[\"dimer\"]\n",
    "    \n",
    "    if iface not in grouped:\n",
    "        grouped[iface] = {\n",
    "            \"sequences\": [],\n",
    "            \"IDs\": [],\n",
    "            \"dimer\": dimer,        # keep the dimer value for this interface\n",
    "        }\n",
    "    else:\n",
    "        # Optional: sanity-check it's consistent per interface\n",
    "        if grouped[iface][\"dimer\"] != dimer:\n",
    "            print(f\"Warning: multiple dimers for interface {iface}:\",\n",
    "                  grouped[iface]['dimer'], \"vs\", dimer)\n",
    "\n",
    "    grouped[iface][\"sequences\"].append(seq)\n",
    "    grouped[iface][\"IDs\"].append(rid)\n",
    "\n",
    "records = []\n",
    "for iface, vals in grouped.items():\n",
    "    seqs = vals[\"sequences\"]\n",
    "    ids = vals[\"IDs\"]\n",
    "    if len(seqs) >= 2 and len(ids) >= 2:\n",
    "        records.append({\n",
    "            \"interface_id\": iface,\n",
    "            \"seq1\": seqs[0],\n",
    "            \"seq2\": seqs[1],\n",
    "            \"ID1\": ids[0],\n",
    "            \"ID2\": ids[1],\n",
    "            \"dimer\": vals[\"dimer\"],   # <- add dimer to final record\n",
    "        })\n",
    "\n",
    "PPint_interactions_NEW = pd.DataFrame(records)\n",
    "PPint_interactions_NEW[\"seq_target_len\"] = [len(row.seq1) for __, row in PPint_interactions_NEW.iterrows()]\n",
    "PPint_interactions_NEW[\"seq_binder_len\"] = [len(row.seq2) for __, row in PPint_interactions_NEW.iterrows()]\n",
    "PPint_interactions_NEW[\"target_binder_id\"] = PPint_interactions_NEW[\"ID1\"] + \"_\" + PPint_interactions_NEW[\"ID2\"]\n",
    "\n",
    "PPint_interactions_NEW.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fa4409c-0d65-455b-9db0-d67f9c205413",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sample random 10%\n",
    "random.seed(0)\n",
    "train_indexes_sample = random.sample(train_indexes, int(len(PPint_interactions_NEW) * 0.1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97ad8c32-1f55-43bc-9dd6-ae99594a7fa0",
   "metadata": {},
   "outputs": [],
   "source": [
    "Df_train = PPint_interactions_NEW[PPint_interactions_NEW.interface_id.isin(train_indexes_sample)]\n",
    "Df_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f587290-f774-42ba-bc41-b24a0d3ac163",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CLIP_PPint_analysis_dataset(Dataset):\n",
    "    def __init__(\n",
    "        self,\n",
    "        dframe,\n",
    "        paths,\n",
    "        embedding_dim=1280,\n",
    "        embedding_pad_value=-5000.0,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.dframe = dframe.copy()\n",
    "        self.embedding_dim = int(embedding_dim)\n",
    "        self.emb_pad = float(embedding_pad_value)\n",
    "\n",
    "        # lengths\n",
    "        self.max_blen = self.dframe[\"seq_binder_len\"].max()\n",
    "        self.max_tlen = self.dframe[\"seq_target_len\"].max()\n",
    "\n",
    "        # paths\n",
    "        self.encoding_bpath, self.encoding_tpath = paths\n",
    "\n",
    "        # index & storage\n",
    "        self.dframe.set_index(\"target_binder_id\", inplace=True)\n",
    "        self.accessions = self.dframe.index.astype(str).tolist()\n",
    "        self.name_to_row = {name: i for i, name in enumerate(self.accessions)}\n",
    "        self.samples = []\n",
    "\n",
    "        for accession in tqdm(self.accessions, total=len(self.accessions), desc=\"#Loading ESM2 embeddings and contacts\"):\n",
    "            parts = accession.split(\"_\") # e.g. accession 7S8T_5_F_7S8T_5_G\n",
    "            tgt_id = \"_\".join(parts[:3])\n",
    "            bnd_id = \"_\".join(parts[3:])\n",
    "\n",
    "            ### --- embeddings (pad to fixed lengths) --- ###\n",
    "            \n",
    "            # laod embeddings\n",
    "            t_emb = np.load(os.path.join(self.encoding_tpath, f\"{tgt_id}.npy\"))     # [Lt, D]\n",
    "            b_emb = np.load(os.path.join(self.encoding_bpath, f\"{bnd_id}.npy\"))     # [Lb, D]\n",
    "\n",
    "            # quich check whether embedding dimmension is as it suppose to be\n",
    "            if t_emb.shape[1] != self.embedding_dim or b_emb.shape[1] != self.embedding_dim:\n",
    "                raise ValueError(\"Embedding dim mismatch with 'embedding_dim'.\")\n",
    "\n",
    "            # add -5000 to all the padded target rows\n",
    "            if t_emb.shape[0] < self.max_tlen:\n",
    "                t_emb = np.concatenate([t_emb, np.full((self.max_tlen - t_emb.shape[0], t_emb.shape[1]), self.emb_pad, dtype=t_emb.dtype)], axis=0)\n",
    "            else:\n",
    "                t_emb = t_emb[: self.max_tlen] # no padding was used\n",
    "\n",
    "            # add -5000 to all the padded binder rows\n",
    "            if b_emb.shape[0] < self.max_blen:\n",
    "                b_emb = np.concatenate([b_emb, np.full((self.max_blen - b_emb.shape[0], b_emb.shape[1]), self.emb_pad, dtype=b_emb.dtype)], axis=0)\n",
    "            else:\n",
    "                b_emb = b_emb[: self.max_blen] # no padding was used\n",
    "\n",
    "            self.samples.append((b_emb, t_emb))\n",
    "\n",
    "    # ---- Dataset API ----\n",
    "    def __len__(self):\n",
    "        return len(self.samples)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        b_arr, t_arr = self.samples[idx]\n",
    "        binder_emb, target_emb = torch.from_numpy(b_arr).float(), torch.from_numpy(t_arr).float()\n",
    "        label = torch.tensor(1, dtype=torch.float32)  # single scalar labe\n",
    "        return binder_emb, target_emb, label\n",
    "\n",
    "    def _get_by_name(self, name):\n",
    "        # Single item -> return exactly what __getitem__ returns\n",
    "        if isinstance(name, str):\n",
    "            return self.__getitem__(self.name_to_row[name])\n",
    "        \n",
    "        # Multiple items -> fetch all\n",
    "        out = [self.__getitem__(self.name_to_row[n]) for n in list(name)]\n",
    "        b_list, t_list, lbl_list = zip(*out)\n",
    "    \n",
    "        # Stack embeddings\n",
    "        b  = torch.stack([torch.as_tensor(x) for x in b_list],  dim=0)  # [B, ...]\n",
    "        t  = torch.stack([torch.as_tensor(x) for x in t_list],  dim=0)  # [B, ...]\n",
    "    \n",
    "        # Stack labels\n",
    "        labels = torch.stack(lbl_list)  # [B]\n",
    "    \n",
    "        return b, t, labels\n",
    "\n",
    "bemb_path = \"/work3/s232958/data/PPint_DB/binders_embeddings_esm2\"\n",
    "temb_path = \"/work3/s232958/data/PPint_DB/targets_embeddings_esm2\"\n",
    "\n",
    "training_Dataset = CLIP_PPint_analysis_dataset(\n",
    "    Df_train,\n",
    "    paths=[bemb_path, temb_path],\n",
    "    embedding_dim=1280\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "978c0242-7d60-4c4e-930a-d843eebcae44",
   "metadata": {},
   "source": [
    "### Loading Meta validation dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7079a2a1-e9d1-4202-8aa3-3beb306857ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "interaction_df = pd.read_csv(\"/work3/s232958/data/meta_analysis/interaction_df_metaanal.csv\")[[\"A_seq\", \"B_seq\", \"target_id_mod\", \"target_binder_ID\", \"binder\"]].rename(columns = {\n",
    "    \"A_seq\" : \"seq_binder\",\n",
    "    \"B_seq\" : \"seq_target\",\n",
    "    \"target_binder_ID\" : \"binder_id\",\n",
    "    \"target_id_mod\" : \"target_id\",\n",
    "    \"binder\" : \"binder_label\"\n",
    "})\n",
    "interaction_df[\"seq_target_len\"] = [len(seq) for seq in interaction_df[\"seq_target\"].tolist()]\n",
    "interaction_df[\"seq_binder_len\"] = [len(seq) for seq in interaction_df[\"seq_binder\"].tolist()]\n",
    "\n",
    "# Targets df\n",
    "target_df = interaction_df[[\"target_id\",\"seq_target\"]].rename(columns={\"seq_target\":\"sequence\", \"target_id\" : \"ID\"})\n",
    "target_df[\"seq_len\"] = target_df[\"sequence\"].apply(len)\n",
    "target_df = target_df.drop_duplicates(subset=[\"ID\",\"sequence\"])\n",
    "target_df = target_df.set_index(\"ID\")\n",
    "\n",
    "# Binders df\n",
    "binder_df = interaction_df[[\"binder_id\",\"seq_binder\"]].rename(columns={\"seq_binder\":\"sequence\", \"binder_id\" : \"ID\"})\n",
    "binder_df[\"seq_len\"] = binder_df[\"sequence\"].apply(len)\n",
    "binder_df = binder_df.set_index(\"ID\")\n",
    "\n",
    "# target_df\n",
    "\n",
    "# Interaction Dict\n",
    "interaction_Dict = dict(enumerate(zip(interaction_df[\"target_id\"], interaction_df[\"binder_id\"]), start=1))\n",
    "interaction_df_shuffled = interaction_df.sample(frac=1, random_state=0).reset_index(drop=True)\n",
    "interaction_df_shuffled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1f0bee0-e259-44f9-ba13-aff0ec3c7686",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CLIP_PPint_MetaData(Dataset):\n",
    "    def __init__(\n",
    "        self,\n",
    "        dframe,\n",
    "        paths,\n",
    "        embedding_dim=1280,\n",
    "        embedding_pad_value=-5000.0\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.dframe = dframe.copy()\n",
    "        self.embedding_dim = int(embedding_dim)\n",
    "        self.emb_pad = float(embedding_pad_value)\n",
    "        self.max_blen = self.dframe[\"seq_binder_len\"].max()\n",
    "        self.max_tlen = self.dframe[\"seq_target_len\"].max()\n",
    "\n",
    "        # paths\n",
    "        self.encoding_bpath, self.encoding_tpath = paths\n",
    "\n",
    "        # index & storage\n",
    "        self.dframe.set_index(\"binder_id\", inplace=True)\n",
    "        self.accessions = self.dframe.index.astype(str).tolist()\n",
    "        self.name_to_row = {name: i for i, name in enumerate(self.accessions)}\n",
    "        self.samples = []\n",
    "\n",
    "        for accession in tqdm(self.accessions, total=len(self.accessions), desc=\"#Loading ESM2 embeddings\"):\n",
    "            lbl = torch.tensor(int(self.dframe.loc[accession, \"binder_label\"]))\n",
    "            parts = accession.split(\"_\") # e.g. accession 7S8T_5_F_7S8T_5_G\n",
    "            tgt_id = \"_\".join(parts[:-1])\n",
    "            bnd_id = accession\n",
    "\n",
    "            ### --- embeddings (pad to fixed lengths) --- ###\n",
    "            \n",
    "            # laod embeddings\n",
    "            t_emb = np.load(os.path.join(self.encoding_tpath, f\"{tgt_id}.npy\"))     # [Lt, D]\n",
    "            b_emb = np.load(os.path.join(self.encoding_bpath, f\"{bnd_id}.npy\"))     # [Lb, D]\n",
    "\n",
    "            # quich check whether embedding dimmension is as it suppose to be\n",
    "            if t_emb.shape[1] != self.embedding_dim or b_emb.shape[1] != self.embedding_dim:\n",
    "                raise ValueError(\"Embedding dim mismatch with 'embedding_dim'.\")\n",
    "\n",
    "            # add -5000 to all the padded target rows\n",
    "            if t_emb.shape[0] < self.max_tlen:\n",
    "                t_emb = np.concatenate([t_emb, np.full((self.max_tlen - t_emb.shape[0], t_emb.shape[1]), self.emb_pad, dtype=t_emb.dtype)], axis=0)\n",
    "            else:\n",
    "                t_emb = t_emb[: self.max_tlen] # no padding was used\n",
    "\n",
    "            # add -5000 to all the padded binder rows\n",
    "            if b_emb.shape[0] < self.max_blen:\n",
    "                b_emb = np.concatenate([b_emb, np.full((self.max_blen - b_emb.shape[0], b_emb.shape[1]), self.emb_pad, dtype=b_emb.dtype)], axis=0)\n",
    "            else:\n",
    "                b_emb = b_emb[: self.max_blen] # no padding was used\n",
    "\n",
    "            self.samples.append((b_emb, t_emb, lbl))\n",
    "\n",
    "    # ---- Dataset API ----\n",
    "    def __len__(self):\n",
    "        return len(self.samples)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        b_arr, t_arr, lbls = self.samples[idx]\n",
    "        binder_emb, target_emb = torch.from_numpy(b_arr).float(), torch.from_numpy(t_arr).float()\n",
    "        return binder_emb, target_emb, lbls\n",
    "\n",
    "    def _get_by_name(self, name):\n",
    "        # Single item -> return exactly what __getitem__ returns\n",
    "        if isinstance(name, str):\n",
    "            return self.__getitem__(self.name_to_row[name])\n",
    "        \n",
    "        # Multiple items -> fetch all\n",
    "        out = [self.__getitem__(self.name_to_row[n]) for n in list(name)]\n",
    "        b_list, t_list, lbl_list = zip(*out)\n",
    "    \n",
    "        # Stack embeddings\n",
    "        b  = torch.stack([torch.as_tensor(x) for x in b_list],  dim=0)  # [B, ...]\n",
    "        t  = torch.stack([torch.as_tensor(x) for x in t_list],  dim=0)  # [B, ...]\n",
    "    \n",
    "        # Stack labels\n",
    "        labels = torch.stack(lbl_list)  # [B]\n",
    "    \n",
    "        return b, t, labels\n",
    "\n",
    "bemb_path = \"/work3/s232958/data/meta_analysis/binders_embeddings_esm2\"\n",
    "temb_path = \"/work3/s232958/data/meta_analysis/targets_embeddings_esm2\"\n",
    "\n",
    "validation_Dataset = CLIP_PPint_MetaData(\n",
    "    interaction_df_shuffled[:500],\n",
    "    paths=[bemb_path, temb_path],\n",
    "    embedding_dim=1280\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fadb2503-da3c-413a-b655-9d2d8e4ec911",
   "metadata": {},
   "source": [
    "### Train model from scratch with 10% of PPint dataset using old architecture (encodings only)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0365067c-759c-4cdd-bfe1-5f35b2033fee",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_key_padding_mask(embeddings, padding_value=-5000, offset=10):\n",
    "    return (embeddings < (padding_value + offset)).all(dim=-1)\n",
    "\n",
    "def create_mean_of_non_masked(embeddings, padding_mask):\n",
    "    # Use masked select and mean to compute the mean of non-masked elements\n",
    "    # embeddings should be of shape (batch_size, seq_len, features)\n",
    "    seq_embeddings = []\n",
    "    for i in range(embeddings.shape[0]): # looping over all batch elements\n",
    "        non_masked_embeddings = embeddings[i][~padding_mask[i]] # shape [num_real_tokens, features]\n",
    "        if len(non_masked_embeddings) == 0:\n",
    "            print(\"You are masking all positions when creating sequence representation\")\n",
    "            sys.exit(1)\n",
    "        mean_embedding = non_masked_embeddings.mean(dim=0) # sequence is represented by the single vecotr [1152] [features]\n",
    "        seq_embeddings.append(mean_embedding)\n",
    "    return torch.stack(seq_embeddings)\n",
    "\n",
    "class MiniCLIP_w_transformer_crossattn(pl.LightningModule):\n",
    "\n",
    "    def __init__(self, padding_value = -5000, embed_dimension=embedding_dimension, num_recycles=2):\n",
    "\n",
    "        super().__init__()\n",
    "        self.num_recycles = num_recycles # how many times you iteratively refine embeddings with self- and cross-attention (ALPHA-Fold-style recycling).\n",
    "        self.padding_value = padding_value\n",
    "        self.embed_dimension = embed_dimension\n",
    "\n",
    "        self.logit_scale = nn.Parameter(torch.tensor(math.log(1/0.07)))  # ~CLIP init\n",
    "\n",
    "        self.transformerencoder =  nn.TransformerEncoderLayer(\n",
    "            d_model=self.embed_dimension,\n",
    "            nhead=8,\n",
    "            dropout=0.1,\n",
    "            batch_first=True,\n",
    "            dim_feedforward=self.embed_dimension\n",
    "            )\n",
    " \n",
    "        self.norm = nn.LayerNorm(self.embed_dimension)  # For residual additions\n",
    "\n",
    "        self.cross_attn = nn.MultiheadAttention(\n",
    "            embed_dim=self.embed_dimension,\n",
    "            num_heads=8,\n",
    "            dropout=0.1,\n",
    "            batch_first=True\n",
    "        )\n",
    "\n",
    "        self.prot_embedder = nn.Sequential(\n",
    "            nn.Linear(self.embed_dimension, 640),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(640, 320),\n",
    "        )\n",
    "        \n",
    "    def forward(self, pep_input, prot_input, label=None, pep_int_mask=None, prot_int_mask=None, int_prob=None, mem_save=True): # , pep_tokens, prot_tokens\n",
    "\n",
    "        pep_mask = create_key_padding_mask(embeddings=pep_input, padding_value=self.padding_value)\n",
    "        prot_mask = create_key_padding_mask(embeddings=prot_input, padding_value=self.padding_value)\n",
    " \n",
    "        # Initialize residual states\n",
    "        pep_emb = pep_input.clone()\n",
    "        prot_emb = prot_input.clone()\n",
    " \n",
    "        for _ in range(self.num_recycles):\n",
    "\n",
    "            # Transformer encoding with residual\n",
    "            pep_trans = self.transformerencoder(self.norm(pep_emb), src_key_padding_mask=pep_mask)\n",
    "            prot_trans = self.transformerencoder(self.norm(prot_emb), src_key_padding_mask=prot_mask)\n",
    "\n",
    "            # Cross-attention with residual\n",
    "            pep_cross, _ = self.cross_attn(query=self.norm(pep_trans), key=self.norm(prot_trans), value=self.norm(prot_trans), key_padding_mask=prot_mask)\n",
    "            prot_cross, _ = self.cross_attn(query=self.norm(prot_trans), key=self.norm(pep_trans), value=self.norm(pep_trans), key_padding_mask=pep_mask)\n",
    "            \n",
    "            # Additive update with residual connection\n",
    "            pep_emb = pep_emb + pep_cross  \n",
    "            prot_emb = prot_emb + prot_cross\n",
    "\n",
    "        pep_seq_coding = create_mean_of_non_masked(pep_emb, pep_mask)\n",
    "        prot_seq_coding = create_mean_of_non_masked(prot_emb, prot_mask)\n",
    "        \n",
    "        # Use self-attention outputs for embeddings\n",
    "        pep_seq_coding = F.normalize(self.prot_embedder(pep_seq_coding))\n",
    "        prot_seq_coding = F.normalize(self.prot_embedder(prot_seq_coding))\n",
    " \n",
    "        if mem_save:\n",
    "            torch.cuda.empty_cache()\n",
    "        \n",
    "        scale = torch.exp(self.logit_scale).clamp(max=100.0)\n",
    "        logits = scale * (pep_seq_coding * prot_seq_coding).sum(dim=-1)\n",
    "        \n",
    "        return logits\n",
    "\n",
    "    def training_step(self, batch, device):\n",
    "        embedding_pep, embedding_prot, labels = batch\n",
    "        embedding_pep, embedding_prot = embedding_pep.to(device), embedding_prot.to(device)\n",
    "        \n",
    "        positive_logits = self.forward(embedding_pep, embedding_prot)\n",
    "        \n",
    "        # Negative indexes\n",
    "        rows, cols = torch.triu_indices(embedding_prot.size(0), embedding_prot.size(0), offset=1)         \n",
    "        \n",
    "        negative_logits = self(embedding_pep[rows,:,:], \n",
    "                          embedding_prot[cols,:,:], \n",
    "                          int_prob=0.0)\n",
    "\n",
    "        # loss of predicting partner using peptide\n",
    "        positive_loss = F.binary_cross_entropy_with_logits(positive_logits, torch.ones_like(positive_logits).to(device))\n",
    " \n",
    "        # loss of predicting peptide using partner\n",
    "        negative_loss =  F.binary_cross_entropy_with_logits(negative_logits, torch.zeros_like(negative_logits).to(device))\n",
    "        \n",
    "        loss = (positive_loss + negative_loss) / 2\n",
    " \n",
    "        # del partner_prediction_loss, peptide_prediction_loss, embedding_pep, embedding_prot\n",
    "        torch.cuda.empty_cache()\n",
    "        return loss\n",
    "    \n",
    "    def validation_step(self, batch, device):\n",
    "        # Predict on random batches of training batch size\n",
    "        embedding_binder, embedding_target, labels = batch\n",
    "        # embedding_binder, embedding_target, contacts_binder, contacts_target = embedding_binder.to(device), embedding_target.to(device), contacts_binder.to(device), contacts_target.to(device)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "\n",
    "            logits = self.forward(embedding_binder, embedding_target)\n",
    "            loss = F.binary_cross_entropy_with_logits(logits.to(device), labels.to(device))\n",
    "            return logits, loss\n",
    "\n",
    "    def calculate_logit_matrix(self,embedding_pep,embedding_prot):\n",
    "        rows, cols = torch.triu_indices(embedding_pep.size(0), embedding_pep.size(0), offset=1)\n",
    "        \n",
    "        positive_logits = self(embedding_pep, embedding_prot)\n",
    "        negative_logits = self(embedding_pep[rows,:,:], embedding_prot[cols,:,:], int_prob=0.0)\n",
    "        \n",
    "        logit_matrix = torch.zeros((embedding_pep.size(0),embedding_pep.size(0)),device=self.device)\n",
    "        logit_matrix[rows, cols] = negative_logits\n",
    "        logit_matrix[cols, rows] = negative_logits\n",
    "        \n",
    "        diag_indices = torch.arange(embedding_pep.size(0), device=self.device)\n",
    "        logit_matrix[diag_indices, diag_indices] = positive_logits.squeeze()\n",
    "        \n",
    "        return logit_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24222398-c484-4e4b-9256-987a529c19ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = MiniCLIP_w_transformer_crossattn(embed_dimension=embedding_dimension, num_recycles=number_of_recycles)\n",
    "model.to(\"cuda\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc2460df-3d34-4548-807d-ca306e006c5d",
   "metadata": {},
   "source": [
    "### Trianing loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b817e401-cca6-44a7-a7d0-8bdf1d51cfec",
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch(iterable, n=1):\n",
    "    \"\"\"Takes any indexable iterable (e.g., a list of observation IDs) and yields contiguous slices of length n.\"\"\"\n",
    "    l = len(iterable)\n",
    "    for ndx in range(0, l, n):\n",
    "        yield iterable[ndx:min(ndx + n, l)]\n",
    "\n",
    "class TrainWrapper():\n",
    "\n",
    "    def __init__(self, \n",
    "                 model, \n",
    "                 training_loader, \n",
    "                 validation_loader,\n",
    "                 optimizer, \n",
    "                 EPOCHS, \n",
    "                 runID, \n",
    "                 device, \n",
    "                 model_save_steps=False, \n",
    "                 model_save_path=False, \n",
    "                 v=False, \n",
    "                 wandb_tracker=False):\n",
    "        \n",
    "        self.model = model \n",
    "        self.training_loader = training_loader\n",
    "        self.validation_loader = validation_loader\n",
    "        \n",
    "        self.EPOCHS = EPOCHS\n",
    "        self.optimizer = optimizer\n",
    "        self.device = device\n",
    "        \n",
    "        self.wandb_tracker = wandb_tracker\n",
    "        self.model_save_steps = model_save_steps # if truthy (e.g., 1, 5), save a checkpoint every N epochs.\n",
    "        self.verbose = v\n",
    "        self.best_vloss = 1_000_000\n",
    "        self.runID = runID\n",
    "        self.trained_model_dir = model_save_path\n",
    "        self.print_frequency_loss = 1\n",
    "\n",
    "    def train_one_epoch(self):\n",
    "\n",
    "        self.model.train() \n",
    "        running_loss = 0 # accumulate the loss over all batches in this epoch.\n",
    "\n",
    "        for batch in tqdm(self.training_loader, total=len(self.training_loader), desc=\"Running through epoch\"):\n",
    "            \n",
    "            if batch[0].size(0) == 1: \n",
    "                continue\n",
    "            \n",
    "            self.optimizer.zero_grad() # clears old gradients from the previous step\n",
    "            loss = self.model.training_step(batch, self.device) # training step calles forward to get logits and calculate loss after\n",
    "            loss.backward() # compute gradients\n",
    "            self.optimizer.step() # update weights\n",
    "            running_loss += loss.item() # accumulation of the loss from all the batches\n",
    "\n",
    "            del loss, batch\n",
    "            torch.cuda.empty_cache()\n",
    "            \n",
    "        return running_loss / len(self.training_loader) # loss is averaged per number of batches\n",
    "\n",
    "    def validate(self):\n",
    "        \n",
    "        self.model.eval()\n",
    "        \n",
    "        running_loss = 0.0\n",
    "        all_logits = []\n",
    "        all_lbls = []\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for batch in tqdm(self.validation_loader, total=len(self.validation_loader)):\n",
    "                if batch[0].size(0) == 1: # We can't make negatives on a batch of 1\n",
    "                    continue\n",
    "                embedding_binder, embedding_target, labels = batch\n",
    "                logits, loss = self.model.validation_step(batch, self.device)\n",
    "                \n",
    "                running_loss += loss.item()\n",
    "                all_logits.append(logits.detach().view(-1).cpu())\n",
    "                all_lbls.append(labels.detach().view(-1).cpu())\n",
    "                \n",
    "            val_loss = running_loss / len(self.validation_loader)\n",
    "            all_logits = torch.cat(all_logits).numpy()\n",
    "            all_lbls   = torch.cat(all_lbls).numpy()\n",
    "        \n",
    "            # ROC / PR metrics\n",
    "            fpr, tpr, thresholds = metrics.roc_curve(all_lbls, all_logits)\n",
    "            auroc = metrics.roc_auc_score(all_lbls, all_logits)\n",
    "            aupr  = metrics.average_precision_score(all_lbls, all_logits)\n",
    "\n",
    "            # Accuracy: threshold logits at 0\n",
    "            # (logit >= 0  <=>  sigmoid(logit) >= 0.5)\n",
    "            y_pred = (all_logits >= 0).astype(int)\n",
    "            y_true = all_lbls.astype(int)\n",
    "            accuracy = (y_pred == y_true).mean()\n",
    "        \n",
    "            return val_loss, all_logits, accuracy, auroc, aupr\n",
    "\n",
    "    def train_model(self):\n",
    "        \n",
    "        torch.cuda.empty_cache()\n",
    "        \n",
    "        if self.verbose:\n",
    "            print(f\"Training model {str(self.runID)}\")\n",
    "        \n",
    "        # --- initial validation before training\n",
    "        print(\"Initial validation before starting training\")\n",
    "        val_loss, all_logits, val_accuracy, auroc, aupr = self.validate()\n",
    "        torch.cuda.empty_cache()\n",
    "        \n",
    "        if self.verbose: \n",
    "            print(f'Before training - Meta Val Loss {round(val_loss,4)}',\n",
    "                  f'Meta Accuracy: {round(val_accuracy,4)}',\n",
    "                  f'Meta AUROC: {round(auroc,4)}',\n",
    "                  f'Meta AUPR: {round(aupr,4)}'\n",
    "                 )\n",
    "        if self.wandb_tracker:\n",
    "            metrics_to_log = {\n",
    "                \"Meta Val-loss\": val_loss,\n",
    "                \"Meta Accuracy\": val_accuracy,\n",
    "                \"Meta Val-AUROC\": auroc,\n",
    "                \"Meta Val-AUPR\": aupr,\n",
    "            }\n",
    "            self.wandb_tracker.log(metrics_to_log)\n",
    "        \n",
    "        # --- training loop\n",
    "        for epoch in tqdm(range(1, self.EPOCHS + 1), total=self.EPOCHS, desc=\"Epochs\"):\n",
    "            \n",
    "            torch.cuda.empty_cache()\n",
    "            \n",
    "            train_loss = self.train_one_epoch()\n",
    "            \n",
    "            # validation after epoch\n",
    "            val_loss, all_logits, val_accuracy, auroc, aupr = self.validate()\n",
    "            \n",
    "            # checkpoint save\n",
    "            if self.model_save_steps:\n",
    "                if epoch % self.model_save_steps == 0:\n",
    "                    check_point_folder = os.path.join(self.trained_model_dir, f\"{str(self.runID)}_checkpoint_{str(epoch)}\")\n",
    "                    if self.verbose:\n",
    "                        print(\"Saving model to:\", check_point_folder)\n",
    "                    if not os.path.exists(check_point_folder):\n",
    "                        os.makedirs(check_point_folder)\n",
    "                    checkpoint_path = os.path.join(check_point_folder, f\"{str(self.runID)}_checkpoint_epoch_{str(epoch)}.pth\")\n",
    "                    torch.save({'epoch': epoch, \n",
    "                                'model_state_dict': self.model.state_dict(),\n",
    "                                'optimizer_state_dict': self.optimizer.state_dict(), \n",
    "                                'val_loss': val_loss},\n",
    "                               checkpoint_path)\n",
    "            \n",
    "            # console logging\n",
    "            if self.verbose:\n",
    "                if epoch % self.print_frequency_loss == 0:\n",
    "                    print(f'EPOCH {epoch} -  Meta Val loss {round(val_loss,4)}',\n",
    "                          f'Meta accuracy: {round(val_accuracy,4)}',\n",
    "                          f'Meta AUROC: {round(auroc,4)}',\n",
    "                          f'Meta AUPR: {round(aupr,4)}')\n",
    "            \n",
    "            # wandb logging\n",
    "            if self.wandb_tracker:\n",
    "                metrics_to_log_epoch = {\n",
    "                    \"Epoch\": epoch,\n",
    "                    \"Train-loss\": train_loss,\n",
    "                    \"Meta Val-loss\": val_loss,\n",
    "                    \"Meta Accuracy\": val_accuracy,\n",
    "                    \"Meta Val-AUROC\": auroc,\n",
    "                    \"Meta Val-AUPR\" : aupr\n",
    "                }\n",
    "\n",
    "                self.wandb_tracker.log(metrics_to_log_epoch)\n",
    "\n",
    "        if self.wandb_tracker:\n",
    "            self.wandb_tracker.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c09c80cc-d9fa-4fec-baef-e6c76311a946",
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 2e-5\n",
    "EPOCHS = 15\n",
    "g = torch.Generator().manual_seed(SEED)\n",
    "batch_size = 10\n",
    "optimizer = AdamW(model.parameters(), lr=learning_rate)\n",
    "accelerator = Accelerator()\n",
    "device = accelerator.device\n",
    "\n",
    "def collate_varlen(batch):\n",
    "    b_emb = torch.stack([x[0] for x in batch], dim=0)  # fixed length -> stack\n",
    "    t_emb = torch.stack([x[1] for x in batch], dim=0)\n",
    "    # lbls = torch.tensor([float(x[4]) for x in batch])\n",
    "    lbls = torch.tensor([x[2].float() for x in batch])\n",
    "    return b_emb, t_emb, lbls\n",
    "\n",
    "train_dataloader = DataLoader(training_Dataset, batch_size=7, collate_fn=collate_varlen)\n",
    "val_dataloader = DataLoader(validation_Dataset, batch_size=20, shuffle=False, drop_last = False, collate_fn=collate_varlen)\n",
    "\n",
    "# accelerator\n",
    "\n",
    "model, optimizer, train_dataloader, val_dataloader = accelerator.prepare(model, optimizer, train_dataloader, val_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b53b5bb7-5350-405e-9d05-ea7729a949e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in val_dataloader:\n",
    "    __, __, lbls = i\n",
    "    print(lbls.to(device))\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "590809f4-b427-4913-8103-b116cffaeeee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# wandb\n",
    "if use_wandb:\n",
    "    run = wandb.init(\n",
    "        project=\"PPint_retrain_w_10percent_ofdata\",\n",
    "        name=f\"PPint_10prcnt_embOnly_MetaVal\",\n",
    "        config={\"learning_rate\": learning_rate, \"batch_size\": batch_size, \"epochs\": EPOCHS,\n",
    "                \"architecture\": \"MiniCLIP_w_transformer_crossattn\", \"dataset\": \"Meta analysis\"},\n",
    "    )\n",
    "    wandb.watch(accelerator.unwrap_model(model), log=\"all\", log_freq=100)\n",
    "else:\n",
    "    run = None\n",
    "\n",
    "# train\n",
    "training_wrapper = TrainWrapper(model=model, \n",
    "                                training_loader = train_dataloader, \n",
    "                                validation_loader = val_dataloader, \n",
    "                                optimizer = optimizer, \n",
    "                                EPOCHS = EPOCHS,\n",
    "                                runID = runID, \n",
    "                                device = device, \n",
    "                                model_save_steps = 1,\n",
    "                                model_save_path = trained_model_dir, \n",
    "                                v = True, \n",
    "                                wandb_tracker = wandb\n",
    "                                )\n",
    "\n",
    "\n",
    "training_wrapper.train_model() # start training"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
