{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7f838074-e829-40d5-9b15-949a9bfe9708",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tesla V100-SXM2-32GB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/work3/s232958/envs/esm_cuda/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: 'Could not load this library: /work3/s232958/envs/esm_cuda/lib/python3.10/site-packages/torchvision/image.so'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?\n",
      "  warn(\n",
      "/work3/s232958/envs/esm_cuda/lib/python3.10/site-packages/transformers/utils/generic.py:441: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.\n",
      "  _torch_pytree._register_pytree_node(\n"
     ]
    }
   ],
   "source": [
    "import uuid, sys, os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import ast\n",
    "import math\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn import metrics\n",
    "from scipy import stats\n",
    "from collections import Counter\n",
    "from sklearn.metrics import average_precision_score, precision_recall_curve, roc_auc_score, confusion_matrix, ConfusionMatrixDisplay, classification_report, roc_curve\n",
    "\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"3\"\n",
    "os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"expandable_segments:True\"\n",
    "import torch\n",
    "torch.cuda.set_device(0)  # 0 == \"first visible\" -> actually GPU 2 on the node\n",
    "print(torch.cuda.get_device_name(0))\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader, TensorDataset, WeightedRandomSampler\n",
    "import pytorch_lightning as pl\n",
    "from torch.optim import AdamW\n",
    "\n",
    "torch.manual_seed(0)\n",
    "\n",
    "from accelerate import Accelerator\n",
    "torch.cuda.empty_cache()\n",
    "import training_utils.partitioning_utils as pat_utils\n",
    "from tqdm import trange"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6a54147-26d3-4e49-b87a-b05209b00a7d",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Loading PPint and meta-analysis data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2815ec4-5467-4b95-8d4b-84a3efb3e93b",
   "metadata": {},
   "outputs": [],
   "source": [
    "Df_train = pd.read_csv(\"/work3/s232958/data/PPint_DB/PPint_train_w_pbd_lens.csv\",index_col=0).reset_index(drop=True)\n",
    "Df_test = pd.read_csv(\"/work3/s232958/data/PPint_DB/PPint_test_w_pbd_lens.csv\",index_col=0).reset_index(drop=True)\n",
    "\n",
    "Df_train[\"target_chain\"] = [str(row.ID1[:5]+row.ID1[-1]) for __, row in Df_train.iterrows()]\n",
    "Df_train[\"binder_chain\"] = [str(row.ID2[:5]+row.ID2[-1]) for __, row in Df_train.iterrows()]\n",
    "\n",
    "Df_test[\"target_chain\"] = [str(row.ID1[:5]+row.ID1[-1]) for __, row in Df_test.iterrows()]\n",
    "Df_test[\"binder_chain\"] = [str(row.ID2[:5]+row.ID2[-1]) for __, row in Df_test.iterrows()]\n",
    "\n",
    "Df_train[\"target_binder_id\"] = [str(row.ID1)+\"_\"+str(row.ID2) for __, row in Df_train.iterrows()]\n",
    "Df_test[\"target_binder_id\"] = [str(row.ID1)+\"_\"+str(row.ID2) for __, row in Df_test.iterrows()]\n",
    "\n",
    "Df_train_small = pd.read_csv(\"/work3/s232958/data/PPint_DB/PPint_train.csv\",index_col=0).reset_index(drop=True)\n",
    "Df_test_small = pd.read_csv(\"/work3/s232958/data/PPint_DB/PPint_test.csv\",index_col=0).reset_index(drop=True)\n",
    "\n",
    "Df_train = pd.merge(Df_train, Df_train_small[[\"target_binder_id\", \"dimer\"]], on=\"target_binder_id\", how=\"inner\")\n",
    "Df_test = pd.merge(Df_test, Df_test_small[[\"target_binder_id\", \"dimer\"]], on=\"target_binder_id\", how=\"inner\")\n",
    "\n",
    "Df_train"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48a572b0-4361-48fd-8573-ea5182eac03f",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## ESM-IF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f587290-f774-42ba-bc41-b24a0d3ac163",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CLIP_PPint_class(Dataset):\n",
    "    def __init__(\n",
    "        self,\n",
    "        dframe,\n",
    "        path,\n",
    "        embedding_dim=512,\n",
    "        embedding_pad_value=-5000.0,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.dframe = dframe.copy()\n",
    "        self.embedding_dim = int(embedding_dim)\n",
    "        self.emb_pad = float(embedding_pad_value)\n",
    "\n",
    "        # lengths\n",
    "        self.max_blen = self.dframe[\"pdb_binder_len\"].max()+2\n",
    "        self.max_tlen = self.dframe[\"pdb_target_len\"].max()+2\n",
    "\n",
    "        # paths\n",
    "        self.encoding_path  = path\n",
    "\n",
    "        # index & storage\n",
    "        self.dframe.set_index(\"target_binder_id\", inplace=True)\n",
    "        self.accessions = self.dframe.index.astype(str).tolist()\n",
    "        self.name_to_row = {name: i for i, name in enumerate(self.accessions)}\n",
    "        self.samples = []\n",
    "\n",
    "        for accession in tqdm(self.accessions, total=len(self.accessions), desc=\"#Loading ESM2 embeddings and contacts\"):\n",
    "            parts = accession.split(\"_\") # e.g. accession 7S8T_5_F_7S8T_5_G\n",
    "            tgt_id = parts[0]+\"_\"+parts[2]\n",
    "            bnd_id = parts[3]+\"_\"+parts[5]\n",
    "\n",
    "            ### --- embeddings (pad to fixed lengths) --- ###\n",
    "            \n",
    "            # laod embeddings\n",
    "            t_emb = np.load(os.path.join(self.encoding_path, f\"{tgt_id}.npy\"))     # [Lt, D]\n",
    "            b_emb = np.load(os.path.join(self.encoding_path, f\"{bnd_id}.npy\"))     # [Lb, D]\n",
    "\n",
    "            assert (b_emb.shape[0] == self.dframe.loc[accession].pdb_binder_len+2)\n",
    "            assert (t_emb.shape[0] == self.dframe.loc[accession].pdb_target_len+2)\n",
    "\n",
    "            # quich check whether embedding dimmension is as it suppose to be\n",
    "            if t_emb.shape[1] != self.embedding_dim or b_emb.shape[1] != self.embedding_dim:\n",
    "                raise ValueError(\"Embedding dim mismatch with 'embedding_dim'.\")\n",
    "\n",
    "            # add -5000 to all the padded target rows\n",
    "            if t_emb.shape[0] < self.max_tlen:\n",
    "                t_emb = np.concatenate([t_emb, np.full((self.max_tlen - t_emb.shape[0], t_emb.shape[1]), self.emb_pad, dtype=t_emb.dtype)], axis=0)\n",
    "            else:\n",
    "                t_emb = t_emb[: self.max_tlen] # no padding was used\n",
    "\n",
    "            # add -5000 to all the padded binder rows\n",
    "            if b_emb.shape[0] < self.max_blen:\n",
    "                b_emb = np.concatenate([b_emb, np.full((self.max_blen - b_emb.shape[0], b_emb.shape[1]), self.emb_pad, dtype=b_emb.dtype)], axis=0)\n",
    "            else:\n",
    "                b_emb = b_emb[: self.max_blen] # no padding was used\n",
    "\n",
    "            self.samples.append((b_emb, t_emb))\n",
    "\n",
    "        # ---- Dataset API ----\n",
    "    def __len__(self):\n",
    "        return len(self.samples)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        b_arr, t_arr = self.samples[idx]\n",
    "        binder_emb, target_emb = torch.from_numpy(b_arr).float(), torch.from_numpy(t_arr).float()\n",
    "        label = torch.tensor(1, dtype=torch.float32)  # single scalar labe\n",
    "        return binder_emb, target_emb, label\n",
    "\n",
    "    def _get_by_name(self, name):\n",
    "        # Single item -> return exactly what __getitem__ returns\n",
    "        if isinstance(name, str):\n",
    "            return self.__getitem__(self.name_to_row[name])\n",
    "        \n",
    "        # Multiple items -> fetch all\n",
    "        out = [self.__getitem__(self.name_to_row[n]) for n in list(name)]\n",
    "        b_list, t_list, lbl_list = zip(*out)\n",
    "    \n",
    "        # Stack embeddings\n",
    "        b  = torch.stack([torch.as_tensor(x) for x in b_list],  dim=0)  # [B, ...]\n",
    "        t  = torch.stack([torch.as_tensor(x) for x in t_list],  dim=0)  # [B, ...]\n",
    "    \n",
    "        # Stack labels\n",
    "        labels = torch.stack(lbl_list)  # [B]\n",
    "    \n",
    "        return b, t, labels\n",
    "\n",
    "emb_path = \"/work3/s232958/data/PPint_DB/esmif_embeddings_noncanonical\"\n",
    "\n",
    "testing_Dataset = CLIP_PPint_class(\n",
    "    Df_test,\n",
    "    path=emb_path,\n",
    "    embedding_dim=512\n",
    ")\n",
    "\n",
    "### Getting indeces of non-dimers\n",
    "indices_non_dimers_Df = Df_test[~Df_test[\"dimer\"]]\n",
    "indices_non_dimers = Df_test[~Df_test[\"dimer\"]].index.tolist()\n",
    "indices_non_dimers[:5]\n",
    "\n",
    "non_dimers_Dataset = CLIP_PPint_class(\n",
    "    indices_non_dimers_Df,\n",
    "    path=emb_path,\n",
    "    embedding_dim=512\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "938812a5-3263-4dfc-ab4d-173f736f94b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "interaction_df = pd.read_csv(\"/work3/s232958/data/meta_analysis/interaction_df_metaanal_w_pbd_lens.csv\").drop(columns = [\"binder_id\", \"target_id\"]).rename(columns = {\n",
    "    \"target_id_mod\" : \"target_id\",\n",
    "    \"target_binder_ID\" : \"binder_id\",\n",
    "})\n",
    "\n",
    "# Interaction Dict\n",
    "interaction_df_shuffled = interaction_df.sample(frac=1, random_state=0).reset_index(drop=True)\n",
    "interaction_df_shuffled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2406cabf-8a37-4a84-abac-0a91c755c432",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CLIP_Meta_class(Dataset):\n",
    "    def __init__(\n",
    "        self,\n",
    "        dframe,\n",
    "        paths,\n",
    "        embedding_dim=512,\n",
    "        embedding_pad_value=-5000.0\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.dframe = dframe.copy()\n",
    "        self.embedding_dim = int(embedding_dim)\n",
    "        self.emb_pad = float(embedding_pad_value)\n",
    "        self.max_blen = self.dframe[\"pdb_len_binder\"].max()+2\n",
    "        self.max_tlen = self.dframe[\"pdb_len_target\"].max()+2\n",
    "\n",
    "        # paths\n",
    "        self.encoding_bpath, self.encoding_tpath = paths\n",
    "\n",
    "        # index & storage\n",
    "        self.dframe.set_index(\"binder_id\", inplace=True)\n",
    "        self.accessions = self.dframe.index.astype(str).tolist()\n",
    "        self.name_to_row = {name: i for i, name in enumerate(self.accessions)}\n",
    "        self.samples = []\n",
    "\n",
    "        for accession in tqdm(self.accessions, total=len(self.accessions), desc=\"#Loading ESM2 embeddings\"):\n",
    "            lbl = torch.tensor(int(self.dframe.loc[accession, \"binder\"]))\n",
    "            parts = accession.split(\"_\")\n",
    "            tgt_id = \"_\".join(parts[:-1])\n",
    "            bnd_id = accession\n",
    "\n",
    "            ### --- embeddings (pad to fixed lengths) --- ###\n",
    "            \n",
    "            # laod embeddings\n",
    "            t_emb = np.load(os.path.join(self.encoding_tpath, f\"{tgt_id}.npy\"))     # [Lt, D]\n",
    "            b_emb = np.load(os.path.join(self.encoding_bpath, f\"{bnd_id}.npy\"))     # [Lb, D]\n",
    "\n",
    "            assert (b_emb.shape[0] == self.dframe.loc[accession].pdb_len_binder+2)\n",
    "            assert (t_emb.shape[0] == self.dframe.loc[accession].pdb_len_target+2)\n",
    "\n",
    "            # quich check whether embedding dimmension is as it suppose to be\n",
    "            if t_emb.shape[1] != self.embedding_dim or b_emb.shape[1] != self.embedding_dim:\n",
    "                raise ValueError(\"Embedding dim mismatch with 'embedding_dim'.\")\n",
    "\n",
    "            # add -5000 to all the padded target rows\n",
    "            if t_emb.shape[0] < self.max_tlen:\n",
    "                t_emb = np.concatenate([t_emb, np.full((self.max_tlen - t_emb.shape[0], t_emb.shape[1]), self.emb_pad, dtype=t_emb.dtype)], axis=0)\n",
    "            else:\n",
    "                t_emb = t_emb[: self.max_tlen] # no padding was used\n",
    "\n",
    "            # add -5000 to all the padded binder rows\n",
    "            if b_emb.shape[0] < self.max_blen:\n",
    "                b_emb = np.concatenate([b_emb, np.full((self.max_blen - b_emb.shape[0], b_emb.shape[1]), self.emb_pad, dtype=b_emb.dtype)], axis=0)\n",
    "            else:\n",
    "                b_emb = b_emb[: self.max_blen] # no padding was used\n",
    "\n",
    "            self.samples.append((b_emb, t_emb, lbl))\n",
    "\n",
    "    # ---- Dataset API ----\n",
    "    def __len__(self):\n",
    "        return len(self.samples)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        b_arr, t_arr, lbls = self.samples[idx]\n",
    "        binder_emb, target_emb = torch.from_numpy(b_arr).float(), torch.from_numpy(t_arr).float()\n",
    "        return binder_emb, target_emb, lbls\n",
    "\n",
    "    def _get_by_name(self, name):\n",
    "        # Single item -> return exactly what __getitem__ returns\n",
    "        if isinstance(name, str):\n",
    "            return self.__getitem__(self.name_to_row[name])\n",
    "        \n",
    "        # Multiple items -> fetch all\n",
    "        out = [self.__getitem__(self.name_to_row[n]) for n in list(name)]\n",
    "        b_list, t_list, lbl_list = zip(*out)\n",
    "    \n",
    "        # Stack embeddings\n",
    "        b  = torch.stack([torch.as_tensor(x) for x in b_list],  dim=0)  # [B, ...]\n",
    "        t  = torch.stack([torch.as_tensor(x) for x in t_list],  dim=0)  # [B, ...]\n",
    "    \n",
    "        # Stack labels\n",
    "        labels = torch.stack(lbl_list)  # [B]\n",
    "    \n",
    "        return b, t, labels\n",
    "\n",
    "bemb_path = \"/work3/s232958/data/meta_analysis/esmif_embeddings_binders\"\n",
    "temb_path = \"/work3/s232958/data/meta_analysis/esmif_embeddings_targets\"\n",
    "\n",
    "validation_Dataset = CLIP_Meta_class(\n",
    "    # interaction_df_shuffled[:len(Df_test)],\n",
    "    interaction_df_shuffled,\n",
    "    paths=[bemb_path, temb_path],\n",
    "    embedding_dim=512\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca0245ea-84ba-4dd3-b127-0ca188c4cd42",
   "metadata": {},
   "source": [
    "### Pre-trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a9a5a68-ae67-41b9-96dc-09f62aa2fe95",
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_dimension = 512\n",
    "\n",
    "def create_key_padding_mask(embeddings, padding_value=-5000, offset=10):\n",
    "    return (embeddings < (padding_value + offset)).all(dim=-1)\n",
    "\n",
    "def create_mean_of_non_masked(embeddings, padding_mask):\n",
    "    # Use masked select and mean to compute the mean of non-masked elements\n",
    "    # embeddings should be of shape (batch_size, seq_len, features)\n",
    "    seq_embeddings = []\n",
    "    for i in range(embeddings.shape[0]): # looping over all batch elements\n",
    "        non_masked_embeddings = embeddings[i][~padding_mask[i]] # shape [num_real_tokens, features]\n",
    "        if len(non_masked_embeddings) == 0:\n",
    "            print(\"You are masking all positions when creating sequence representation\")\n",
    "            sys.exit(1)\n",
    "        mean_embedding = non_masked_embeddings.mean(dim=0) # sequence is represented by the single vecotr [1152] [features]\n",
    "        seq_embeddings.append(mean_embedding)\n",
    "    return torch.stack(seq_embeddings)\n",
    "\n",
    "class MiniCLIP_w_transformer_crossattn(pl.LightningModule):\n",
    "\n",
    "    def __init__(self, padding_value = -5000, embed_dimension=embedding_dimension, num_recycles=2):\n",
    "\n",
    "        super().__init__()\n",
    "        self.num_recycles = num_recycles # how many times you iteratively refine embeddings with self- and cross-attention (ALPHA-Fold-style recycling).\n",
    "        self.padding_value = padding_value\n",
    "        self.embed_dimension = 512\n",
    "\n",
    "        self.logit_scale = nn.Parameter(torch.tensor(math.log(1/0.07)))  # ~CLIP init\n",
    "\n",
    "        self.transformerencoder =  nn.TransformerEncoderLayer(\n",
    "            d_model=self.embed_dimension,\n",
    "            nhead=8,\n",
    "            dropout=0.1,\n",
    "            batch_first=True,\n",
    "            dim_feedforward=self.embed_dimension*2\n",
    "            )\n",
    " \n",
    "        self.norm = nn.LayerNorm(self.embed_dimension)  # For residual additions\n",
    "\n",
    "        self.cross_attn = nn.MultiheadAttention(\n",
    "            embed_dim=self.embed_dimension,\n",
    "            num_heads=8,\n",
    "            dropout=0.1,\n",
    "            batch_first=True\n",
    "        )\n",
    "\n",
    "        self.prot_embedder = nn.Sequential(\n",
    "            nn.Linear(self.embed_dimension, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 256),\n",
    "        )\n",
    "        \n",
    "    def forward(self, pep_input, prot_input, label=None, pep_int_mask=None, prot_int_mask=None, int_prob=None, mem_save=True): # , pep_tokens, prot_tokens\n",
    "\n",
    "        pep_mask = create_key_padding_mask(embeddings=pep_input, padding_value=self.padding_value)\n",
    "        prot_mask = create_key_padding_mask(embeddings=prot_input, padding_value=self.padding_value)\n",
    " \n",
    "        # Initialize residual states\n",
    "        pep_emb = pep_input.clone()\n",
    "        prot_emb = prot_input.clone()\n",
    " \n",
    "        for _ in range(self.num_recycles):\n",
    "\n",
    "            # Transformer encoding with residual\n",
    "            pep_trans = self.transformerencoder(self.norm(pep_emb), src_key_padding_mask=pep_mask)\n",
    "            prot_trans = self.transformerencoder(self.norm(prot_emb), src_key_padding_mask=prot_mask)\n",
    "\n",
    "            # Cross-attention with residual\n",
    "            pep_cross, _ = self.cross_attn(query=self.norm(pep_trans), key=self.norm(prot_trans), value=self.norm(prot_trans), key_padding_mask=prot_mask)\n",
    "            prot_cross, _ = self.cross_attn(query=self.norm(prot_trans), key=self.norm(pep_trans), value=self.norm(pep_trans), key_padding_mask=pep_mask)\n",
    "            \n",
    "            # Additive update with residual connection\n",
    "            pep_emb = pep_emb + pep_cross  \n",
    "            prot_emb = prot_emb + prot_cross\n",
    "\n",
    "        pep_seq_coding = create_mean_of_non_masked(pep_emb, pep_mask)\n",
    "        prot_seq_coding = create_mean_of_non_masked(prot_emb, prot_mask)\n",
    "        \n",
    "        # Use self-attention outputs for embeddings\n",
    "        pep_seq_coding = F.normalize(self.prot_embedder(pep_seq_coding), dim=-1)\n",
    "        prot_seq_coding = F.normalize(self.prot_embedder(prot_seq_coding), dim=-1)\n",
    " \n",
    "        if mem_save:\n",
    "            torch.cuda.empty_cache()\n",
    "        \n",
    "        scale = torch.exp(self.logit_scale).clamp(max=100.0)\n",
    "        logits = scale * (pep_seq_coding * prot_seq_coding).sum(dim=-1)\n",
    "        \n",
    "        return logits\n",
    "\n",
    "    def training_step(self, batch, device):\n",
    "        embedding_pep, embedding_prot, labels = batch\n",
    "        embedding_pep, embedding_prot = embedding_pep.to(device), embedding_prot.to(device)\n",
    "        \n",
    "        positive_logits = self.forward(embedding_pep, embedding_prot)\n",
    "        \n",
    "        # Negative indexes\n",
    "        rows, cols = torch.triu_indices(embedding_prot.size(0), embedding_prot.size(0), offset=1)         \n",
    "        \n",
    "        negative_logits = self(embedding_pep[rows,:,:], embedding_prot[cols,:,:], int_prob=0.0)\n",
    "\n",
    "        # loss of predicting partner using peptide\n",
    "        positive_loss = F.binary_cross_entropy_with_logits(positive_logits, torch.ones_like(positive_logits).to(device))\n",
    " \n",
    "        # loss of predicting peptide using partner\n",
    "        negative_loss =  F.binary_cross_entropy_with_logits(negative_logits, torch.zeros_like(negative_logits).to(device))\n",
    "        \n",
    "        loss = (positive_loss + negative_loss) / 2\n",
    " \n",
    "        # del partner_prediction_loss, peptide_prediction_loss, embedding_pep, embedding_prot\n",
    "        torch.cuda.empty_cache()\n",
    "        return loss\n",
    "\n",
    "    def validation_step_PPint(self, batch, device):\n",
    "        # Predict on random batches of training batch size\n",
    "        embedding_pep, embedding_prot, labels = batch\n",
    "        embedding_pep, embedding_prot = embedding_pep.to(device), embedding_prot.to(device)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "\n",
    "            positive_logits = self(embedding_pep, embedding_prot)\n",
    "            \n",
    "            # loss of predicting partner using peptide\n",
    "            positive_loss = F.binary_cross_entropy_with_logits(positive_logits, torch.ones_like(positive_logits).to(device))\n",
    "            \n",
    "            # Negaive indexes\n",
    "            rows, cols = torch.triu_indices(embedding_prot.size(0), embedding_prot.size(0), offset=1)\n",
    "            \n",
    "            negative_logits = self(embedding_pep[rows,:,:], embedding_prot[cols,:,:], int_prob=0.0)\n",
    "    \n",
    "            negative_loss =  F.binary_cross_entropy_with_logits(negative_logits, torch.zeros_like(negative_logits).to(device))\n",
    "\n",
    "            loss = (positive_loss + negative_loss) / 2\n",
    "           \n",
    "            logit_matrix = torch.zeros((embedding_pep.size(0),embedding_pep.size(0)),device=self.device)\n",
    "            logit_matrix[rows, cols] = negative_logits\n",
    "            logit_matrix[cols, rows] = negative_logits\n",
    "            \n",
    "            # Fill diagonal with positive scores\n",
    "            diag_indices = torch.arange(embedding_pep.size(0), device=self.device)\n",
    "            logit_matrix[diag_indices, diag_indices] = positive_logits.squeeze()\n",
    "\n",
    "            labels = torch.arange(embedding_prot.size(0)).to(self.device)\n",
    "            peptide_predictions = logit_matrix.argmax(dim=0)\n",
    "            peptide_ranks = logit_matrix.argsort(dim=0).diag() + 1\n",
    "            peptide_mrr = (peptide_ranks).float().pow(-1).mean()\n",
    "            \n",
    "            # partner_accuracy = partner_predictions.eq(labels).float().mean()\n",
    "            peptide_accuracy = peptide_predictions.eq(labels).float().mean()\n",
    "    \n",
    "            k = 3\n",
    "            peptide_topk_accuracy = torch.any((logit_matrix.topk(k, dim=0).indices - labels.reshape(1, -1)) == 0, dim=0).sum() / logit_matrix.shape[0]\n",
    "    \n",
    "            del logit_matrix,positive_logits,negative_logits,embedding_pep,embedding_prot\n",
    "\n",
    "            return loss, peptide_accuracy, peptide_topk_accuracy\n",
    "    \n",
    "    def validation_step_MetaDataset(self, batch, device):\n",
    "        embedding_binder, embedding_target, labels = batch\n",
    "        embedding_binder = embedding_binder.to(device)\n",
    "        embedding_target = embedding_target.to(device)\n",
    "        labels = labels.to(device).float()\n",
    "    \n",
    "        with torch.no_grad():\n",
    "            logits = self.forward(embedding_binder, embedding_target)\n",
    "            logits = logits.float()\n",
    "            loss = F.binary_cross_entropy_with_logits(logits.view(-1), labels.view(-1))\n",
    "            return logits, loss\n",
    "\n",
    "    def calculate_logit_matrix(self,embedding_pep,embedding_prot):\n",
    "        rows, cols = torch.triu_indices(embedding_pep.size(0), embedding_pep.size(0), offset=1)\n",
    "        \n",
    "        positive_logits = self(embedding_pep, embedding_prot)\n",
    "        negative_logits = self(embedding_pep[rows,:,:], embedding_prot[cols,:,:], int_prob=0.0)\n",
    "        \n",
    "        logit_matrix = torch.zeros((embedding_pep.size(0),embedding_pep.size(0)),device=self.device)\n",
    "        logit_matrix[rows, cols] = negative_logits\n",
    "        logit_matrix[cols, rows] = negative_logits\n",
    "        \n",
    "        diag_indices = torch.arange(embedding_pep.size(0), device=self.device)\n",
    "        logit_matrix[diag_indices, diag_indices] = positive_logits.squeeze()\n",
    "        \n",
    "        return logit_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "731e405f-cad9-4b82-8159-a9933a97b401",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = MiniCLIP_w_transformer_crossattn().to(\"cuda\")\n",
    "path = \"/work3/s232958/data/trained/original_architecture/cb12a130-9881-423e-88ba-9e18969fdb5f/cb12a130-9881-423e-88ba-9e18969fdb5f_checkpoint_6/cb12a130-9881-423e-88ba-9e18969fdb5f_checkpoint_epoch_6.pth\"\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "checkpoint = torch.load(path, map_location=device)\n",
    "# print(list(checkpoint[\"model_state_dict\"]))\n",
    "model.load_state_dict(checkpoint['model_state_dict'])\n",
    "model.to(device)\n",
    "\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50e27414-4d07-4b79-a7f8-b96e3011904f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_varlen(batch):\n",
    "    b_emb = torch.stack([x[0] for x in batch], dim=0)\n",
    "    t_emb = torch.stack([x[1] for x in batch], dim=0)\n",
    "    lbls = torch.tensor([x[2].float() for x in batch])\n",
    "    return b_emb, t_emb, lbls\n",
    "\n",
    "test_dataloader = DataLoader(testing_Dataset, batch_size=10, collate_fn=collate_varlen)\n",
    "non_dimers_dataloader = DataLoader(non_dimers_Dataset, batch_size=10, collate_fn=collate_varlen)\n",
    "validation_dataloader = DataLoader(validation_Dataset, batch_size=15, shuffle=False, drop_last = False)\n",
    "\n",
    "print(\"len(validation_Dataset):\", len(validation_Dataset))\n",
    "print(\"len(validation_dataloader.dataset):\", len(validation_dataloader.dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddec1cf2-8645-47e0-9074-062cdd7e91ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "ckpt = torch.load(path, map_location=\"cpu\")\n",
    "missing, unexpected = model.load_state_dict(ckpt[\"model_state_dict\"], strict=False)\n",
    "print(\"epoch in ckpt:\", ckpt.get(\"epoch\"))\n",
    "print(\"missing:\", missing)\n",
    "print(\"unexpected:\", unexpected)\n",
    "print(\"logit_scale:\", model.logit_scale.item())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cd705cc-605e-431d-b7b5-8e68137dadb6",
   "metadata": {},
   "source": [
    "### test-data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec237c15-9668-48ff-8f95-3c474601b9ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "interaction_scores_pos = []\n",
    "interaction_scores_neg = []    \n",
    "\n",
    "for batch in tqdm(test_dataloader, total=round(len(Df_test)/10), desc=\"#Iterating through batched data\"):\n",
    "    b_emb, t_emb, lbls = batch\n",
    "    embedding_pep = b_emb.to(\"cuda\")\n",
    "    embedding_prot = t_emb.to(\"cuda\")\n",
    "\n",
    "    with torch.no_grad():\n",
    "\n",
    "        rows, cols = torch.triu_indices(embedding_pep.size(0), embedding_pep.size(0), offset=1)\n",
    "        \n",
    "        positive_logits = model(embedding_pep, embedding_prot)\n",
    "        # negative_logits = self.forward(embedding_pep[rows,:,:], embedding_prot[cols,:,:], contacts_pep[rows,:,:], contacts_prot[cols,:,:], int_prob=0.0)\n",
    "        negative_logits = model(embedding_pep[rows,:,:], embedding_prot[cols,:,:], int_prob=0.0)\n",
    "        \n",
    "        # print(logit_matrix)\n",
    "        interaction_scores_pos.append(positive_logits)\n",
    "        interaction_scores_neg.append(negative_logits)\n",
    "\n",
    "# Convert list of tensors to single 1D tensors\n",
    "pos_logits = torch.cat(interaction_scores_pos).detach().cpu().numpy()\n",
    "neg_logits = torch.cat(interaction_scores_neg).detach().cpu().numpy()\n",
    "print(\"Positives:\", pos_logits.shape)\n",
    "print(\"Negatives:\", neg_logits.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b8210b6-7e6a-4f9b-8f63-5db6f32959f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(6, 4))\n",
    "plt.hist(pos_logits, bins=50, alpha=0.6, label=\"Positive pairs\", density=True)\n",
    "plt.hist(neg_logits, bins=50, alpha=0.6, label=\"Negative pairs\", density=True)\n",
    "\n",
    "plt.xlabel(\"Logit score\")\n",
    "plt.ylabel(\"Density\")\n",
    "plt.title(\"Distribution of Positive vs Negative Interaction Scores\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06320982-3625-4b6e-9102-c9e822781a75",
   "metadata": {},
   "source": [
    "#### non-dimers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e81400c-1e6d-4178-a22c-ee4e1129f9bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "interaction_scores = []\n",
    "for batch in tqdm(non_dimers_dataloader, total=round(len(non_dimers_Dataset)/10), desc=\"#Iterating through batched data\"):\n",
    "    b_emb, t_emb, lbls = batch\n",
    "    embedding_pep = b_emb.to(\"cuda\")\n",
    "    embedding_prot = t_emb.to(\"cuda\")\n",
    "\n",
    "    with torch.no_grad():\n",
    "\n",
    "        rows, cols = torch.triu_indices(embedding_pep.size(0), embedding_pep.size(0), offset=1)\n",
    "        \n",
    "        positive_logits = model(embedding_pep, embedding_prot)\n",
    "        # negative_logits = self.forward(embedding_pep[rows,:,:], embedding_prot[cols,:,:], contacts_pep[rows,:,:], contacts_prot[cols,:,:], int_prob=0.0)\n",
    "        negative_logits = model(embedding_pep[rows,:,:], embedding_prot[cols,:,:], int_prob=0.0)\n",
    "        \n",
    "        # print(logit_matrix)\n",
    "        interaction_scores_pos.append(positive_logits)\n",
    "        interaction_scores_neg.append(negative_logits)\n",
    "\n",
    "# Convert list of tensors to single 1D tensors\n",
    "pos_logits = torch.cat(interaction_scores_pos).detach().cpu().numpy()\n",
    "neg_logits = torch.cat(interaction_scores_neg).detach().cpu().numpy()\n",
    "print(\"Positives:\", pos_logits.shape)\n",
    "print(\"Negatives:\", neg_logits.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2ffcc8e-c045-46b6-a3a8-72b465e024af",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(6, 4))\n",
    "plt.hist(pos_logits, bins=50, alpha=0.6, label=\"Positive pairs\", density=True)\n",
    "plt.hist(neg_logits, bins=50, alpha=0.6, label=\"Negative pairs\", density=True)\n",
    "\n",
    "plt.xlabel(\"Logit score\")\n",
    "plt.ylabel(\"Density\")\n",
    "plt.title(\"Distribution of Positive vs Negative Interaction Scores\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2118cec-09af-4a52-b6e8-6ec2988fc06d",
   "metadata": {},
   "source": [
    "#### meta-binders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48470d28-b44d-4bd9-a31b-80d9b5d29846",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import metrics\n",
    "\n",
    "model.eval()\n",
    "all_logits, all_lbls = [], []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for batch in validation_dataloader:\n",
    "        embedding_binder, embedding_target, labels = batch\n",
    "        logits, _ = model.validation_step_MetaDataset(batch, device=\"cuda\")\n",
    "        all_logits.append(logits.detach().view(-1).cpu())\n",
    "        all_lbls.append(labels.detach().view(-1).cpu())\n",
    "        \n",
    "all_logits = torch.cat(all_logits).numpy()\n",
    "all_lbls   = torch.cat(all_lbls).numpy()\n",
    "fpr, tpr, thresholds = metrics.roc_curve(all_lbls, all_logits)\n",
    "meta_auroc = metrics.roc_auc_score(all_lbls, all_logits)\n",
    "meta_aupr  = metrics.average_precision_score(all_lbls, all_logits)\n",
    "# from sklearn.metrics import roc_auc_score\n",
    "print(\"AUROC:\", meta_auroc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfd218ac-5b5d-4aa4-8edd-3b72bbe074c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_lbls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b71dd4d2-a93e-4498-a4de-0944cd151be4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading batches\n",
    "interaction_scores = []\n",
    "\n",
    "for batch in tqdm(validation_dataloader, total = round(len(interaction_df_shuffled)/10),  desc= \"#Iterating through batched data\"):\n",
    "    b_emb, t_emb, lbls = batch\n",
    "    embedding_pep = b_emb.to(\"cuda\")\n",
    "    embedding_prot = t_emb.to(\"cuda\")\n",
    "\n",
    "    with torch.no_grad():\n",
    "        positive_logits = model(embedding_pep, embedding_prot)\n",
    "        interaction_scores.append(positive_logits.unsqueeze(0))\n",
    "\n",
    "predicted_interaction_scores = np.concatenate([batch_score.cpu().detach().numpy().reshape(-1,) for batch_score in interaction_scores])\n",
    "interaction_probabilities = np.concatenate([torch.sigmoid(batch_score[0]).cpu().numpy() for batch_score in interaction_scores])\n",
    "\n",
    "pos_logits, neg_logits = [], []\n",
    "for i, row in interaction_df_shuffled.iterrows():\n",
    "    logit = predicted_interaction_scores[i]\n",
    "    if row.binder == False:\n",
    "        neg_logits.append(logit)\n",
    "    elif row.binder == True:\n",
    "        pos_logits.append(logit)\n",
    "\n",
    "plt.figure(figsize=(5, 4))\n",
    "\n",
    "plt.hist(neg_logits, bins=50, alpha=0.6, label=\"Negative pairs\", density=True)\n",
    "plt.hist(pos_logits, bins=50, alpha=0.6, label=\"Positive pairs\", density=True)\n",
    "\n",
    "plt.xlabel(\"Logit score\")\n",
    "plt.ylabel(\"Density\")\n",
    "plt.title(\"Distribution of Positive vs Negative Interaction Scores\")\n",
    "\n",
    "# --- Simple grid behind ---\n",
    "plt.grid(True, linestyle=\"--\", alpha=0.5)\n",
    "\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5400ad8-7c06-4d92-af06-e233f076c838",
   "metadata": {},
   "outputs": [],
   "source": [
    "interaction_df_shuffled[\"inter_prob\"] = interaction_probabilities\n",
    "interaction_df_shuffled[\"pred_binder\"] = interaction_df_shuffled[\"inter_prob\"] >= 0.5\n",
    "interaction_df_shuffled[\"intr_scores\"] = predicted_interaction_scores\n",
    "\n",
    "pred_labels = interaction_probabilities >= 0.5\n",
    "true_labels = np.array(interaction_df_shuffled[\"binder\"])\n",
    "\n",
    "true_positives = ((pred_labels == 1) & (true_labels == 1)).sum().item()\n",
    "true_negatives = ((pred_labels == 0) & (true_labels == 0)).sum().item()\n",
    "false_positives = ((pred_labels == 1) & (true_labels == 0)).sum().item()\n",
    "false_negatives = ((pred_labels == 0) & (true_labels == 1)).sum().item()\n",
    "\n",
    "predicted_positives = true_positives + false_positives\n",
    "all_real_positives = true_positives + false_negatives\n",
    "\n",
    "print(classification_report(true_labels, pred_labels, digits = 4))\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(8, 4))\n",
    "\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=confusion_matrix(true_labels, pred_labels))\n",
    "disp.plot(ax=axes[0])\n",
    "axes[0].set_title(\"Confusion Matrix\")\n",
    "\n",
    "TPR = true_positives / (true_positives + true_negatives) # how good the model is at predicting the positive class when the actual outcome is positive.\n",
    "# sensitivity = true_positives / (true_positives + false_negatives) # the same as TPR\n",
    "FPR = false_positives / (false_positives + true_negatives) # how often a positive class is predicted when the actual outcome is negative.\n",
    "# specificity = true_negatives / (true_negatives + false_positives) # FPR = 1 - specificity\n",
    "\n",
    "fpr, tpr, thresholds = roc_curve(true_labels, interaction_probabilities)\n",
    "auc = roc_auc_score(true_labels, interaction_probabilities)\n",
    "print('AUC: %.3f' % auc)\n",
    "\n",
    "axes[1].plot(fpr, tpr, linewidth=2)\n",
    "axes[1].plot([0, 1], [0, 1], linestyle=\"--\", linewidth=1)  # diagonal reference\n",
    "axes[1].set_xlabel('False Positive Rate')\n",
    "axes[1].set_ylabel('True Positive Rate')\n",
    "axes[1].set_xlim(0, 1)\n",
    "axes[1].set_ylim(0, 1)\n",
    "axes[1].set_title('ROC Curve')\n",
    "\n",
    "# show the plot\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d9f27fa-2049-408a-8ff8-fcfe1ccb58ed",
   "metadata": {},
   "source": [
    "## ESM-IF + ESM-2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1699d735-cc61-4201-93d5-07033ed9be90",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CLIP_PPint_w_esmIF(Dataset):\n",
    "    def __init__(\n",
    "        self,\n",
    "        dframe,\n",
    "        paths,\n",
    "        embedding_dim_struct=512,\n",
    "        embedding_dim_seq=1280,\n",
    "        embedding_pad_value=-5000.0,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.dframe = dframe.copy()\n",
    "        self.embedding_dim_seq = embedding_dim_seq\n",
    "        self.embedding_dim_struct = embedding_dim_struct\n",
    "        self.emb_pad = embedding_pad_value\n",
    "\n",
    "        # lengths\n",
    "        self.max_blen_seq = self.dframe[\"seq_binder_len\"].max()\n",
    "        self.max_tlen_seq = self.dframe[\"seq_target_len\"].max()\n",
    "        self.max_blen_struct = self.dframe[\"pdb_binder_len\"].max()\n",
    "        self.max_tlen_struct = self.dframe[\"pdb_target_len\"].max()\n",
    "\n",
    "        # paths\n",
    "        self.seq_encodings_path, self.struct_encodings_path = paths\n",
    "\n",
    "        # index & storage\n",
    "        self.dframe.set_index(\"target_binder_id\", inplace=True)\n",
    "        self.accessions = self.dframe.index.astype(str).tolist()\n",
    "        self.name_to_row = {name: i for i, name in enumerate(self.accessions)}\n",
    "        self.samples = []\n",
    "\n",
    "        for accession in tqdm(self.accessions, total=len(self.accessions), desc=\"#Loading ESM2 embeddings and contacts\"):\n",
    "            parts = accession.split(\"_\") # e.g. accession 7S8T_5_F_7S8T_5_G\n",
    "            tgt_id = parts[0]+\"_\"+parts[2]\n",
    "            bnd_id = parts[-3]+\"_\"+parts[-1]\n",
    "\n",
    "            ### --- SEQ embeddings (pad to fixed lengths) --- ###\n",
    "            # laod embeddings\n",
    "            t_emb_seq = np.load(os.path.join(self.seq_encodings_path, f\"{tgt_id}.npy\"))     # [Lt, D]\n",
    "            b_emb_seq = np.load(os.path.join(self.seq_encodings_path, f\"{bnd_id}.npy\"))     # [Lb, D]\n",
    "            t_emb_struct = np.load(os.path.join(self.struct_encodings_path, f\"{tgt_id}.npy\"))     # [Lt, D]\n",
    "            b_emb_struct = np.load(os.path.join(self.struct_encodings_path, f\"{bnd_id}.npy\"))     # [Lb, D]\n",
    "\n",
    "            # quich check whether embedding dimmension is as it suppose to be\n",
    "            if t_emb_seq.shape[1] != self.embedding_dim_seq or b_emb_seq.shape[1] != self.embedding_dim_seq:\n",
    "                raise ValueError(\"Embedding dim mismatch with 'embedding_dim_seq'.\")\n",
    "            if t_emb_struct.shape[1] != self.embedding_dim_struct or b_emb_struct.shape[1] != self.embedding_dim_struct:\n",
    "                raise ValueError(\"Embedding dim mismatch with 'embedding_dim'.\")\n",
    "                \n",
    "            # add -5000 to all the padded target rows\n",
    "                ### SEQ_embeddings ###\n",
    "            if t_emb_seq.shape[0] < self.max_tlen_seq:\n",
    "                t_emb_seq = np.concatenate([t_emb_seq, np.full((self.max_tlen_seq - t_emb_seq.shape[0], t_emb_seq.shape[1]), self.emb_pad, dtype=t_emb_seq.dtype)], axis=0)\n",
    "            else:\n",
    "                t_emb_seq = t_emb_seq[: self.max_tlen_seq] # no padding was used\n",
    "            if b_emb_seq.shape[0] < self.max_blen_seq:\n",
    "                b_emb_seq = np.concatenate([b_emb_seq, np.full((self.max_blen_seq - b_emb_seq.shape[0], b_emb_seq.shape[1]), self.emb_pad, dtype=b_emb_seq.dtype)], axis=0)\n",
    "            else:\n",
    "                b_emb_seq = b_emb_seq[: self.max_blen_seq] # no padding was used\n",
    "\n",
    "                ### STRUCT_embeddings ###\n",
    "            if t_emb_struct.shape[0] < self.max_tlen_struct:\n",
    "                t_emb_struct = np.concatenate([t_emb_struct, np.full((self.max_tlen_struct - t_emb_struct.shape[0], t_emb_struct.shape[1]), self.emb_pad, dtype=t_emb_struct.dtype)], axis=0)\n",
    "            else:\n",
    "                t_emb_struct = t_emb_struct[: self.max_tlen_struct] # no padding was used\n",
    "            if b_emb_struct.shape[0] < self.max_blen_struct:\n",
    "                b_emb_struct = np.concatenate([b_emb_struct, np.full((self.max_blen_struct - b_emb_struct.shape[0], b_emb_struct.shape[1]), self.emb_pad, dtype=b_emb_struct.dtype)], axis=0)\n",
    "            else:\n",
    "                b_emb_struct = b_emb_struct[: self.max_blen_struct] # no padding was used\n",
    "\n",
    "            self.samples.append((b_emb_seq, t_emb_seq, b_emb_struct, t_emb_struct))\n",
    "\n",
    "    # ---- Dataset API ----\n",
    "    def __len__(self):\n",
    "        return len(self.samples)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        b_emb_seq, t_emb_seq, b_emb_struct, t_emb_struct = self.samples[idx]\n",
    "        b_emb_seq, t_emb_seq = torch.from_numpy(b_emb_seq).float(), torch.from_numpy(t_emb_seq).float()\n",
    "        b_emb_struct, t_emb_struct = torch.from_numpy(b_emb_struct).float(), torch.from_numpy(t_emb_struct).float()\n",
    "        label = torch.tensor(1, dtype=torch.float32)  # single scalar labe\n",
    "        return b_emb_seq, t_emb_seq, b_emb_struct, t_emb_struct, label\n",
    "\n",
    "    def _get_by_name(self, name):\n",
    "        # Single item -> return exactly what __getitem__ returns\n",
    "        if isinstance(name, str):\n",
    "            return self.__getitem__(self.name_to_row[name])\n",
    "        \n",
    "        # Multiple items -> fetch all\n",
    "        out = [self.__getitem__(self.name_to_row[n]) for n in list(name)]\n",
    "        b_emb_seq_list, t_emb_seq_list, b_emb_struct_list, t_emb_struct_list, lbl_list = zip(*out)\n",
    "    \n",
    "        # Stack embeddings\n",
    "        b_emb_seq  = torch.stack([torch.as_tensor(x) for x in b_emb_seq_list],  dim=0)  # [B, ...]\n",
    "        t_emb_seq  = torch.stack([torch.as_tensor(x) for x in t_emb_seq_list],  dim=0)  # [B, ...]\n",
    "        \n",
    "        b_emb_struct  = torch.stack([torch.as_tensor(x) for x in b_emb_struct_list],  dim=0)  # [B, ...]\n",
    "        t_emb_struct  = torch.stack([torch.as_tensor(x) for x in t_emb_struct_list],  dim=0)  # [B, ...]\n",
    "    \n",
    "        # Stack labels\n",
    "        labels = torch.stack(lbl_list)  # [B]\n",
    "    \n",
    "        return b_emb_seq, t_emb_seq, b_emb_struct, t_emb_struct, labels\n",
    "\n",
    "emb_seq_path = \"/work3/s232958/data/PPint_DB/embeddings_esm2\"\n",
    "emb_struct_path = \"/work3/s232958/data/PPint_DB/esmif_embeddings_noncanonical\"\n",
    "\n",
    "testing_Dataset = CLIP_PPint_w_esmIF(\n",
    "    Df_test,\n",
    "    paths=[emb_seq_path, emb_struct_path],\n",
    "    embedding_dim_seq=1280,\n",
    "    embedding_dim_struct=512\n",
    ")\n",
    "\n",
    "### Getting indeces of non-dimers\n",
    "non_dimers_Df = Df_test[~Df_test[\"dimer\"]]\n",
    "indices_non_dimers = Df_test[~Df_test[\"dimer\"]].index.tolist()\n",
    "\n",
    "non_dimers_Dataset = CLIP_PPint_w_esmIF(\n",
    "    non_dimers_Df,\n",
    "    paths=[emb_seq_path, emb_struct_path],\n",
    "    embedding_dim_seq=1280,\n",
    "    embedding_dim_struct=512\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "119cec72-48f1-468e-8027-b4f00b77185a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CLIP_ESM2_ESMIF(Dataset):\n",
    "    def __init__(\n",
    "        self,\n",
    "        dframe,\n",
    "        paths,\n",
    "        seq_embedding_dim=1280,\n",
    "        struct_embedding_dim=512,\n",
    "        embedding_pad_value=-5000.0,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.dframe = dframe.copy()\n",
    "        self.embedding_dim_seq = int(seq_embedding_dim)\n",
    "        self.embedding_dim_struct = int(struct_embedding_dim)\n",
    "        self.emb_pad = float(embedding_pad_value)\n",
    "\n",
    "        # lengths\n",
    "        self.max_blen_seq = self.dframe[\"seq_len_binder\"].max()\n",
    "        self.max_tlen_seq = self.dframe[\"seq_len_target\"].max()\n",
    "        self.max_blen_struct = self.dframe[\"pdb_len_binder\"].max()\n",
    "        self.max_tlen_struct = self.dframe[\"pdb_len_target\"].max()\n",
    "\n",
    "        # paths\n",
    "        self.seq_bembed, self.seq_tembed, self.struct_bembed, self.struct_tembed = paths\n",
    "\n",
    "        # index & storage\n",
    "        self.dframe.set_index(\"binder_id\", inplace=True)\n",
    "        self.accessions = self.dframe.index.astype(str).tolist()\n",
    "        self.name_to_row = {name: i for i, name in enumerate(self.accessions)}\n",
    "        self.samples = []\n",
    "\n",
    "        for accession in tqdm(self.accessions, total=len(self.accessions), desc=\"#Loading ESM2 embeddings and contacts\"):\n",
    "            lbl = torch.tensor(int(self.dframe.loc[accession, \"binder\"]))\n",
    "            parts = accession.split(\"_\") # e.g. accession 7S8T_5_F_7S8T_5_G\n",
    "            tgt_id = \"_\".join(parts[:-1])\n",
    "            bnd_id = accession\n",
    "\n",
    "            ### --- SEQ embeddings (pad to fixed lengths) --- ###\n",
    "            # laod embeddings\n",
    "            b_emb_seq = np.load(os.path.join(self.seq_bembed, f\"{bnd_id}.npy\"))     # [Lb, D]\n",
    "            t_emb_seq = np.load(os.path.join(self.seq_tembed, f\"{tgt_id}.npy\"))     # [Lt, D]\n",
    "            b_emb_struct = np.load(os.path.join(self.struct_bembed, f\"{bnd_id}.npy\"))     # [Lb, D]\n",
    "            t_emb_struct = np.load(os.path.join(self.struct_tembed, f\"{tgt_id}.npy\"))     # [Lt, D]\n",
    "\n",
    "            # quich check whether embedding dimmension is as it suppose to be\n",
    "            if t_emb_seq.shape[1] != self.embedding_dim_seq or b_emb_seq.shape[1] != self.embedding_dim_seq:\n",
    "                raise ValueError(\"Embedding dim mismatch with 'embedding_dim_seq'.\")\n",
    "            if t_emb_struct.shape[1] != self.embedding_dim_struct or b_emb_struct.shape[1] != self.embedding_dim_struct:\n",
    "                raise ValueError(\"Embedding dim mismatch with 'embedding_dim'.\")\n",
    "                \n",
    "            # add -5000 to all the padded target rows\n",
    "                ### SEQ_embeddings ###\n",
    "            if t_emb_seq.shape[0] < self.max_tlen_seq:\n",
    "                t_emb_seq = np.concatenate([t_emb_seq, np.full((self.max_tlen_seq - t_emb_seq.shape[0], t_emb_seq.shape[1]), self.emb_pad, dtype=t_emb_seq.dtype)], axis=0)\n",
    "            else:\n",
    "                t_emb_seq = t_emb_seq[: self.max_tlen_seq] # no padding was used\n",
    "            if b_emb_seq.shape[0] < self.max_blen_seq:\n",
    "                b_emb_seq = np.concatenate([b_emb_seq, np.full((self.max_blen_seq - b_emb_seq.shape[0], b_emb_seq.shape[1]), self.emb_pad, dtype=b_emb_seq.dtype)], axis=0)\n",
    "            else:\n",
    "                b_emb_seq = b_emb_seq[: self.max_blen_seq] # no padding was used\n",
    "\n",
    "                ### STRUCT_embeddings ###\n",
    "            if t_emb_struct.shape[0] < self.max_tlen_struct:\n",
    "                t_emb_struct = np.concatenate([t_emb_struct, np.full((self.max_tlen_struct - t_emb_struct.shape[0], t_emb_struct.shape[1]), self.emb_pad, dtype=t_emb_struct.dtype)], axis=0)\n",
    "            else:\n",
    "                t_emb_struct = t_emb_struct[: self.max_tlen_struct] # no padding was used\n",
    "            if b_emb_struct.shape[0] < self.max_blen_struct:\n",
    "                b_emb_struct = np.concatenate([b_emb_struct, np.full((self.max_blen_struct - b_emb_struct.shape[0], b_emb_struct.shape[1]), self.emb_pad, dtype=b_emb_struct.dtype)], axis=0)\n",
    "            else:\n",
    "                b_emb_struct = b_emb_struct[: self.max_blen_struct] # no padding was used\n",
    "\n",
    "            self.samples.append((b_emb_seq, t_emb_seq, b_emb_struct, t_emb_struct, lbl))\n",
    "        \n",
    "    # ---- Dataset API ----\n",
    "    def __len__(self):\n",
    "        return len(self.samples)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        b_emb_seq, t_emb_seq, b_emb_struct, t_emb_struct, lbls = self.samples[idx]\n",
    "        b_emb_seq, t_emb_seq = torch.from_numpy(b_emb_seq).float(), torch.from_numpy(t_emb_seq).float()\n",
    "        b_emb_struct, t_emb_struct = torch.from_numpy(b_emb_struct).float(), torch.from_numpy(t_emb_struct).float()\n",
    "        return b_emb_seq, t_emb_seq, b_emb_struct, t_emb_struct, lbls\n",
    "\n",
    "    def _get_by_name(self, name):\n",
    "        # Single item -> return exactly what __getitem__ returns\n",
    "        if isinstance(name, str):\n",
    "            return self.__getitem__(self.name_to_row[name])\n",
    "        \n",
    "        # Multiple items -> fetch all\n",
    "        out = [self.__getitem__(self.name_to_row[n]) for n in list(name)]\n",
    "        b_emb_seq_list, t_emb_seq_list, b_emb_struct_list, t_emb_struct_list, lbl_list = zip(*out)\n",
    "    \n",
    "        # Stack embeddings\n",
    "        b_emb_seq  = torch.stack([torch.as_tensor(x) for x in b_emb_seq_list],  dim=0)  # [B, ...]\n",
    "        t_emb_seq  = torch.stack([torch.as_tensor(x) for x in t_emb_seq_list],  dim=0)  # [B, ...]\n",
    "        \n",
    "        b_emb_struct  = torch.stack([torch.as_tensor(x) for x in b_emb_struct_list],  dim=0)  # [B, ...]\n",
    "        t_emb_struct  = torch.stack([torch.as_tensor(x) for x in t_emb_struct_list],  dim=0)  # [B, ...]\n",
    "    \n",
    "        # Stack labels\n",
    "        labels = torch.stack(lbl_list)  # [B]\n",
    "    \n",
    "        return b_emb_seq, t_emb_seq, b_emb_struct, t_emb_struct, labels\n",
    "\n",
    "esm2_path_binders = \"/work3/s232958/data/meta_analysis/embeddings_esm2_binders\"\n",
    "esm2_path_targets = \"/work3/s232958/data/meta_analysis/embeddings_esm2_targets\"\n",
    "\n",
    "## Contact maps paths\n",
    "esmIF_path_binders = \"/work3/s232958/data/meta_analysis/esmif_embeddings_binders\"\n",
    "esmIF_path_targets = \"/work3/s232958/data/meta_analysis/esmif_embeddings_targets\"\n",
    "\n",
    "validation_Dataset = CLIP_ESM2_ESMIF(\n",
    "    interaction_df_shuffled,\n",
    "    paths=[esm2_path_binders, esm2_path_targets, esmIF_path_binders, esmIF_path_targets],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "998e80b1-841a-4c29-8f1d-0e7960ccdfaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataloader = DataLoader(testing_Dataset, batch_size=10)\n",
    "non_dimers_dataloader = DataLoader(non_dimers_Dataset, batch_size=10)\n",
    "validation_dataloader = DataLoader(validation_Dataset, batch_size=10, shuffle=False, drop_last = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58559a9b-d826-4238-ac79-0ac93ed8fca4",
   "metadata": {},
   "source": [
    "### Pre-trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c8034700-7d6e-48dd-95b5-724365980cbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_key_padding_mask(embeddings, padding_value=-5000, offset=10):\n",
    "    return (embeddings < (padding_value + offset)).all(dim=-1)\n",
    "\n",
    "def create_mean_of_non_masked(embeddings, padding_mask):\n",
    "    # Use masked select and mean to compute the mean of non-masked elements\n",
    "    # embeddings should be of shape (batch_size, seq_len, features)\n",
    "    seq_embeddings = []\n",
    "    for i in range(embeddings.shape[0]): # looping over all batch elements\n",
    "        non_masked_embeddings = embeddings[i][~padding_mask[i]] # shape [num_real_tokens, features]\n",
    "        if len(non_masked_embeddings) == 0:\n",
    "            print(\"You are masking all positions when creating sequence representation\")\n",
    "            sys.exit(1)\n",
    "        mean_embedding = non_masked_embeddings.mean(dim=0) # sequence is represented by the single vecotr [1152] [features]\n",
    "        seq_embeddings.append(mean_embedding)\n",
    "    return torch.stack(seq_embeddings)\n",
    "\n",
    "class MiniCLIP_ESM2_ESMIF(pl.LightningModule):\n",
    "\n",
    "    def __init__(self, padding_value = -5000, seq_embed_dimension=1280, struct_embed_dimension=512, num_recycles=2):\n",
    "\n",
    "        super().__init__()\n",
    "        self.num_recycles = num_recycles # how many times you iteratively refine embeddings with self- and cross-attention (ALPHA-Fold-style recycling).\n",
    "        self.padding_value = padding_value\n",
    "        self.seq_embed_dimension = seq_embed_dimension\n",
    "        self.struct_embed_dimension = struct_embed_dimension\n",
    "\n",
    "        self.logit_scale = nn.Parameter(torch.tensor(math.log(1/0.07)))  # ~CLIP init\n",
    "        self.struct_alpha = nn.Parameter(torch.tensor(0.0))\n",
    "\n",
    "        # --- SEQUENCE embeddings --- #\n",
    "        \n",
    "        self.norm_seq = nn.LayerNorm(self.seq_embed_dimension)  # For residual additions\n",
    "        \n",
    "        self.seq_encoder =  nn.TransformerEncoderLayer(\n",
    "            d_model=self.seq_embed_dimension,\n",
    "            nhead=8,\n",
    "            dropout=0.1,\n",
    "            batch_first=True,\n",
    "            dim_feedforward=self.seq_embed_dimension\n",
    "            )\n",
    "\n",
    "        self.seq_cross_attn = nn.MultiheadAttention(\n",
    "            embed_dim=self.seq_embed_dimension,\n",
    "            num_heads=8,\n",
    "            dropout=0.1,\n",
    "            batch_first=True\n",
    "        )\n",
    "\n",
    "        self.seq_proj = nn.Sequential(\n",
    "            nn.Linear(self.seq_embed_dimension, 640),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(640, 320),\n",
    "        )\n",
    "\n",
    "        # --- STRUCTURE embeddings --- #\n",
    "\n",
    "        self.norm_struct = nn.LayerNorm(self.seq_embed_dimension)  # For residual additions\n",
    "        \n",
    "        self.initial_stuct_proj = nn.Linear(self.struct_embed_dimension, self.seq_embed_dimension)\n",
    "\n",
    "        self.struct_encoder =  nn.TransformerEncoderLayer(\n",
    "            d_model=self.seq_embed_dimension,\n",
    "            nhead=8,\n",
    "            dropout=0.1,\n",
    "            batch_first=True,\n",
    "            dim_feedforward=self.seq_embed_dimension\n",
    "            )\n",
    "\n",
    "        self.struct_to_seq_attn = nn.MultiheadAttention(\n",
    "            embed_dim=self.seq_embed_dimension,\n",
    "            num_heads=8,\n",
    "            dropout=0.1,\n",
    "            batch_first=True\n",
    "        )\n",
    "        \n",
    "    def forward(self, pep_seq_emb, prot_seq_emb, pep_struct_emb, prot_struct_emb, label=None, pep_int_mask=None, prot_int_mask=None, int_prob=None, mem_save=True):\n",
    "        \n",
    "        # Key padding masks (True = pad -> to be ignored by attention)\n",
    "        pep_seq_mask = create_key_padding_mask(embeddings = pep_seq_emb, padding_value = self.padding_value).to(device)   # [B, Lp]\n",
    "        prot_seq_mask = create_key_padding_mask(embeddings = prot_seq_emb, padding_value = self.padding_value).to(device)    # [B, Lt]\n",
    "        \n",
    "        pep_struct_mask = create_key_padding_mask(embeddings = pep_struct_emb, padding_value = self.padding_value).to(device)     # [B, Lp_cm]\n",
    "        prot_struct_mask = create_key_padding_mask(embeddings = prot_struct_emb, padding_value = self.padding_value).to(device)     # [B, Lt_cm]\n",
    "    \n",
    "        # Residual states\n",
    "        pep_seq_emb = pep_seq_emb.to(device)\n",
    "        prot_seq_emb = prot_seq_emb.to(device)\n",
    "        pep_struct_emb = pep_struct_emb.to(device)\n",
    "        prot_struct_emb = prot_struct_emb.to(device)\n",
    "    \n",
    "        for _ in range(self.num_recycles):\n",
    "            \n",
    "            # --- Self-attention encoders (sequence streams) ---\n",
    "            pep_trans_seq = self.seq_encoder(self.norm_seq(pep_seq_emb), src_key_padding_mask=pep_seq_mask)   # [B, Lp, E]\n",
    "            prot_trans_seq = self.seq_encoder(self.norm_seq(prot_seq_emb), src_key_padding_mask=prot_seq_mask)  # [B, Lt, E]\n",
    "    \n",
    "            # --- Self-attention encoders (structure streams) ---\n",
    "            pep_trans_str = self.struct_encoder(self.norm_struct(self.initial_stuct_proj(pep_struct_emb)), src_key_padding_mask=pep_struct_mask)   # [B, Lp_cm, E]\n",
    "            prot_trans_str = self.struct_encoder(self.norm_struct(self.initial_stuct_proj(prot_struct_emb)), src_key_padding_mask=prot_struct_mask)  # [B, Lt_cm, E]\n",
    "\n",
    "            # --- Cross-attend to structures ---\n",
    "            pep_struct_upd, _ = self.struct_to_seq_attn(query=self.norm_seq(pep_trans_seq), key=self.norm_struct(pep_trans_str), value=self.norm_struct(pep_trans_str), key_padding_mask=pep_struct_mask)\n",
    "            prot_struct_upd, _ = self.struct_to_seq_attn(query=self.norm_seq(prot_trans_seq), key=self.norm_struct(prot_trans_str), value=self.norm_struct(prot_trans_str), key_padding_mask=prot_struct_mask)\n",
    "\n",
    "            pep_trans_seq  = pep_trans_seq  + self.struct_alpha.tanh() * pep_struct_upd    # [B, Lp, E]\n",
    "            prot_trans_seq = prot_trans_seq + self.struct_alpha.tanh() * prot_struct_upd    # [B, Lt, E]\n",
    "    \n",
    "            # --- Cross-attend binder vs target ---\n",
    "            pep_cross,  _  = self.seq_cross_attn(query=self.norm_seq(pep_trans_seq), key=self.norm_seq(prot_trans_seq), value=self.norm_seq(prot_trans_seq), key_padding_mask=prot_seq_mask)\n",
    "            prot_cross, _  = self.seq_cross_attn(query=self.norm_seq(prot_trans_seq), key=self.norm_seq(pep_trans_seq), value=self.norm_seq(pep_trans_seq), key_padding_mask=pep_seq_mask)\n",
    "    \n",
    "            # --- Residual updates ---\n",
    "            pep_seq_emb = pep_seq_emb + pep_cross\n",
    "            prot_seq_emb = prot_seq_emb + prot_cross\n",
    "    \n",
    "        # Pool (mean over non-masked positions)\n",
    "        pep_seq_coding   = create_mean_of_non_masked(pep_seq_emb, pep_seq_mask)\n",
    "        prot_seq_coding  = create_mean_of_non_masked(prot_seq_emb, prot_seq_mask)\n",
    "\n",
    "        # Projections + L2-normalize\n",
    "        pep_full   = F.normalize(self.seq_proj(pep_seq_coding),   dim=-1)\n",
    "        prot_full  = F.normalize(self.seq_proj(prot_seq_coding),  dim=-1)\n",
    "    \n",
    "        if mem_save:\n",
    "            torch.cuda.empty_cache()\n",
    "    \n",
    "        scale  = torch.exp(self.logit_scale).clamp(max=100.0)\n",
    "        logits = scale * (pep_full * prot_full).sum(dim=-1)  # [B]\n",
    "        \n",
    "        return logits\n",
    "\n",
    "    def training_step(self, batch, device):\n",
    "        pep_seq_emb, prot_seq_emb, pep_struct_emb, prot_struct_emb, labels = batch\n",
    "\n",
    "        # loss of predicting partner using peptide\n",
    "        positive_logits = self.forward(pep_seq_emb, prot_seq_emb, pep_struct_emb, prot_struct_emb)\n",
    "        positive_loss = F.binary_cross_entropy_with_logits(positive_logits, torch.ones_like(positive_logits).to(device)) # F.binary_cross_entropy_with_logits does sigmoid transfromation inside, excepts data, labels\n",
    "        \n",
    "        # Negative indexes\n",
    "        rows, cols = torch.triu_indices(pep_seq_emb.size(0), pep_seq_emb.size(0), offset=1) # upper triangle\n",
    "        \n",
    "        # loss of predicting peptide using partner\n",
    "        # negative_logits = self.forward(embedding_pep[rows,:,:], embedding_prot[cols,:,:], contacts_pep[rows,:,:], contacts_prot[cols,:,:], int_prob=0.0)\n",
    "        negative_logits = self.forward(pep_seq_emb[rows,:,:], prot_seq_emb[cols,:,:], pep_struct_emb[rows,:,:], prot_struct_emb[cols,:,:], int_prob=0.0)\n",
    "        negative_loss =  F.binary_cross_entropy_with_logits(negative_logits, torch.zeros_like(negative_logits).to(device))\n",
    "        \n",
    "        loss = (positive_loss + negative_loss) / 2\n",
    " \n",
    "        torch.cuda.empty_cache()\n",
    "        return loss\n",
    "\n",
    "    def validation_step_PPint(self, batch, device):\n",
    "        # Predict on random batches of training batch size\n",
    "        pep_seq_emb, prot_seq_emb, pep_struct_emb, prot_struct_emb, labels = batch\n",
    "        pep_seq_emb, prot_seq_emb, pep_struct_emb, prot_struct_emb = pep_seq_emb.to(device), prot_seq_emb.to(device), pep_struct_emb.to(device), prot_struct_emb.to(device)\n",
    "        # contacts_pep, contacts_prot = contacts_pep.to(device), contacts_prot.to(device)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "\n",
    "            positive_logits = self.forward(pep_seq_emb, prot_seq_emb, pep_struct_emb, prot_struct_emb)\n",
    "            \n",
    "            # loss of predicting partner using peptide\n",
    "            positive_loss = F.binary_cross_entropy_with_logits(positive_logits, torch.ones_like(positive_logits).to(device))\n",
    "            \n",
    "            # Negaive indexes\n",
    "            rows, cols = torch.triu_indices(pep_seq_emb.size(0), pep_seq_emb.size(0), offset=1)\n",
    "            \n",
    "            negative_logits = self.forward(pep_seq_emb[rows,:,:], prot_seq_emb[cols,:,:], pep_struct_emb[rows,:,:], prot_struct_emb[cols,:,:], int_prob=0.0)\n",
    "            negative_loss =  F.binary_cross_entropy_with_logits(negative_logits, torch.zeros_like(negative_logits).to(device))\n",
    "\n",
    "            loss = (positive_loss + negative_loss) / 2\n",
    "\n",
    "            logit_matrix = torch.zeros((pep_seq_emb.size(0), pep_seq_emb.size(0)),device=self.device)\n",
    "            logit_matrix[rows, cols] = negative_logits\n",
    "            logit_matrix[cols, rows] = negative_logits\n",
    "            \n",
    "            # Fill diagonal with positive scores\n",
    "            diag_indices = torch.arange(pep_seq_emb.size(0), device=self.device)\n",
    "            logit_matrix[diag_indices, diag_indices] = positive_logits.squeeze()\n",
    "\n",
    "            labels = torch.arange(pep_seq_emb.size(0)).to(self.device)\n",
    "            peptide_predictions = logit_matrix.argmax(dim=0)\n",
    "            peptide_ranks = logit_matrix.argsort(dim=0).diag() + 1\n",
    "            peptide_mrr = (peptide_ranks).float().pow(-1).mean()\n",
    "            \n",
    "            # partner_accuracy = partner_predictions.eq(labels).float().mean()\n",
    "            peptide_accuracy = peptide_predictions.eq(labels).float().mean()\n",
    "    \n",
    "            return loss, peptide_accuracy\n",
    "    \n",
    "    def validation_step_MetaDataset(self, batch, device):\n",
    "        pep_seq_emb, prot_seq_emb, pep_struct_emb, prot_struct_emb, labels = batch\n",
    "        pep_seq_emb, prot_seq_emb = pep_seq_emb.to(device), prot_seq_emb.to(device) \n",
    "        pep_struct_emb, prot_struct_emb = pep_struct_emb.to(device), prot_struct_emb.to(device)\n",
    "        # contacts_pep, contacts_prot = contacts_pep.to(device), contacts_prot.to(device)\n",
    "        labels = labels.to(device).float()\n",
    "    \n",
    "        with torch.no_grad():\n",
    "            logits = self.forward(pep_seq_emb, prot_seq_emb, pep_struct_emb, prot_struct_emb).float()\n",
    "            loss = F.binary_cross_entropy_with_logits(logits.view(-1), labels.view(-1))\n",
    "            return logits, loss\n",
    "\n",
    "    def calculate_logit_matrix(self, pep_seq_emb, prot_seq_emb, pep_struct_emb, prot_struct_emb):\n",
    "        \n",
    "        rows, cols = torch.triu_indices(pep_seq_emb.size(0), pep_seq_emb.size(0), offset=1)\n",
    "        positive_logits = self.forward(pep_seq_emb, prot_seq_emb, pep_struct_emb, prot_struct_emb)\n",
    "        negative_logits = self.forward(pep_seq_emb[rows,:,:], prot_seq_emb[cols,:,:], pep_struct_emb[rows,:,:], prot_struct_emb[cols,:,:], int_prob=0.0)\n",
    "        \n",
    "        logit_matrix = torch.zeros((pep_seq_emb.size(0),pep_seq_emb.size(0)),device=self.device)\n",
    "        logit_matrix[rows, cols] = negative_logits\n",
    "        logit_matrix[cols, rows] = negative_logits\n",
    "        \n",
    "        diag_indices = torch.arange(pep_seq_emb.size(0), device=self.device)\n",
    "        logit_matrix[diag_indices, diag_indices] = positive_logits.squeeze()\n",
    "        \n",
    "        return logit_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9aa713e4-d4de-46dd-9876-06244f1f76d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-0.00883762538433075\n",
      "tensor(0.4978)\n"
     ]
    }
   ],
   "source": [
    "model = MiniCLIP_ESM2_ESMIF().to(\"cuda\")\n",
    "path = \"/work3/s232958/data/trained/with_structure/2dca0ab0-422d-4567-8970-30ab1504f5b2/9644ac4d-47d5-4c18-a6f4-285950dbfb97_checkpoint_9/9644ac4d-47d5-4c18-a6f4-285950dbfb97_checkpoint_epoch_9.pth\"\n",
    "checkpoint = torch.load(path, weights_only=False, map_location=torch.device('cpu'))\n",
    "# print(list(checkpoint[\"model_state_dict\"]))\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "model.load_state_dict(checkpoint['model_state_dict'])\n",
    "model.to(device)\n",
    "model.eval()\n",
    "\n",
    "alpha_raw = model.struct_alpha.item()\n",
    "print(alpha_raw)\n",
    "print(torch.sigmoid(torch.tensor(alpha_raw)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f949341f-e089-44c0-9c28-e599d60d589b",
   "metadata": {},
   "source": [
    "#### Sigmoid(0.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7634a635-7b11-40fd-9c07-17f13f4a5872",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_key_padding_mask(embeddings, padding_value=-5000, offset=10):\n",
    "    return (embeddings < (padding_value + offset)).all(dim=-1)\n",
    "\n",
    "def create_mean_of_non_masked(embeddings, padding_mask):\n",
    "    # Use masked select and mean to compute the mean of non-masked elements\n",
    "    # embeddings should be of shape (batch_size, seq_len, features)\n",
    "    seq_embeddings = []\n",
    "    for i in range(embeddings.shape[0]): # looping over all batch elements\n",
    "        non_masked_embeddings = embeddings[i][~padding_mask[i]] # shape [num_real_tokens, features]\n",
    "        if len(non_masked_embeddings) == 0:\n",
    "            print(\"You are masking all positions when creating sequence representation\")\n",
    "            sys.exit(1)\n",
    "        mean_embedding = non_masked_embeddings.mean(dim=0) # sequence is represented by the single vecotr [1152] [features]\n",
    "        seq_embeddings.append(mean_embedding)\n",
    "    return torch.stack(seq_embeddings)\n",
    "\n",
    "class MiniCLIP_ESM2_ESMIF(pl.LightningModule):\n",
    "\n",
    "    def __init__(self, padding_value = -5000, seq_embed_dimension=1280, struct_embed_dimension=512, num_recycles=2):\n",
    "\n",
    "        super().__init__()\n",
    "        self.num_recycles = num_recycles # how many times you iteratively refine embeddings with self- and cross-attention (ALPHA-Fold-style recycling).\n",
    "        self.padding_value = padding_value\n",
    "        self.seq_embed_dimension = seq_embed_dimension\n",
    "        self.struct_embed_dimension = struct_embed_dimension\n",
    "\n",
    "        self.logit_scale = nn.Parameter(torch.tensor(math.log(1/0.07)))  # ~CLIP init\n",
    "        self.struct_alpha = nn.Parameter(torch.tensor(0.0)) # Sigmoid(0) = 0.5\n",
    "\n",
    "        # --- SEQUENCE embeddings --- #\n",
    "        \n",
    "        self.norm_seq = nn.LayerNorm(self.seq_embed_dimension)  # For residual additions\n",
    "        \n",
    "        self.seq_encoder =  nn.TransformerEncoderLayer(\n",
    "            d_model=self.seq_embed_dimension,\n",
    "            nhead=8,\n",
    "            dropout=0.1,\n",
    "            batch_first=True,\n",
    "            dim_feedforward=self.seq_embed_dimension\n",
    "            )\n",
    "\n",
    "        self.seq_cross_attn = nn.MultiheadAttention(\n",
    "            embed_dim=self.seq_embed_dimension,\n",
    "            num_heads=8,\n",
    "            dropout=0.1,\n",
    "            batch_first=True\n",
    "        )\n",
    "\n",
    "        self.seq_proj = nn.Sequential(\n",
    "            nn.Linear(self.seq_embed_dimension, 640),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(640, 320),\n",
    "        )\n",
    "\n",
    "        # --- STRUCTURE embeddings --- #\n",
    "\n",
    "        self.norm_struct = nn.LayerNorm(self.seq_embed_dimension)  # For residual additions\n",
    "        \n",
    "        self.initial_stuct_proj = nn.Linear(self.struct_embed_dimension, self.seq_embed_dimension)\n",
    "\n",
    "        self.struct_encoder =  nn.TransformerEncoderLayer(\n",
    "            d_model=self.seq_embed_dimension,\n",
    "            nhead=8,\n",
    "            dropout=0.1,\n",
    "            batch_first=True,\n",
    "            dim_feedforward=self.seq_embed_dimension\n",
    "            )\n",
    "\n",
    "        self.struct_to_seq_attn = nn.MultiheadAttention(\n",
    "            embed_dim=self.seq_embed_dimension,\n",
    "            num_heads=8,\n",
    "            dropout=0.1,\n",
    "            batch_first=True\n",
    "        )\n",
    "        \n",
    "    def forward(self, pep_seq_emb, prot_seq_emb, pep_struct_emb, prot_struct_emb, label=None, pep_int_mask=None, prot_int_mask=None, int_prob=None, mem_save=True):\n",
    "        \n",
    "        # Key padding masks (True = pad -> to be ignored by attention)\n",
    "        pep_seq_mask = create_key_padding_mask(embeddings = pep_seq_emb, padding_value = self.padding_value).to(device)   # [B, Lp]\n",
    "        prot_seq_mask = create_key_padding_mask(embeddings = prot_seq_emb, padding_value = self.padding_value).to(device)    # [B, Lt]\n",
    "        \n",
    "        pep_struct_mask = create_key_padding_mask(embeddings = pep_struct_emb, padding_value = self.padding_value).to(device)     # [B, Lp_cm]\n",
    "        prot_struct_mask = create_key_padding_mask(embeddings = prot_struct_emb, padding_value = self.padding_value).to(device)     # [B, Lt_cm]\n",
    "    \n",
    "        # Residual states\n",
    "        pep_seq_emb = pep_seq_emb.to(device)\n",
    "        prot_seq_emb = prot_seq_emb.to(device)\n",
    "        pep_struct_emb = pep_struct_emb.to(device)\n",
    "        prot_struct_emb = prot_struct_emb.to(device)\n",
    "    \n",
    "        for _ in range(self.num_recycles):\n",
    "            \n",
    "            # --- Self-attention encoders (sequence streams) ---\n",
    "            pep_trans_seq = self.seq_encoder(self.norm_seq(pep_seq_emb), src_key_padding_mask=pep_seq_mask)   # [B, Lp, E]\n",
    "            prot_trans_seq = self.seq_encoder(self.norm_seq(prot_seq_emb), src_key_padding_mask=prot_seq_mask)  # [B, Lt, E]\n",
    "    \n",
    "            # --- Self-attention encoders (structure streams) ---\n",
    "            pep_trans_str = self.struct_encoder(self.norm_struct(self.initial_stuct_proj(pep_struct_emb)), src_key_padding_mask=pep_struct_mask)   # [B, Lp_cm, E]\n",
    "            prot_trans_str = self.struct_encoder(self.norm_struct(self.initial_stuct_proj(prot_struct_emb)), src_key_padding_mask=prot_struct_mask)  # [B, Lt_cm, E]\n",
    "\n",
    "            # --- Cross-attend to structures ---\n",
    "            pep_struct_upd, _ = self.struct_to_seq_attn(query=self.norm_seq(pep_trans_seq), key=self.norm_struct(pep_trans_str), value=self.norm_struct(pep_trans_str), key_padding_mask=pep_struct_mask)\n",
    "            prot_struct_upd, _ = self.struct_to_seq_attn(query=self.norm_seq(prot_trans_seq), key=self.norm_struct(prot_trans_str), value=self.norm_struct(prot_trans_str), key_padding_mask=prot_struct_mask)\n",
    "\n",
    "            current_alpha = torch.sigmoid(self.struct_alpha)\n",
    "            pep_trans_emb  = pep_trans_seq  + current_alpha * pep_struct_upd    # [B, Lp, E]\n",
    "            prot_trans_emb = prot_trans_seq + current_alpha * prot_struct_upd    # [B, Lt, E]\n",
    "    \n",
    "            # --- Cross-attend binder vs target ---\n",
    "            pep_cross,  _  = self.seq_cross_attn(query=self.norm_seq(pep_trans_seq), key=self.norm_seq(prot_trans_seq), value=self.norm_seq(prot_trans_seq), key_padding_mask=prot_seq_mask)\n",
    "            prot_cross, _  = self.seq_cross_attn(query=self.norm_seq(prot_trans_seq), key=self.norm_seq(pep_trans_seq), value=self.norm_seq(pep_trans_seq), key_padding_mask=pep_seq_mask)\n",
    "    \n",
    "            # --- Residual updates ---\n",
    "            pep_seq_emb = pep_seq_emb + pep_cross\n",
    "            prot_seq_emb = prot_seq_emb + prot_cross\n",
    "    \n",
    "        # Pool (mean over non-masked positions)\n",
    "        pep_seq_coding   = create_mean_of_non_masked(pep_seq_emb, pep_seq_mask)\n",
    "        prot_seq_coding  = create_mean_of_non_masked(prot_seq_emb, prot_seq_mask)\n",
    "\n",
    "        # Projections + L2-normalize\n",
    "        pep_full   = F.normalize(self.seq_proj(pep_seq_coding),   dim=-1)\n",
    "        prot_full  = F.normalize(self.seq_proj(prot_seq_coding),  dim=-1)\n",
    "    \n",
    "        if mem_save:\n",
    "            torch.cuda.empty_cache()\n",
    "    \n",
    "        scale  = torch.exp(self.logit_scale).clamp(max=100.0)\n",
    "        logits = scale * (pep_full * prot_full).sum(dim=-1)  # [B]\n",
    "        \n",
    "        return logits\n",
    "\n",
    "    def training_step(self, batch, device):\n",
    "        pep_seq_emb, prot_seq_emb, pep_struct_emb, prot_struct_emb, labels = batch\n",
    "\n",
    "        # loss of predicting partner using peptide\n",
    "        positive_logits = self.forward(pep_seq_emb, prot_seq_emb, pep_struct_emb, prot_struct_emb)\n",
    "        positive_loss = F.binary_cross_entropy_with_logits(positive_logits, torch.ones_like(positive_logits).to(device)) # F.binary_cross_entropy_with_logits does sigmoid transfromation inside, excepts data, labels\n",
    "        \n",
    "        # Negative indexes\n",
    "        rows, cols = torch.triu_indices(pep_seq_emb.size(0), pep_seq_emb.size(0), offset=1) # upper triangle\n",
    "        \n",
    "        # loss of predicting peptide using partner\n",
    "        # negative_logits = self.forward(embedding_pep[rows,:,:], embedding_prot[cols,:,:], contacts_pep[rows,:,:], contacts_prot[cols,:,:], int_prob=0.0)\n",
    "        negative_logits = self.forward(pep_seq_emb[rows,:,:], prot_seq_emb[cols,:,:], pep_struct_emb[rows,:,:], prot_struct_emb[cols,:,:], int_prob=0.0)\n",
    "        negative_loss =  F.binary_cross_entropy_with_logits(negative_logits, torch.zeros_like(negative_logits).to(device))\n",
    "        \n",
    "        loss = (positive_loss + negative_loss) / 2\n",
    " \n",
    "        torch.cuda.empty_cache()\n",
    "        return loss\n",
    "\n",
    "    def validation_step_PPint(self, batch, device):\n",
    "        # Predict on random batches of training batch size\n",
    "        pep_seq_emb, prot_seq_emb, pep_struct_emb, prot_struct_emb, labels = batch\n",
    "        pep_seq_emb, prot_seq_emb, pep_struct_emb, prot_struct_emb = pep_seq_emb.to(device), prot_seq_emb.to(device), pep_struct_emb.to(device), prot_struct_emb.to(device)\n",
    "        # contacts_pep, contacts_prot = contacts_pep.to(device), contacts_prot.to(device)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "\n",
    "            positive_logits = self.forward(pep_seq_emb, prot_seq_emb, pep_struct_emb, prot_struct_emb)\n",
    "            \n",
    "            # loss of predicting partner using peptide\n",
    "            positive_loss = F.binary_cross_entropy_with_logits(positive_logits, torch.ones_like(positive_logits).to(device))\n",
    "            \n",
    "            # Negaive indexes\n",
    "            rows, cols = torch.triu_indices(pep_seq_emb.size(0), pep_seq_emb.size(0), offset=1)\n",
    "            \n",
    "            negative_logits = self.forward(pep_seq_emb[rows,:,:], prot_seq_emb[cols,:,:], pep_struct_emb[rows,:,:], prot_struct_emb[cols,:,:], int_prob=0.0)\n",
    "            negative_loss =  F.binary_cross_entropy_with_logits(negative_logits, torch.zeros_like(negative_logits).to(device))\n",
    "\n",
    "            loss = (positive_loss + negative_loss) / 2\n",
    "\n",
    "            logit_matrix = torch.zeros((pep_seq_emb.size(0), pep_seq_emb.size(0)),device=self.device)\n",
    "            logit_matrix[rows, cols] = negative_logits\n",
    "            logit_matrix[cols, rows] = negative_logits\n",
    "            \n",
    "            # Fill diagonal with positive scores\n",
    "            diag_indices = torch.arange(pep_seq_emb.size(0), device=self.device)\n",
    "            logit_matrix[diag_indices, diag_indices] = positive_logits.squeeze()\n",
    "\n",
    "            labels = torch.arange(pep_seq_emb.size(0)).to(self.device)\n",
    "            peptide_predictions = logit_matrix.argmax(dim=0)\n",
    "            peptide_ranks = logit_matrix.argsort(dim=0).diag() + 1\n",
    "            peptide_mrr = (peptide_ranks).float().pow(-1).mean()\n",
    "            \n",
    "            # partner_accuracy = partner_predictions.eq(labels).float().mean()\n",
    "            peptide_accuracy = peptide_predictions.eq(labels).float().mean()\n",
    "    \n",
    "            return loss, peptide_accuracy\n",
    "    \n",
    "    def validation_step_MetaDataset(self, batch, device):\n",
    "        pep_seq_emb, prot_seq_emb, pep_struct_emb, prot_struct_emb, labels = batch\n",
    "        pep_seq_emb, prot_seq_emb = pep_seq_emb.to(device), prot_seq_emb.to(device) \n",
    "        pep_struct_emb, prot_struct_emb = pep_struct_emb.to(device), prot_struct_emb.to(device)\n",
    "        # contacts_pep, contacts_prot = contacts_pep.to(device), contacts_prot.to(device)\n",
    "        labels = labels.to(device).float()\n",
    "    \n",
    "        with torch.no_grad():\n",
    "            logits = self.forward(pep_seq_emb, prot_seq_emb, pep_struct_emb, prot_struct_emb).float()\n",
    "            loss = F.binary_cross_entropy_with_logits(logits.view(-1), labels.view(-1))\n",
    "            return logits, loss\n",
    "\n",
    "    def calculate_logit_matrix(self, pep_seq_emb, prot_seq_emb, pep_struct_emb, prot_struct_emb):\n",
    "        \n",
    "        rows, cols = torch.triu_indices(pep_seq_emb.size(0), pep_seq_emb.size(0), offset=1)\n",
    "        positive_logits = self.forward(pep_seq_emb, prot_seq_emb, pep_struct_emb, prot_struct_emb)\n",
    "        negative_logits = self.forward(pep_seq_emb[rows,:,:], prot_seq_emb[cols,:,:], pep_struct_emb[rows,:,:], prot_struct_emb[cols,:,:], int_prob=0.0)\n",
    "        \n",
    "        logit_matrix = torch.zeros((pep_seq_emb.size(0),pep_seq_emb.size(0)),device=self.device)\n",
    "        logit_matrix[rows, cols] = negative_logits\n",
    "        logit_matrix[cols, rows] = negative_logits\n",
    "        \n",
    "        diag_indices = torch.arange(pep_seq_emb.size(0), device=self.device)\n",
    "        logit_matrix[diag_indices, diag_indices] = positive_logits.squeeze()\n",
    "        \n",
    "        return logit_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "14c1e33c-83da-47d9-bdb8-c276bee26867",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0\n",
      "tensor(0.5000)\n"
     ]
    }
   ],
   "source": [
    "model = MiniCLIP_ESM2_ESMIF().to(\"cuda\")\n",
    "path = \"/work3/s232958/data/trained/with_structure/ab0444eb-1438-4513-a60f-6fef0340bb95/d26175a6-5994-4146-b8f4-041f1e8fd80a_checkpoint_9/d26175a6-5994-4146-b8f4-041f1e8fd80a_checkpoint_epoch_9.pth\"\n",
    "checkpoint = torch.load(path, weights_only=False, map_location=torch.device('cpu'))\n",
    "# print(list(checkpoint[\"model_state_dict\"]))\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "model.load_state_dict(checkpoint['model_state_dict'])\n",
    "model.to(device)\n",
    "model.eval()\n",
    "\n",
    "alpha_raw = model.struct_alpha.item()\n",
    "print(alpha_raw)\n",
    "print(torch.sigmoid(torch.tensor(alpha_raw)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c47e3bfd-f141-4bf3-ae61-88e5635a7fc4",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "#### test-dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c232fcb-9d4f-43b4-b0ae-0a8a6bedaea1",
   "metadata": {},
   "outputs": [],
   "source": [
    "interaction_scores_pos = []\n",
    "interaction_scores_neg = []    \n",
    "\n",
    "for batch in tqdm(test_dataloader, total=round(len(Df_test)/10), desc=\"#Iterating through batched data\"):\n",
    "    seq_embedding_pep, seq_embedding_prot, str_embedding_pep, str_embedding_prot, lbls = batch\n",
    "    \n",
    "    seq_embedding_pep = seq_embedding_pep.to(\"cuda\")\n",
    "    seq_embedding_prot = seq_embedding_prot.to(\"cuda\")\n",
    "    str_embedding_pep = str_embedding_pep.to(\"cuda\")\n",
    "    str_embedding_prot = str_embedding_prot.to(\"cuda\")\n",
    "\n",
    "    with torch.no_grad():\n",
    "\n",
    "        rows, cols = torch.triu_indices(seq_embedding_pep.size(0), seq_embedding_pep.size(0), offset=1)\n",
    "        positive_logits = model(seq_embedding_pep, seq_embedding_prot, str_embedding_pep, str_embedding_prot)\n",
    "        # negative_logits = self.forward(embedding_pep[rows,:,:], embedding_prot[cols,:,:], str_embedding_pep[rows,:,:], str_embedding_prot[cols,:,:], int_prob=0.0)\n",
    "        negative_logits = model(seq_embedding_pep[rows,:,:], seq_embedding_prot[cols,:,:], str_embedding_pep[rows,:,:], str_embedding_prot[cols,:,:], int_prob=0.0)\n",
    "        \n",
    "        # print(logit_matrix)\n",
    "        interaction_scores_pos.append(positive_logits)\n",
    "        interaction_scores_neg.append(negative_logits)\n",
    "\n",
    "# Convert list of tensors to single 1D tensors\n",
    "pos_logits = torch.cat(interaction_scores_pos).detach().cpu().numpy()\n",
    "neg_logits = torch.cat(interaction_scores_neg).detach().cpu().numpy()\n",
    "print(\"Positives:\", pos_logits.shape)\n",
    "print(\"Negatives:\", neg_logits.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e2ed1ce-7afc-4e5e-940b-2d0ff6ea5d3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(6, 4))\n",
    "plt.hist(pos_logits, bins=50, alpha=0.6, label=\"Positive pairs\", density=True)\n",
    "plt.hist(neg_logits, bins=50, alpha=0.6, label=\"Negative pairs\", density=True)\n",
    "\n",
    "plt.xlabel(\"Logit score\")\n",
    "plt.ylabel(\"Density\")\n",
    "plt.title(\"Distribution of Positive vs Negative Interaction Scores\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17b7fdad-07df-425e-ba39-fe7e8d5616b8",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "#### non-dimers datset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "813a824b-1f17-4851-8a27-f6f32943d535",
   "metadata": {},
   "outputs": [],
   "source": [
    "interaction_scores = []\n",
    "for batch in tqdm(non_dimers_dataloader, total=round(len(non_dimers_Df)/10), desc=\"#Iterating through batched data\"):\n",
    "    seq_embedding_pep, seq_embedding_prot, str_embedding_pep, str_embedding_prot, lbls = batch\n",
    "    \n",
    "    seq_embedding_pep = seq_embedding_pep.to(\"cuda\")\n",
    "    seq_embedding_prot = seq_embedding_prot.to(\"cuda\")\n",
    "    str_embedding_pep = str_embedding_pep.to(\"cuda\")\n",
    "    str_embedding_prot = str_embedding_prot.to(\"cuda\")\n",
    "\n",
    "    with torch.no_grad():\n",
    "\n",
    "        rows, cols = torch.triu_indices(seq_embedding_pep.size(0), seq_embedding_pep.size(0), offset=1)\n",
    "        positive_logits = model(seq_embedding_pep, seq_embedding_prot, str_embedding_pep, str_embedding_prot)\n",
    "        # negative_logits = self.forward(embedding_pep[rows,:,:], embedding_prot[cols,:,:], str_embedding_pep[rows,:,:], str_embedding_prot[cols,:,:], int_prob=0.0)\n",
    "        negative_logits = model(seq_embedding_pep[rows,:,:], seq_embedding_prot[cols,:,:], str_embedding_pep[rows,:,:], str_embedding_prot[cols,:,:], int_prob=0.0)\n",
    "        \n",
    "        # print(logit_matrix)\n",
    "        interaction_scores_pos.append(positive_logits)\n",
    "        interaction_scores_neg.append(negative_logits)\n",
    "\n",
    "# Convert list of tensors to single 1D tensors\n",
    "pos_logits = torch.cat(interaction_scores_pos).detach().cpu().numpy()\n",
    "neg_logits = torch.cat(interaction_scores_neg).detach().cpu().numpy()\n",
    "print(\"Positives:\", pos_logits.shape)\n",
    "print(\"Negatives:\", neg_logits.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f7f52d1-3da4-4280-910e-3fca8ecd2efe",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(6, 4))\n",
    "plt.hist(pos_logits, bins=50, alpha=0.6, label=\"Positive pairs\", density=True)\n",
    "plt.hist(neg_logits, bins=50, alpha=0.6, label=\"Negative pairs\", density=True)\n",
    "\n",
    "plt.xlabel(\"Logit score\")\n",
    "plt.ylabel(\"Density\")\n",
    "plt.title(\"Distribution of Positive vs Negative Interaction Scores\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8517c37-5007-45f0-a141-280e98b19d3b",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "#### meta-analysis dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cf879d1-9b63-497f-bb63-c311df166161",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading batches\n",
    "interaction_scores = []\n",
    "\n",
    "for batch in tqdm(validation_dataloader, total = round(len(interaction_df_shuffled)/10),  desc= \"#Iterating through batched data\"):\n",
    "    seq_embedding_pep, seq_embedding_prot, str_embedding_pep, str_embedding_prot, lbls = batch\n",
    "    \n",
    "    seq_embedding_pep = seq_embedding_pep.to(\"cuda\")\n",
    "    seq_embedding_prot = seq_embedding_prot.to(\"cuda\")\n",
    "    str_embedding_pep = str_embedding_pep.to(\"cuda\")\n",
    "    str_embedding_prot = str_embedding_prot.to(\"cuda\")\n",
    "\n",
    "    with torch.no_grad():\n",
    "        positive_logits = model(seq_embedding_pep, seq_embedding_prot, str_embedding_pep, str_embedding_prot)\n",
    "        interaction_scores.append(positive_logits.unsqueeze(0))\n",
    "\n",
    "predicted_interaction_scores = np.concatenate([batch_score.cpu().detach().numpy().reshape(-1,) for batch_score in interaction_scores])\n",
    "interaction_probabilities = np.concatenate([torch.sigmoid(batch_score[0]).cpu().numpy() for batch_score in interaction_scores])\n",
    "\n",
    "pos_logits, neg_logits = [], []\n",
    "for i, row in interaction_df_shuffled.iterrows():\n",
    "    logit = predicted_interaction_scores[i]\n",
    "    if row.binder == False:\n",
    "        neg_logits.append(logit)\n",
    "    elif row.binder == True:\n",
    "        pos_logits.append(logit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d1c1bd5-70f5-48dd-bca7-9aa43613bd07",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(6, 4))\n",
    "plt.hist(pos_logits, bins=50, alpha=0.6, label=\"Positive pairs\", density=True)\n",
    "plt.hist(neg_logits, bins=50, alpha=0.6, label=\"Negative pairs\", density=True)\n",
    "\n",
    "plt.xlabel(\"Logit score\")\n",
    "plt.ylabel(\"Density\")\n",
    "plt.title(\"Distribution of Positive vs Negative Interaction Scores\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ESM CUDA Env",
   "language": "python",
   "name": "esm_cuda"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
