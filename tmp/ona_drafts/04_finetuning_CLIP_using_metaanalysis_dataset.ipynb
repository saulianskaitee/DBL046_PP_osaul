{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "63399e5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, os\n",
    "os.chdir(\"/zhome/c9/0/203261/DBL046_PP_osaul/DBL046_PP_osaul/tmp/ona_drafts\")\n",
    "# print(os.getcwd())\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import ast\n",
    "import math\n",
    "import random\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader, Subset, WeightedRandomSampler\n",
    "import pytorch_lightning as pl\n",
    "from torch.optim import AdamW\n",
    "torch.manual_seed(0)\n",
    "# from accelerate import Accelerator\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from Levenshtein import distance as Ldistance\n",
    "\n",
    "import training_utils.dataset_utils as data_utils\n",
    "import training_utils.partitioning_utils as pat_utils\n",
    "\n",
    "memory_verbose = False\n",
    "use_wandb = True # Used to track loss in real-time without printing\n",
    "model_save_steps = 1\n",
    "train_frac = 1.0\n",
    "test_frac = 1.0\n",
    "\n",
    "embedding_dimension = 1152 # 1280 | 960 | 1152\n",
    "number_of_recycles = 2\n",
    "padding_value = -5000\n",
    "\n",
    "batch_size = 20\n",
    "learning_rate = 2e-5\n",
    "EPOCHS = 15"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "f31b2d9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gaussian_kernel(x, sigma):\n",
    "    return np.exp(-x**2 / (2 * sigma**2))\n",
    "\n",
    "def transform_vector(vector, sigma):\n",
    "\n",
    "    interacting_indices = np.where(vector == 1)[0]   # positions where vector == 1\n",
    "    transformed_vector = np.zeros_like(vector, dtype=float)\n",
    "    \n",
    "    for i in range(len(vector)):\n",
    "        if vector[i] == 0:\n",
    "            distances = np.abs(interacting_indices - i)   # distance to all \"1\"s\n",
    "            min_distance = np.min(distances)              # closest \"1\"\n",
    "            transformed_vector[i] = gaussian_kernel(min_distance, sigma)\n",
    "        else:\n",
    "            transformed_vector[i] = 1.0\n",
    "    return transformed_vector\n",
    "\n",
    "def safe_shuffle(n, device):\n",
    "    shuffled = torch.randperm(n, device=device)\n",
    "    while torch.any(shuffled == torch.arange(n, device=device)):\n",
    "        shuffled = torch.randperm(n, device=device)\n",
    "    return shuffled\n",
    "\n",
    "def create_key_padding_mask(embeddings, padding_value=0, offset=10):\n",
    "    return (embeddings < (padding_value + offset)).all(dim=-1)\n",
    "\n",
    "def create_mean_of_non_masked(embeddings, padding_mask):\n",
    "    # Use masked select and mean to compute the mean of non-masked elements\n",
    "    # embeddings should be of shape (batch_size, seq_len, features)\n",
    "    seq_embeddings = []\n",
    "    for i in range(embeddings.shape[0]): # looping over all batch elements\n",
    "        non_masked_embeddings = embeddings[i][~padding_mask[i]] # shape [num_real_tokens, features]\n",
    "        if len(non_masked_embeddings) == 0:\n",
    "            print(\"You are masking all positions when creating sequence representation\")\n",
    "            sys.exit(1)\n",
    "        mean_embedding = non_masked_embeddings.mean(dim=0) # sequence is represented by the single vecotr [1152] [features]\n",
    "        seq_embeddings.append(mean_embedding)\n",
    "    return torch.stack(seq_embeddings)\n",
    "\n",
    "class MiniCLIP_w_transformer_crossattn(pl.LightningModule):\n",
    "\n",
    "    def __init__(self, padding_value = -5000, embed_dimension=1152, num_recycles=1):\n",
    "\n",
    "        super().__init__()\n",
    "        self.num_recycles = num_recycles # how many times you iteratively refine embeddings with self- and cross-attention (ALPHA-Fold-style recycling).\n",
    "        self.padding_value = padding_value\n",
    "        self.embed_dimension = embed_dimension\n",
    "\n",
    "        self.logit_scale = nn.Parameter(torch.tensor(math.log(1/0.07)))  # ~CLIP init\n",
    "\n",
    "        self.transformerencoder =  nn.TransformerEncoderLayer(\n",
    "            d_model=self.embed_dimension,\n",
    "            nhead=8,\n",
    "            dropout=0.1,\n",
    "            batch_first=True,\n",
    "            dim_feedforward=self.embed_dimension\n",
    "            )\n",
    " \n",
    "        self.norm = nn.LayerNorm(self.embed_dimension)  # For residual additions\n",
    "\n",
    "        self.cross_attn = nn.MultiheadAttention(\n",
    "            embed_dim=self.embed_dimension,\n",
    "            num_heads=8,\n",
    "            dropout=0.1,\n",
    "            batch_first=True\n",
    "        )\n",
    "\n",
    "        self.prot_embedder = nn.Sequential(\n",
    "            nn.Linear(self.embed_dimension, 640),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(640, 320),\n",
    "        )\n",
    "        \n",
    "    def forward(self, pep_input, prot_input, pep_int_mask=None, prot_int_mask=None, int_prob=None, mem_save=True): # , pep_tokens, prot_tokens\n",
    "\n",
    "        pep_mask = create_key_padding_mask(embeddings=pep_input,padding_value=self.padding_value)\n",
    "        prot_mask = create_key_padding_mask(embeddings=prot_input,padding_value=self.padding_value)\n",
    " \n",
    "        # Initialize residual states\n",
    "        pep_emb = pep_input.clone()\n",
    "        prot_emb = prot_input.clone()\n",
    "\n",
    " \n",
    "        for _ in range(self.num_recycles):\n",
    "\n",
    "            # Transformer encoding with residual\n",
    "            pep_trans = self.transformerencoder(self.norm(pep_emb), src_key_padding_mask=pep_mask)\n",
    "            prot_trans = self.transformerencoder(self.norm(prot_emb), src_key_padding_mask=prot_mask)\n",
    "\n",
    "            # Cross-attention with residual\n",
    "            pep_cross, _ = self.cross_attn(query=self.norm(pep_trans), key=self.norm(prot_trans), value=self.norm(prot_trans), key_padding_mask=prot_mask)\n",
    "            prot_cross, _ = self.cross_attn(query=self.norm(prot_trans), key=self.norm(pep_trans), value=self.norm(pep_trans), key_padding_mask=pep_mask)\n",
    "            \n",
    "            # Additive update with residual connection\n",
    "            pep_emb = pep_emb + pep_cross  \n",
    "            prot_emb = prot_emb + prot_cross\n",
    "\n",
    "        pep_seq_coding = create_mean_of_non_masked(pep_emb, pep_mask)\n",
    "        prot_seq_coding = create_mean_of_non_masked(prot_emb, prot_mask)\n",
    "        \n",
    "        # Use self-attention outputs for embeddings\n",
    "        pep_seq_coding = F.normalize(self.prot_embedder(pep_seq_coding))\n",
    "        prot_seq_coding = F.normalize(self.prot_embedder(prot_seq_coding))\n",
    " \n",
    "        if mem_save:\n",
    "            torch.cuda.empty_cache()\n",
    "        \n",
    "        scale = torch.exp(self.logit_scale).clamp(max=100.0)\n",
    "        logits = scale * (pep_seq_coding * prot_seq_coding).sum(dim=-1) # Dot-Product for comparison\n",
    "        \n",
    "        return logits\n",
    "    \n",
    "\n",
    "    def training_step(self, batch, device):\n",
    "        # Passing the sequences to the models\n",
    " \n",
    "        embedding_pep = batch[0]\n",
    "        embedding_prot = batch[1]\n",
    " \n",
    "        embedding_pep = embedding_pep.to(device)\n",
    "        embedding_prot = embedding_prot.to(device)\n",
    "\n",
    "        positive_logits = self(embedding_pep, embedding_prot)\n",
    "        \n",
    "        # rows, cols = torch.triu_indices(embedding_prot.size(0), embedding_prot.size(0), offset=1)         \n",
    "        # negative_logits = self(embedding_pep[rows,:,:], embedding_prot[cols,:,:], int_prob=0.0)\n",
    "\n",
    "        loss = F.binary_cross_entropy_with_logits(positive_logits, torch.ones_like(positive_logits).to(device))\n",
    " \n",
    "        # # loss of predicting peptide using partner\n",
    "        # negative_loss =  F.binary_cross_entropy_with_logits(negative_logits, torch.zeros_like(negative_logits).to(device))\n",
    "        \n",
    "        loss = (positive_loss + negative_loss) / 2\n",
    " \n",
    "        torch.cuda.empty_cache()\n",
    "        return loss\n",
    "    \n",
    "    def validation_step(self, batch, device):\n",
    "        # Predict on random batches of training batch size\n",
    "        embedding_pep = batch[0]\n",
    "        embedding_prot = batch[1]\n",
    "        embedding_pep = embedding_pep.to(device)\n",
    "        embedding_prot = embedding_prot.to(device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "\n",
    "            positive_logits = self(\n",
    "                    embedding_pep,\n",
    "                    embedding_prot,\n",
    "                    # interaction_pep,\n",
    "                    # interaction_prot,\n",
    "                    # int_prob = 0.0\n",
    "                    )\n",
    "            \n",
    "            # loss of predicting partner using peptide\n",
    "            positive_loss = F.binary_cross_entropy_with_logits(positive_logits, torch.ones_like(positive_logits).to(device))\n",
    "            \n",
    "            # Negaive indexes\n",
    "            rows, cols = torch.triu_indices(embedding_prot.size(0), embedding_prot.size(0), offset=1) \n",
    "            \n",
    "\n",
    "            negative_logits = self(embedding_pep[rows,:,:], \n",
    "                              embedding_prot[cols,:,:], \n",
    "                              int_prob=0.0)\n",
    "                   \n",
    "            \n",
    "            negative_loss =  F.binary_cross_entropy_with_logits(negative_logits, torch.zeros_like(negative_logits).to(device))\n",
    "\n",
    "            loss = (positive_loss + negative_loss) / 2\n",
    "           \n",
    "            logit_matrix = torch.zeros((embedding_pep.size(0),embedding_pep.size(0)),device=self.device)\n",
    "            logit_matrix[rows, cols] = negative_logits\n",
    "            logit_matrix[cols, rows] = negative_logits\n",
    "            \n",
    "            # Fill diagonal with positive scores\n",
    "            diag_indices = torch.arange(embedding_pep.size(0), device=self.device)\n",
    "            logit_matrix[diag_indices, diag_indices] = positive_logits.squeeze()\n",
    "\n",
    "            labels = torch.arange(embedding_prot.size(0)).to(self.device)\n",
    "            peptide_predictions = logit_matrix.argmax(dim=0)\n",
    "            peptide_ranks = logit_matrix.argsort(dim=0).diag() + 1\n",
    "            peptide_mrr = (peptide_ranks).float().pow(-1).mean()\n",
    "            \n",
    "            # partner_accuracy = partner_predictions.eq(labels).float().mean()\n",
    "            peptide_accuracy = peptide_predictions.eq(labels).float().mean()\n",
    "    \n",
    "            k = 3\n",
    "            peptide_topk_accuracy = torch.any((logit_matrix.topk(k, dim=0).indices - labels.reshape(1, -1)) == 0, dim=0).sum() / logit_matrix.shape[0]\n",
    "            # partner_topk_accuracy = torch.any((logits.topk(k, dim=1).indices - labels.reshape(-1, 1)) == 0, dim=1).sum() / logits.shape[0]\n",
    "    \n",
    "            del logit_matrix,positive_logits,negative_logits,embedding_pep,embedding_prot\n",
    "\n",
    "            return loss, peptide_accuracy, peptide_topk_accuracy\n",
    "\n",
    "    def calculate_logit_matrix(self,embedding_pep,embedding_prot):\n",
    "        positive_logits = self(\n",
    "            embedding_pep,\n",
    "            embedding_prot)\n",
    "        \n",
    "        # Negaive indexes\n",
    "        rows, cols = torch.triu_indices(embedding_pep.size(0), embedding_pep.size(0), offset=1) \n",
    "        \n",
    "        negative_logits = self(embedding_pep[rows,:,:], \n",
    "                      embedding_prot[cols,:,:], \n",
    "                      int_prob=0.0)\n",
    "        \n",
    "        logit_matrix = torch.zeros((embedding_pep.size(0),embedding_pep.size(0)),device=self.device)\n",
    "        logit_matrix[rows, cols] = negative_logits\n",
    "        logit_matrix[cols, rows] = negative_logits\n",
    "        # Fill diagonal with positive scores\n",
    "        diag_indices = torch.arange(embedding_pep.size(0), device=self.device)\n",
    "        logit_matrix[diag_indices, diag_indices] = positive_logits.squeeze()\n",
    "        return logit_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "7bb488e8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MiniCLIP_w_transformer_crossattn(\n",
       "  (transformerencoder): TransformerEncoderLayer(\n",
       "    (self_attn): MultiheadAttention(\n",
       "      (out_proj): NonDynamicallyQuantizableLinear(in_features=1152, out_features=1152, bias=True)\n",
       "    )\n",
       "    (linear1): Linear(in_features=1152, out_features=1152, bias=True)\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "    (linear2): Linear(in_features=1152, out_features=1152, bias=True)\n",
       "    (norm1): LayerNorm((1152,), eps=1e-05, elementwise_affine=True)\n",
       "    (norm2): LayerNorm((1152,), eps=1e-05, elementwise_affine=True)\n",
       "    (dropout1): Dropout(p=0.1, inplace=False)\n",
       "    (dropout2): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (norm): LayerNorm((1152,), eps=1e-05, elementwise_affine=True)\n",
       "  (cross_attn): MultiheadAttention(\n",
       "    (out_proj): NonDynamicallyQuantizableLinear(in_features=1152, out_features=1152, bias=True)\n",
       "  )\n",
       "  (prot_embedder): Sequential(\n",
       "    (0): Linear(in_features=1152, out_features=640, bias=True)\n",
       "    (1): ReLU()\n",
       "    (2): Linear(in_features=640, out_features=320, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "path = '../PPI_PLM/models/CLIP_no_structural_information/a1d0549b-3f90-4ce2-b795-7bca2276cb07_checkpoint_4/a1d0549b-3f90-4ce2-b795-7bca2276cb07_checkpoint_epoch_4.pth'\n",
    "checkpoint = torch.load(path, weights_only=False, map_location=torch.device('cpu'))\n",
    "# print(list(checkpoint[\"model_state_dict\"]))\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "model = MiniCLIP_w_transformer_crossattn()\n",
    "model.load_state_dict(checkpoint['model_state_dict'])\n",
    "model.to(device)\n",
    "model.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "2bb156f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Output path\n",
    "trained_model_dir = \"/zhome/c9/0/203261/DBL046_PP_osaul/DBL046_PP_osaul/tmp/ona_drafts\"\n",
    "\n",
    "## Embeddings paths\n",
    "binders_embeddings = \"../data/meta_analysis/binders_embeddings\"\n",
    "targets_embeddings = \"../data/meta_analysis/targets_embeddings\"\n",
    "\n",
    "# ## Training variables\n",
    "# runID = uuid.uuid4()\n",
    "\n",
    "# def print_mem_consumption():\n",
    "#     # 1. Total memory available on the GPU (device 0)\n",
    "#     t = torch.cuda.get_device_properties(0).total_memory\n",
    "#     # 2. How much memory PyTorch has *reserved* from CUDA\n",
    "#     r = torch.cuda.memory_reserved(0)\n",
    "#     # 3. How much of that reserved memory is actually *used* by tensors\n",
    "#     a = torch.cuda.memory_allocated(0)\n",
    "#     # 4. Reserved but not currently allocated (so “free inside PyTorch’s pool”)\n",
    "#     f = r - a\n",
    "\n",
    "#     print(\"Total memory: \", t/1e9)      # total VRAM in GB\n",
    "#     print(\"Reserved memory: \", r/1e9)   # PyTorch’s reserved pool in GB\n",
    "#     print(\"Allocated memory: \", a//1e9) # actually in use (integer division)\n",
    "#     print(\"Free memory: \", f/1e9)       # slack in the reserved pool in GB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "332b707f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(('FGFR2', False), 1930),\n",
       " (('EGFR_2', False), 286),\n",
       " (('FGFR2', True), 193),\n",
       " (('IL7Ra', False), 133),\n",
       " (('EGFR', False), 120),\n",
       " (('TrkA', False), 119),\n",
       " (('InsulinR', False), 97),\n",
       " (('SARS_CoV2_RBD', False), 90),\n",
       " (('VirB8', False), 90),\n",
       " (('Pdl1', False), 83)]"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Loading the dataset\n",
    "interaction_df = pd.read_csv(\"../data/meta_analysis/interaction_df_metaanal.csv\", index_col = 0).drop(columns = [\"binder_id\", \"target_id\"]).rename(columns={\n",
    "    \"A_seq\" : \"binder_seq\",\n",
    "    \"B_seq\" : \"target_seq\"\n",
    "})\n",
    "all_targets = interaction_df.target_id_mod.unique()\n",
    "binder_nonbinder = interaction_df.binder.value_counts()\n",
    "target_binder_nonbinder_Dict = dict(interaction_df.groupby(\"target_id_mod\")[\"binder\"].value_counts())\n",
    "sorted_items = sorted(target_binder_nonbinder_Dict.items(), key=lambda kv: kv[1], reverse=True)\n",
    "sorted_items[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "66860473",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>binder_chain</th>\n",
       "      <th>target_chains</th>\n",
       "      <th>binder</th>\n",
       "      <th>binder_seq</th>\n",
       "      <th>target_seq</th>\n",
       "      <th>target_id_mod</th>\n",
       "      <th>target_binder_ID</th>\n",
       "      <th>observation_weight</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>A</td>\n",
       "      <td>[\"B\"]</td>\n",
       "      <td>False</td>\n",
       "      <td>LDFIVFAGPEKAIKFYKEMAKRNLEVKIWIDGDWAVVQVK</td>\n",
       "      <td>ANPYISVANIMLQNYVKQREKYNYDTLKEQFTFIKNASTSIVYMQF...</td>\n",
       "      <td>VirB8</td>\n",
       "      <td>VirB8_1</td>\n",
       "      <td>0.000159</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>A</td>\n",
       "      <td>[\"B\"]</td>\n",
       "      <td>False</td>\n",
       "      <td>SEQDETMHRIVRSVIQHAYKHNDEMAEYFAQNAAEIYKEQNKSEEA...</td>\n",
       "      <td>RSPHRPILQAGLPANASTVVGGDVEFVCKVYSDAQPHIQWIKHVPY...</td>\n",
       "      <td>FGFR2</td>\n",
       "      <td>FGFR2_1</td>\n",
       "      <td>0.000159</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>A</td>\n",
       "      <td>[\"B\"]</td>\n",
       "      <td>False</td>\n",
       "      <td>DYKQLKKHATKLLELAKKDPSSKRDLLRTAASYANKVLFEDSDPRA...</td>\n",
       "      <td>RSPHRPILQAGLPANASTVVGGDVEFVCKVYSDAQPHIQWIKHVPY...</td>\n",
       "      <td>FGFR2</td>\n",
       "      <td>FGFR2_2</td>\n",
       "      <td>0.000159</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>A</td>\n",
       "      <td>[\"B\"]</td>\n",
       "      <td>False</td>\n",
       "      <td>DEKEELERRANRVAFLAIQIQNEEYHRILAELYVQFMKAAENNDTE...</td>\n",
       "      <td>RSPHRPILQAGLPANASTVVGGDVEFVCKVYSDAQPHIQWIKHVPY...</td>\n",
       "      <td>FGFR2</td>\n",
       "      <td>FGFR2_3</td>\n",
       "      <td>0.000159</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>A</td>\n",
       "      <td>[\"B\"]</td>\n",
       "      <td>False</td>\n",
       "      <td>PDNKEKLMSIAVQLILRINEAARSEEQWRYANRAAFAAVEASSGSD...</td>\n",
       "      <td>RSPHRPILQAGLPANASTVVGGDVEFVCKVYSDAQPHIQWIKHVPY...</td>\n",
       "      <td>FGFR2</td>\n",
       "      <td>FGFR2_4</td>\n",
       "      <td>0.000159</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3527</th>\n",
       "      <td>A</td>\n",
       "      <td>[\"B\"]</td>\n",
       "      <td>False</td>\n",
       "      <td>DLRKYAAELVDRLAEKYNLDSDQYNALVRLASELVWQGKSKEEIEK...</td>\n",
       "      <td>ELCDDDPPEIPHATFKAMAYKEGTMLNCECKRGFRRIKSGSLYMLC...</td>\n",
       "      <td>IL2Ra</td>\n",
       "      <td>IL2Ra_62</td>\n",
       "      <td>0.000159</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3528</th>\n",
       "      <td>A</td>\n",
       "      <td>[\"B\"]</td>\n",
       "      <td>False</td>\n",
       "      <td>SKEEIKKEAEELIEELKKKGYNLPLRILEFALKEIEETNSEKYYEQ...</td>\n",
       "      <td>ELCDDDPPEIPHATFKAMAYKEGTMLNCECKRGFRRIKSGSLYMLC...</td>\n",
       "      <td>IL2Ra</td>\n",
       "      <td>IL2Ra_63</td>\n",
       "      <td>0.000159</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3529</th>\n",
       "      <td>A</td>\n",
       "      <td>[\"B\"]</td>\n",
       "      <td>False</td>\n",
       "      <td>SPEYKKFLELIKEAEAARKAGDLDKAKELLEKALELAKKMKAKSLI...</td>\n",
       "      <td>ELCDDDPPEIPHATFKAMAYKEGTMLNCECKRGFRRIKSGSLYMLC...</td>\n",
       "      <td>IL2Ra</td>\n",
       "      <td>IL2Ra_64</td>\n",
       "      <td>0.000159</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3530</th>\n",
       "      <td>A</td>\n",
       "      <td>[\"B\"]</td>\n",
       "      <td>False</td>\n",
       "      <td>DPLLAYKLLKLSQKALEKAYAEDRERAEELLEEAEAALRSLGDEAG...</td>\n",
       "      <td>ELCDDDPPEIPHATFKAMAYKEGTMLNCECKRGFRRIKSGSLYMLC...</td>\n",
       "      <td>IL2Ra</td>\n",
       "      <td>IL2Ra_65</td>\n",
       "      <td>0.000159</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3531</th>\n",
       "      <td>A</td>\n",
       "      <td>[\"B\"]</td>\n",
       "      <td>False</td>\n",
       "      <td>SEAARRARELFHEADELDKRGNPEEAEEVLREALELAREAGSPNLA...</td>\n",
       "      <td>ELCDDDPPEIPHATFKAMAYKEGTMLNCECKRGFRRIKSGSLYMLC...</td>\n",
       "      <td>IL2Ra</td>\n",
       "      <td>IL2Ra_66</td>\n",
       "      <td>0.000159</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3532 rows × 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     binder_chain target_chains  binder  \\\n",
       "0               A         [\"B\"]   False   \n",
       "1               A         [\"B\"]   False   \n",
       "2               A         [\"B\"]   False   \n",
       "3               A         [\"B\"]   False   \n",
       "4               A         [\"B\"]   False   \n",
       "...           ...           ...     ...   \n",
       "3527            A         [\"B\"]   False   \n",
       "3528            A         [\"B\"]   False   \n",
       "3529            A         [\"B\"]   False   \n",
       "3530            A         [\"B\"]   False   \n",
       "3531            A         [\"B\"]   False   \n",
       "\n",
       "                                             binder_seq  \\\n",
       "0              LDFIVFAGPEKAIKFYKEMAKRNLEVKIWIDGDWAVVQVK   \n",
       "1     SEQDETMHRIVRSVIQHAYKHNDEMAEYFAQNAAEIYKEQNKSEEA...   \n",
       "2     DYKQLKKHATKLLELAKKDPSSKRDLLRTAASYANKVLFEDSDPRA...   \n",
       "3     DEKEELERRANRVAFLAIQIQNEEYHRILAELYVQFMKAAENNDTE...   \n",
       "4     PDNKEKLMSIAVQLILRINEAARSEEQWRYANRAAFAAVEASSGSD...   \n",
       "...                                                 ...   \n",
       "3527  DLRKYAAELVDRLAEKYNLDSDQYNALVRLASELVWQGKSKEEIEK...   \n",
       "3528  SKEEIKKEAEELIEELKKKGYNLPLRILEFALKEIEETNSEKYYEQ...   \n",
       "3529  SPEYKKFLELIKEAEAARKAGDLDKAKELLEKALELAKKMKAKSLI...   \n",
       "3530  DPLLAYKLLKLSQKALEKAYAEDRERAEELLEEAEAALRSLGDEAG...   \n",
       "3531  SEAARRARELFHEADELDKRGNPEEAEEVLREALELAREAGSPNLA...   \n",
       "\n",
       "                                             target_seq target_id_mod  \\\n",
       "0     ANPYISVANIMLQNYVKQREKYNYDTLKEQFTFIKNASTSIVYMQF...         VirB8   \n",
       "1     RSPHRPILQAGLPANASTVVGGDVEFVCKVYSDAQPHIQWIKHVPY...         FGFR2   \n",
       "2     RSPHRPILQAGLPANASTVVGGDVEFVCKVYSDAQPHIQWIKHVPY...         FGFR2   \n",
       "3     RSPHRPILQAGLPANASTVVGGDVEFVCKVYSDAQPHIQWIKHVPY...         FGFR2   \n",
       "4     RSPHRPILQAGLPANASTVVGGDVEFVCKVYSDAQPHIQWIKHVPY...         FGFR2   \n",
       "...                                                 ...           ...   \n",
       "3527  ELCDDDPPEIPHATFKAMAYKEGTMLNCECKRGFRRIKSGSLYMLC...         IL2Ra   \n",
       "3528  ELCDDDPPEIPHATFKAMAYKEGTMLNCECKRGFRRIKSGSLYMLC...         IL2Ra   \n",
       "3529  ELCDDDPPEIPHATFKAMAYKEGTMLNCECKRGFRRIKSGSLYMLC...         IL2Ra   \n",
       "3530  ELCDDDPPEIPHATFKAMAYKEGTMLNCECKRGFRRIKSGSLYMLC...         IL2Ra   \n",
       "3531  ELCDDDPPEIPHATFKAMAYKEGTMLNCECKRGFRRIKSGSLYMLC...         IL2Ra   \n",
       "\n",
       "     target_binder_ID  observation_weight  \n",
       "0             VirB8_1            0.000159  \n",
       "1             FGFR2_1            0.000159  \n",
       "2             FGFR2_2            0.000159  \n",
       "3             FGFR2_3            0.000159  \n",
       "4             FGFR2_4            0.000159  \n",
       "...               ...                 ...  \n",
       "3527         IL2Ra_62            0.000159  \n",
       "3528         IL2Ra_63            0.000159  \n",
       "3529         IL2Ra_64            0.000159  \n",
       "3530         IL2Ra_65            0.000159  \n",
       "3531         IL2Ra_66            0.000159  \n",
       "\n",
       "[3532 rows x 8 columns]"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# %%\n",
    "# Annotating each observation with a weight corresponding to whether it is considered a binder or not\n",
    "N_bins = len(interaction_df[\"binder\"].value_counts())\n",
    "pr_class_uniform_weight = 1 / N_bins\n",
    "pr_class_weight_informed_with_size_of_bins = pr_class_uniform_weight  / interaction_df[\"binder\"].value_counts()\n",
    "pr_class_weight_informed_with_size_of_bins = pr_class_weight_informed_with_size_of_bins.to_dict()\n",
    "interaction_df[\"observation_weight\"] = interaction_df.binder.apply(lambda x: pr_class_weight_informed_with_size_of_bins[x])\n",
    "interaction_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "ce9f510a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3029 503\n",
      "0.16606140640475403\n"
     ]
    }
   ],
   "source": [
    "sum = 0\n",
    "sum_all = 0\n",
    "for i in target_binder_nonbinder_Dict.keys():\n",
    "    sum_all += target_binder_nonbinder_Dict[i]\n",
    "    if i[0] in [\"IL7Ra\", \"sntx_2\", \"sntx\", \"VirB8\",\"Mdm2\", \"IL10Ra\", \"IL2Ra\"]:\n",
    "        sum += target_binder_nonbinder_Dict[i]\n",
    "print(sum_all-sum, sum)\n",
    "print(sum / (sum_all-sum))\n",
    "\n",
    "# [\"IL7Ra\", \"sntx_2\", \"sntx\", \"VirB8\",\"Mdm2\", \"IL10Ra\", \"IL2Ra\"] validation set"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec2f8c77",
   "metadata": {},
   "source": [
    "[\"IL7Ra\", \"sntx_2\", \"sntx\", \"VirB8\",\"Mdm2\", \"IL10Ra\", \"IL2Ra\"] validation set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "4d5147ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Targets df\n",
    "target_df = interaction_df[[\"target_id_mod\",\"target_seq\"]].rename(columns={\"target_seq\":\"sequence\", \"target_id_mod\" : \"ID\"})\n",
    "target_df[\"seq_len\"] = target_df[\"sequence\"].apply(len)\n",
    "target_df = target_df.drop_duplicates(subset=[\"ID\",\"sequence\"])\n",
    "target_df = target_df.set_index(\"ID\")\n",
    "\n",
    "# Binders df\n",
    "binder_df = interaction_df[[\"target_binder_ID\",\"binder_seq\", \"binder\"]].rename(columns={\"binder_seq\":\"sequence\", \"target_binder_ID\" : \"ID\", \"binder\" : \"label\"})\n",
    "binder_df[\"seq_len\"] = binder_df[\"sequence\"].apply(len)\n",
    "binder_df = binder_df.set_index(\"ID\")\n",
    "\n",
    "# target_df\n",
    "\n",
    "# Interaction Dict\n",
    "interaction_Dict = dict(enumerate(zip(interaction_df[\"target_id_mod\"], interaction_df[\"target_binder_ID\"]), start=1))\n",
    "# interaction_Dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "8208fee1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3029 503\n"
     ]
    }
   ],
   "source": [
    "target_df_val = target_df.loc[[\"IL7Ra\", \"sntx_2\", \"sntx\", \"VirB8\",\"Mdm2\", \"IL10Ra\", \"IL2Ra\"]]\n",
    "target_df_train = target_df.loc[target_df.index.difference([\"IL7Ra\", \"sntx_2\", \"sntx\", \"VirB8\",\"Mdm2\", \"IL10Ra\", \"IL2Ra\"])]\n",
    "# print(len(target_df_val), len(target_df_train))\n",
    "\n",
    "idx = binder_df.index.astype(str)\n",
    "mask = pd.Series(idx).str.startswith(tuple([\"IL7Ra\", \"sntx_2\", \"sntx\", \"VirB8\",\"Mdm2\", \"IL10Ra\", \"IL2Ra\"])).to_numpy()\n",
    "\n",
    "binder_df_val   = binder_df[mask]    # IDs whose index starts with any of the names\n",
    "binder_df_train = binder_df[~mask]   # everything else\n",
    "print(len(binder_df_train), len(binder_df_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "caf48126",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sequence</th>\n",
       "      <th>label</th>\n",
       "      <th>seq_len</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ID</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [sequence, label, seq_len]\n",
       "Index: []"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "binder_df_train[binder_df_train.index.str.startswith(\"IL7Ra\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "66479109",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sequence</th>\n",
       "      <th>label</th>\n",
       "      <th>seq_len</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ID</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>IL7Ra_1</th>\n",
       "      <td>SVQLEALEILIKELKKELEKAKKELEKASPEEKKKLEEKAKKLEEL...</td>\n",
       "      <td>False</td>\n",
       "      <td>84</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>IL7Ra_2</th>\n",
       "      <td>EEKDKYIEEAQYVAVEALEYIKDGTAEEGEKAKEEAEKKIRELLTK...</td>\n",
       "      <td>False</td>\n",
       "      <td>74</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>IL7Ra_3</th>\n",
       "      <td>DEEEKKKLAEEAIEAVEKGDLEKAKELLKKLAEAAKTEEEAEKWLS...</td>\n",
       "      <td>True</td>\n",
       "      <td>78</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>IL7Ra_4</th>\n",
       "      <td>EEKEKAKEIIDRAVKEAKKEAEKEDEETKKKTLEIIERAELVVKAD...</td>\n",
       "      <td>False</td>\n",
       "      <td>85</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>IL7Ra_5</th>\n",
       "      <td>VEEDLKKALKALKEGNKIEAAEHLLAARVEALLKGDEETAEKVEEA...</td>\n",
       "      <td>False</td>\n",
       "      <td>65</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>IL7Ra_167</th>\n",
       "      <td>GITIFINADDPTVAELAKSANPKHAHFHPAGAVWIELDDPTASKIV...</td>\n",
       "      <td>False</td>\n",
       "      <td>62</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>IL7Ra_168</th>\n",
       "      <td>SREDRAARIVLEALRQMIKNVEDPKDARLIYLKAEQAKKIVDDPTV...</td>\n",
       "      <td>False</td>\n",
       "      <td>63</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>IL7Ra_169</th>\n",
       "      <td>DESQKETLTKLIKLAVKAIMNNDPDTAKKVVDKLRKVASEANDHMA...</td>\n",
       "      <td>False</td>\n",
       "      <td>61</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>IL7Ra_170</th>\n",
       "      <td>DKEELKKKIHKLAQIVARHHREDDSTVNDVAVIVLKLLRQDTEEAL...</td>\n",
       "      <td>False</td>\n",
       "      <td>62</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>IL7Ra_171</th>\n",
       "      <td>SLDEILKEVKKLAKLAGISEEEAEKTARRIRRAGDEELAKMMARQL...</td>\n",
       "      <td>False</td>\n",
       "      <td>52</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>171 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    sequence  label  seq_len\n",
       "ID                                                                          \n",
       "IL7Ra_1    SVQLEALEILIKELKKELEKAKKELEKASPEEKKKLEEKAKKLEEL...  False       84\n",
       "IL7Ra_2    EEKDKYIEEAQYVAVEALEYIKDGTAEEGEKAKEEAEKKIRELLTK...  False       74\n",
       "IL7Ra_3    DEEEKKKLAEEAIEAVEKGDLEKAKELLKKLAEAAKTEEEAEKWLS...   True       78\n",
       "IL7Ra_4    EEKEKAKEIIDRAVKEAKKEAEKEDEETKKKTLEIIERAELVVKAD...  False       85\n",
       "IL7Ra_5    VEEDLKKALKALKEGNKIEAAEHLLAARVEALLKGDEETAEKVEEA...  False       65\n",
       "...                                                      ...    ...      ...\n",
       "IL7Ra_167  GITIFINADDPTVAELAKSANPKHAHFHPAGAVWIELDDPTASKIV...  False       62\n",
       "IL7Ra_168  SREDRAARIVLEALRQMIKNVEDPKDARLIYLKAEQAKKIVDDPTV...  False       63\n",
       "IL7Ra_169  DESQKETLTKLIKLAVKAIMNNDPDTAKKVVDKLRKVASEANDHMA...  False       61\n",
       "IL7Ra_170  DKEELKKKIHKLAQIVARHHREDDSTVNDVAVIVLKLLRQDTEEAL...  False       62\n",
       "IL7Ra_171  SLDEILKEVKKLAKLAGISEEEAEKTARRIRRAGDEELAKMMARQL...  False       52\n",
       "\n",
       "[171 rows x 3 columns]"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "binder_df_val[binder_df_val.index.str.startswith(\"IL7Ra\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "a4dcda87",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "# Reading in ESM-embeddings from folder: 100%|██████████| 7/7 [00:00<00:00, 519.61it/s]\n",
      "/zhome/c9/0/203261/DBL046_PP_osaul/DBL046_PP_osaul/tmp/ona_drafts/training_utils/dataset_utils.py:196: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  self.sequence_df[\"index_num\"] = np.arange(len(self.sequence_df))\n",
      "# Reading in ESM-embeddings from folder: 100%|██████████| 503/503 [00:00<00:00, 603.95it/s]\n",
      "# Reading in ESM-embeddings from folder: 100%|██████████| 9/9 [00:00<00:00, 229.02it/s]\n",
      "/zhome/c9/0/203261/DBL046_PP_osaul/DBL046_PP_osaul/tmp/ona_drafts/training_utils/dataset_utils.py:196: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  self.sequence_df[\"index_num\"] = np.arange(len(self.sequence_df))\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "[enforce fail at alloc_cpu.cpp:117] err == 0. DefaultCPUAllocator: can't allocate memory: you tried to allocate 3489408000 bytes. Error code 12 (Cannot allocate memory)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[94], line 5\u001b[0m\n\u001b[1;32m      2\u001b[0m binders_dataset_val \u001b[38;5;241m=\u001b[39m data_utils\u001b[38;5;241m.\u001b[39mCLIP_meta_analysis_dataset(binder_df_val, esm_encoding_paths\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m../data/meta_analysis/binders_embeddings\u001b[39m\u001b[38;5;124m\"\u001b[39m, embedding_dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1152\u001b[39m)\n\u001b[1;32m      4\u001b[0m targets_dataset_train \u001b[38;5;241m=\u001b[39m data_utils\u001b[38;5;241m.\u001b[39mCLIP_meta_analysis_dataset(target_df_train, esm_encoding_paths\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m../data/meta_analysis/targets_embeddings\u001b[39m\u001b[38;5;124m\"\u001b[39m, embedding_dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1152\u001b[39m)\n\u001b[0;32m----> 5\u001b[0m binders_dataset_train \u001b[38;5;241m=\u001b[39m \u001b[43mdata_utils\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mCLIP_meta_analysis_dataset\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbinder_df_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mesm_encoding_paths\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m../data/meta_analysis/binders_embeddings\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43membedding_dim\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1152\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;66;03m# %%\u001b[39;00m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;66;03m### Creating the CLIP datasets\u001b[39;00m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;66;03m# train_dataset = data_utils.CLIP_dataset(train_df,\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[38;5;66;03m#                         max_len=max_sequence_length)\u001b[39;00m\n\u001b[1;32m     22\u001b[0m \u001b[38;5;66;03m# test_dataset._get_item_from_PDB_index(unique_test_indexes_nondimer)[0].shape\u001b[39;00m\n\u001b[1;32m     24\u001b[0m \u001b[38;5;28;01mclass\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mPairDataset\u001b[39;00m(Dataset):\n",
      "File \u001b[0;32m~/DBL046_PP_osaul/DBL046_PP_osaul/tmp/ona_drafts/training_utils/dataset_utils.py:199\u001b[0m, in \u001b[0;36mCLIP_meta_analysis_dataset.__init__\u001b[0;34m(self, sequence_df, esm_encoding_paths, embedding_dim, padding_value)\u001b[0m\n\u001b[1;32m    197\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mesm_encoding_paths \u001b[38;5;241m=\u001b[39m esm_encoding_paths\n\u001b[1;32m    198\u001b[0m num_samples \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msequence_df)\n\u001b[0;32m--> 199\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mx \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfull\u001b[49m\u001b[43m(\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnum_samples\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmax_length\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43membedding_dim\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpadding_value\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfloat32\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    201\u001b[0m \u001b[38;5;66;03m# Load embeddings into the pre-allocated tensor\u001b[39;00m\n\u001b[1;32m    202\u001b[0m iterator \u001b[38;5;241m=\u001b[39m tqdm(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msequence_df\u001b[38;5;241m.\u001b[39mindex\u001b[38;5;241m.\u001b[39mtolist(), position\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m, leave\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, total\u001b[38;5;241m=\u001b[39mnum_samples, desc\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m# Reading in ESM-embeddings from folder\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: [enforce fail at alloc_cpu.cpp:117] err == 0. DefaultCPUAllocator: can't allocate memory: you tried to allocate 3489408000 bytes. Error code 12 (Cannot allocate memory)"
     ]
    }
   ],
   "source": [
    "targets_dataset_val = data_utils.CLIP_meta_analysis_dataset(target_df_val, esm_encoding_paths=\"../data/meta_analysis/targets_embeddings\", embedding_dim=1152)\n",
    "binders_dataset_val = data_utils.CLIP_meta_analysis_dataset(binder_df_val, esm_encoding_paths=\"../data/meta_analysis/binders_embeddings\", embedding_dim=1152)\n",
    "\n",
    "targets_dataset_train = data_utils.CLIP_meta_analysis_dataset(target_df_train, esm_encoding_paths=\"../data/meta_analysis/targets_embeddings\", embedding_dim=1152)\n",
    "binders_dataset_train = data_utils.CLIP_meta_analysis_dataset(binder_df_train, esm_encoding_paths=\"../data/meta_analysis/binders_embeddings\", embedding_dim=1152)\n",
    "\n",
    "# %%\n",
    "### Creating the CLIP datasets\n",
    "# train_dataset = data_utils.CLIP_dataset(train_df,\n",
    "#                        esm_encoding_paths=path_to_esm_embeddings,\n",
    "#                       embedding_dim=embedding_dimension,\n",
    "#                     padding_value=padding_value,\n",
    "#                     max_len=max_sequence_length,\n",
    "#                         # shuffle_embeddings=True\n",
    "#                             )\n",
    "\n",
    "# test_dataset = data_utils.CLIP_dataset(test_df,\n",
    "#                        esm_encoding_paths=path_to_esm_embeddings,\n",
    "#                       embedding_dim=embedding_dimension,\n",
    "#                       padding_value=padding_value,\n",
    "#                         max_len=max_sequence_length)\n",
    "# test_dataset._get_item_from_PDB_index(unique_test_indexes_nondimer)[0].shape\n",
    "\n",
    "class PairDataset(Dataset):\n",
    "    def __init__(self, targets_dataset, binders_dataset, binder_df):\n",
    "        self.targets = targets_dataset    # expects string-key access\n",
    "        self.binders = binders_dataset    # expects string-key access\n",
    "        self.binder_df = binder_df\n",
    "\n",
    "    def __len__(self):\n",
    "        # if you won't use DataLoader's default sampling, this can be any valid length;\n",
    "        # keeping it tied to binders is reasonable if it implements __len__.\n",
    "        return len(self.binders)\n",
    "\n",
    "    def __getitem__(self, bname):\n",
    "        # bname is already the binder name (string)\n",
    "        parts = bname.split(\"_\")\n",
    "        if len(parts) >= 3:\n",
    "            target_name = f\"{parts[0]}_{parts[1]}\"\n",
    "        elif len(parts) >= 2:\n",
    "            target_name = parts[0]\n",
    "        else:\n",
    "            target_name = parts[0]\n",
    "\n",
    "        binder_emb = self.binders[bname]\n",
    "        target_emb = self.targets[target_name]\n",
    "        label = int(self.binder_df.label.loc[bname])\n",
    "\n",
    "        return binder_emb, target_emb, label\n",
    "    \n",
    "training_dataset = PairDataset(targets_dataset_train, binders_dataset_train, binder_df_train)\n",
    "validation_dataset = PairDataset(targets_dataset_val, binders_dataset_val, binder_df_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9bcd3f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "503\n",
      "3029\n"
     ]
    }
   ],
   "source": [
    "__, __, label = validation_dataset[\"IL7Ra_4\"]\n",
    "label\n",
    "\n",
    "print(len(validation_dataset))\n",
    "print(len(training_dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d27b290",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch.utils.data.dataloader.DataLoader at 0x7f875a7cf2b0>"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# def collate_pad(batch, pad_value=-5000.0):\n",
    "#     \"\"\"\n",
    "#     Pads variable-length sequences in the batch to the max length (separately for binders and targets).\n",
    "#     Expects items like (binder_emb [Lb,D], target_emb [Lt,D], bname, tname).\n",
    "#     Returns:\n",
    "#       binders: [B, Lb_max, D], targets: [B, Lt_max, D], names: list of (bname, tname)\n",
    "#     \"\"\"\n",
    "#     binders, targets, bnames, tnames = [], [], [], []\n",
    "#     for binder_emb, target_emb, bname, tname in batch:\n",
    "#         binders.append(binder_emb)  # [L,D]\n",
    "#         targets.append(target_emb)  # [L,D]\n",
    "#         bnames.append(bname)\n",
    "#         tnames.append(tname)\n",
    "\n",
    "#     # pad along sequence length (dim=0), keep embed dim\n",
    "#     binders = pad_sequence(binders, batch_first=True, padding_value=pad_value)  # [B, Lb_max, D]\n",
    "#     targets = pad_sequence(targets, batch_first=True, padding_value=pad_value)  # [B, Lt_max, D]\n",
    "#     return binders, targets, bnames, tnames\n",
    "\n",
    "# # ---- Build datasets and loaders ----\n",
    "# train_dataset = PairDataset(targets_dataset_train, binders_dataset_train, list_binders_train)\n",
    "# val_dataset   = PairDataset(targets_dataset_val,   binders_dataset_val,   list_binders_val)\n",
    "\n",
    "train_loader = DataLoader(training_dataset, batch_size=32)\n",
    "val_loader = DataLoader(validation_dataset, batch_size=32)\n",
    "\n",
    "train_loader\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ba92e7e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>binder_chain</th>\n",
       "      <th>target_chains</th>\n",
       "      <th>binder</th>\n",
       "      <th>binder_seq</th>\n",
       "      <th>target_seq</th>\n",
       "      <th>target_id_mod</th>\n",
       "      <th>target_binder_ID</th>\n",
       "      <th>observation_weight</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>A</td>\n",
       "      <td>[\"B\"]</td>\n",
       "      <td>False</td>\n",
       "      <td>LDFIVFAGPEKAIKFYKEMAKRNLEVKIWIDGDWAVVQVK</td>\n",
       "      <td>ANPYISVANIMLQNYVKQREKYNYDTLKEQFTFIKNASTSIVYMQF...</td>\n",
       "      <td>VirB8</td>\n",
       "      <td>VirB8_1</td>\n",
       "      <td>0.000159</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>A</td>\n",
       "      <td>[\"B\"]</td>\n",
       "      <td>False</td>\n",
       "      <td>SEQDETMHRIVRSVIQHAYKHNDEMAEYFAQNAAEIYKEQNKSEEA...</td>\n",
       "      <td>RSPHRPILQAGLPANASTVVGGDVEFVCKVYSDAQPHIQWIKHVPY...</td>\n",
       "      <td>FGFR2</td>\n",
       "      <td>FGFR2_1</td>\n",
       "      <td>0.000159</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>A</td>\n",
       "      <td>[\"B\"]</td>\n",
       "      <td>False</td>\n",
       "      <td>DYKQLKKHATKLLELAKKDPSSKRDLLRTAASYANKVLFEDSDPRA...</td>\n",
       "      <td>RSPHRPILQAGLPANASTVVGGDVEFVCKVYSDAQPHIQWIKHVPY...</td>\n",
       "      <td>FGFR2</td>\n",
       "      <td>FGFR2_2</td>\n",
       "      <td>0.000159</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>A</td>\n",
       "      <td>[\"B\"]</td>\n",
       "      <td>False</td>\n",
       "      <td>DEKEELERRANRVAFLAIQIQNEEYHRILAELYVQFMKAAENNDTE...</td>\n",
       "      <td>RSPHRPILQAGLPANASTVVGGDVEFVCKVYSDAQPHIQWIKHVPY...</td>\n",
       "      <td>FGFR2</td>\n",
       "      <td>FGFR2_3</td>\n",
       "      <td>0.000159</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>A</td>\n",
       "      <td>[\"B\"]</td>\n",
       "      <td>False</td>\n",
       "      <td>PDNKEKLMSIAVQLILRINEAARSEEQWRYANRAAFAAVEASSGSD...</td>\n",
       "      <td>RSPHRPILQAGLPANASTVVGGDVEFVCKVYSDAQPHIQWIKHVPY...</td>\n",
       "      <td>FGFR2</td>\n",
       "      <td>FGFR2_4</td>\n",
       "      <td>0.000159</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3527</th>\n",
       "      <td>A</td>\n",
       "      <td>[\"B\"]</td>\n",
       "      <td>False</td>\n",
       "      <td>DLRKYAAELVDRLAEKYNLDSDQYNALVRLASELVWQGKSKEEIEK...</td>\n",
       "      <td>ELCDDDPPEIPHATFKAMAYKEGTMLNCECKRGFRRIKSGSLYMLC...</td>\n",
       "      <td>IL2Ra</td>\n",
       "      <td>IL2Ra_62</td>\n",
       "      <td>0.000159</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3528</th>\n",
       "      <td>A</td>\n",
       "      <td>[\"B\"]</td>\n",
       "      <td>False</td>\n",
       "      <td>SKEEIKKEAEELIEELKKKGYNLPLRILEFALKEIEETNSEKYYEQ...</td>\n",
       "      <td>ELCDDDPPEIPHATFKAMAYKEGTMLNCECKRGFRRIKSGSLYMLC...</td>\n",
       "      <td>IL2Ra</td>\n",
       "      <td>IL2Ra_63</td>\n",
       "      <td>0.000159</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3529</th>\n",
       "      <td>A</td>\n",
       "      <td>[\"B\"]</td>\n",
       "      <td>False</td>\n",
       "      <td>SPEYKKFLELIKEAEAARKAGDLDKAKELLEKALELAKKMKAKSLI...</td>\n",
       "      <td>ELCDDDPPEIPHATFKAMAYKEGTMLNCECKRGFRRIKSGSLYMLC...</td>\n",
       "      <td>IL2Ra</td>\n",
       "      <td>IL2Ra_64</td>\n",
       "      <td>0.000159</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3530</th>\n",
       "      <td>A</td>\n",
       "      <td>[\"B\"]</td>\n",
       "      <td>False</td>\n",
       "      <td>DPLLAYKLLKLSQKALEKAYAEDRERAEELLEEAEAALRSLGDEAG...</td>\n",
       "      <td>ELCDDDPPEIPHATFKAMAYKEGTMLNCECKRGFRRIKSGSLYMLC...</td>\n",
       "      <td>IL2Ra</td>\n",
       "      <td>IL2Ra_65</td>\n",
       "      <td>0.000159</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3531</th>\n",
       "      <td>A</td>\n",
       "      <td>[\"B\"]</td>\n",
       "      <td>False</td>\n",
       "      <td>SEAARRARELFHEADELDKRGNPEEAEEVLREALELAREAGSPNLA...</td>\n",
       "      <td>ELCDDDPPEIPHATFKAMAYKEGTMLNCECKRGFRRIKSGSLYMLC...</td>\n",
       "      <td>IL2Ra</td>\n",
       "      <td>IL2Ra_66</td>\n",
       "      <td>0.000159</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3532 rows × 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     binder_chain target_chains  binder  \\\n",
       "0               A         [\"B\"]   False   \n",
       "1               A         [\"B\"]   False   \n",
       "2               A         [\"B\"]   False   \n",
       "3               A         [\"B\"]   False   \n",
       "4               A         [\"B\"]   False   \n",
       "...           ...           ...     ...   \n",
       "3527            A         [\"B\"]   False   \n",
       "3528            A         [\"B\"]   False   \n",
       "3529            A         [\"B\"]   False   \n",
       "3530            A         [\"B\"]   False   \n",
       "3531            A         [\"B\"]   False   \n",
       "\n",
       "                                             binder_seq  \\\n",
       "0              LDFIVFAGPEKAIKFYKEMAKRNLEVKIWIDGDWAVVQVK   \n",
       "1     SEQDETMHRIVRSVIQHAYKHNDEMAEYFAQNAAEIYKEQNKSEEA...   \n",
       "2     DYKQLKKHATKLLELAKKDPSSKRDLLRTAASYANKVLFEDSDPRA...   \n",
       "3     DEKEELERRANRVAFLAIQIQNEEYHRILAELYVQFMKAAENNDTE...   \n",
       "4     PDNKEKLMSIAVQLILRINEAARSEEQWRYANRAAFAAVEASSGSD...   \n",
       "...                                                 ...   \n",
       "3527  DLRKYAAELVDRLAEKYNLDSDQYNALVRLASELVWQGKSKEEIEK...   \n",
       "3528  SKEEIKKEAEELIEELKKKGYNLPLRILEFALKEIEETNSEKYYEQ...   \n",
       "3529  SPEYKKFLELIKEAEAARKAGDLDKAKELLEKALELAKKMKAKSLI...   \n",
       "3530  DPLLAYKLLKLSQKALEKAYAEDRERAEELLEEAEAALRSLGDEAG...   \n",
       "3531  SEAARRARELFHEADELDKRGNPEEAEEVLREALELAREAGSPNLA...   \n",
       "\n",
       "                                             target_seq target_id_mod  \\\n",
       "0     ANPYISVANIMLQNYVKQREKYNYDTLKEQFTFIKNASTSIVYMQF...         VirB8   \n",
       "1     RSPHRPILQAGLPANASTVVGGDVEFVCKVYSDAQPHIQWIKHVPY...         FGFR2   \n",
       "2     RSPHRPILQAGLPANASTVVGGDVEFVCKVYSDAQPHIQWIKHVPY...         FGFR2   \n",
       "3     RSPHRPILQAGLPANASTVVGGDVEFVCKVYSDAQPHIQWIKHVPY...         FGFR2   \n",
       "4     RSPHRPILQAGLPANASTVVGGDVEFVCKVYSDAQPHIQWIKHVPY...         FGFR2   \n",
       "...                                                 ...           ...   \n",
       "3527  ELCDDDPPEIPHATFKAMAYKEGTMLNCECKRGFRRIKSGSLYMLC...         IL2Ra   \n",
       "3528  ELCDDDPPEIPHATFKAMAYKEGTMLNCECKRGFRRIKSGSLYMLC...         IL2Ra   \n",
       "3529  ELCDDDPPEIPHATFKAMAYKEGTMLNCECKRGFRRIKSGSLYMLC...         IL2Ra   \n",
       "3530  ELCDDDPPEIPHATFKAMAYKEGTMLNCECKRGFRRIKSGSLYMLC...         IL2Ra   \n",
       "3531  ELCDDDPPEIPHATFKAMAYKEGTMLNCECKRGFRRIKSGSLYMLC...         IL2Ra   \n",
       "\n",
       "     target_binder_ID  observation_weight  \n",
       "0             VirB8_1            0.000159  \n",
       "1             FGFR2_1            0.000159  \n",
       "2             FGFR2_2            0.000159  \n",
       "3             FGFR2_3            0.000159  \n",
       "4             FGFR2_4            0.000159  \n",
       "...               ...                 ...  \n",
       "3527         IL2Ra_62            0.000159  \n",
       "3528         IL2Ra_63            0.000159  \n",
       "3529         IL2Ra_64            0.000159  \n",
       "3530         IL2Ra_65            0.000159  \n",
       "3531         IL2Ra_66            0.000159  \n",
       "\n",
       "[3532 rows x 8 columns]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e8b928a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%\n",
    "# Variables for making the dataset\n",
    "# max_sequence_length = interaction_df[\"seq. len\"].max()\n",
    "# train_df = disordered_interfaces_df.loc[random.sample(train_indexes,k=int(len(train_indexes)*train_frac)),:].copy() # Making a sample of the training dataset\n",
    "# # train_df = disordered_interfaces_df.loc[train_indexes,:].copy() # Using the entire dataset\n",
    "# train_df = train_df[train_df.dimer == False] # Removing Dimers from the dataset (TODO: remove)\n",
    "\n",
    "# test_df = disordered_interfaces_df.loc[random.sample(test_indexes,k=int(len(test_indexes)*test_frac)),:].copy() # Making a sample of the testing dataset\n",
    "# # test_df = disordered_interfaces_df.loc[test_indexes,:].copy()\n",
    "# test_df = test_df[test_df.dimer == False]\n",
    "\n",
    "# # test_nondimer_df = test_df[(test_df.dimer == False) & (test_df[\"mean disorder interface\"] > 0.0)] # Here i can filter for different disorder scores in the interface\n",
    "# unique_test_indexes_nondimer = test_df.index.unique().tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "284db06b",
   "metadata": {},
   "outputs": [],
   "source": [
    "interaction_Dict = dict(enumerate(zip(interaction_df[\"target_id_mod\"], interaction_df[\"target_binder_ID\"]), start=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5ca69af",
   "metadata": {},
   "outputs": [],
   "source": [
    "random.seed(0)\n",
    "# ### Creating the DataLoaders\n",
    "train_weights = train_dataset._get_observation_weights()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cdef5b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%\n",
    "random.seed(0)\n",
    "# ### Creating the DataLoaders\n",
    "train_weights = train_dataset._get_observation_weights()\n",
    "# train_sampler = WeightedRandomSampler(train_weights, len(train_weights), replacement=True)\n",
    "train_dataloader = DataLoader(train_dataset, \n",
    "                              batch_size=batch_size, \n",
    "                              # sampler=train_sampler\n",
    "                             )\n",
    "val_weights = test_dataset._get_observation_weights()\n",
    "# val_sampler = WeightedRandomSampler(val_weights, len(val_weights), replacement=True)\n",
    "val_dataloader = DataLoader(test_dataset, \n",
    "                            batch_size=batch_size,\n",
    "                            # sampler=val_sampler\n",
    "                           )\n",
    "\n",
    "# Exanmple of getting observation from the dataset based on indexes\n",
    "# test_dimer_df = test_df[test_df.dimer == False]\n",
    "# unique_test_indexes_nondimer = test_dimer_df.index.unique().tolist()\n",
    "# input_x, input_y, int_x, int_y = test_dataset._get_item_from_PDB_index(unique_test_indexes_nondimer)\n",
    "\n",
    "# %%\n",
    "## Model Class\n",
    "### MiniClip\n",
    "def create_key_padding_mask(embeddings, padding_value=0, offset=10):\n",
    "    return (embeddings < (padding_value + offset)).all(dim=-1)\n",
    "\n",
    "def create_mean_of_non_masked(embeddings, padding_mask):\n",
    "    # Use masked select and mean to compute the mean of non-masked elements\n",
    "    # embeddings should be of shape (batch_size, seq_len, features)\n",
    "    seq_embeddings = []\n",
    "    for i in range(embeddings.shape[0]):\n",
    "        non_masked_embeddings = embeddings[i][~padding_mask[i]]\n",
    "        if len(non_masked_embeddings) == 0:\n",
    "            print(\"You are masking all positions when creating sequence representation\")\n",
    "            sys.exit(1)\n",
    "        mean_embedding = non_masked_embeddings.mean(dim=0)\n",
    "        seq_embeddings.append(mean_embedding)\n",
    "    \n",
    "    return torch.stack(seq_embeddings)\n",
    "\n",
    "class MiniCLIP_w_transformer_crossattn(pl.LightningModule):\n",
    "    def __init__(self, padding_value = -5000, embed_dimension=1280,num_recycles=1):\n",
    "        super().__init__()\n",
    "        self.num_recycles = num_recycles\n",
    "        self.padding_value = padding_value\n",
    "        self.embed_dimension = embed_dimension\n",
    "        self.logit_scale = nn.Parameter(torch.tensor(math.log(1/0.07)))  # ~CLIP init\n",
    "        self.transformerencoder =  nn.TransformerEncoderLayer(d_model=self.embed_dimension, \n",
    "                                                              nhead=8,\n",
    "                                                             dropout=0.1,\n",
    "                                                             batch_first=True,\n",
    "                                                            dim_feedforward=self.embed_dimension,\n",
    "                                                             )\n",
    " \n",
    "        self.norm = nn.LayerNorm(self.embed_dimension)  # For residual additions\n",
    "\n",
    "        self.cross_attn = nn.MultiheadAttention(\n",
    "            embed_dim=self.embed_dimension,\n",
    "            num_heads=8,\n",
    "            dropout=0.1,\n",
    "            batch_first=True)\n",
    "\n",
    "        self.prot_embedder = nn.Sequential(\n",
    "            nn.Linear(self.embed_dimension, 640),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(640, 320),\n",
    "        )\n",
    "        \n",
    "\n",
    " \n",
    "    def forward(self, pep_input, prot_input, pep_int_mask=None, prot_int_mask=None, int_prob=None, mem_save=True): # , pep_tokens, prot_tokens\n",
    "        \n",
    "        # PAdding masks\n",
    "        pep_mask = create_key_padding_mask(embeddings=pep_input, padding_value=self.padding_value)\n",
    "        prot_mask = create_key_padding_mask(embeddings=prot_input, padding_value=self.padding_value)\n",
    " \n",
    "        # Initialize residual states: start from the raw embeddings\n",
    "        pep_emb = pep_input.clone()\n",
    "        prot_emb = prot_input.clone()\n",
    " \n",
    "        for _ in range(self.num_recycles):\n",
    "\n",
    "            # Transformer encoding with residual\n",
    "            # Encode each sequence separately with the same Transformer encoder (weight sharing)\n",
    "            pep_trans = self.transformerencoder(self.norm(pep_emb), src_key_padding_mask=pep_mask)\n",
    "            prot_trans = self.transformerencoder(self.norm(prot_emb), src_key_padding_mask=prot_mask)\n",
    "            \n",
    "            # Cross-attention with residual\n",
    "            pep_cross, _ = self.cross_attn(query=self.norm(pep_trans), key=self.norm(prot_trans), value=self.norm(prot_trans), key_padding_mask=prot_mask)\n",
    "            prot_cross, _ = self.cross_attn(query=self.norm(prot_trans), key=self.norm(pep_trans), value=self.norm(pep_trans), key_padding_mask=pep_mask)\n",
    "            \n",
    "            # Additive update with residual connection\n",
    "            # Residual update (add the cross information back)\n",
    "            pep_emb = pep_emb + pep_cross  \n",
    "            prot_emb = prot_emb + prot_cross\n",
    "\n",
    "        # Mean-pool only over non-masked tokens (i.e., real residues):\n",
    "        pep_seq_coding = create_mean_of_non_masked(pep_emb, pep_mask)\n",
    "        prot_seq_coding = create_mean_of_non_masked(prot_emb, prot_mask)\n",
    "        \n",
    "        # Use self-attention outputs for embeddings\n",
    "        pep_seq_coding = F.normalize(self.prot_embedder(pep_seq_coding))\n",
    "        prot_seq_coding = F.normalize(self.prot_embedder(prot_seq_coding))\n",
    " \n",
    "        if mem_save:\n",
    "            torch.cuda.empty_cache()\n",
    "            # del pep_seq_coding, prot_seq_coding, pep_self_embedding, prot_self_embedding, pep_self_attn, prot_self_attn, pep_mask, prot_mask\n",
    "        \n",
    "        scale = torch.exp(self.logit_scale).clamp(max=100.0)\n",
    "        logits = scale * (pep_seq_coding * prot_seq_coding).sum(dim=-1) # Dot-Product for comparison\n",
    "        \n",
    "        return logits\n",
    "    \n",
    "\n",
    "    def training_step(self, batch, device):\n",
    "        # Passing the sequences to the models\n",
    " \n",
    "        embedding_pep = batch[0]\n",
    "        embedding_prot = batch[1]\n",
    "        # interaction_pep = batch[2]\n",
    "        # interaction_prot = batch[3]\n",
    " \n",
    "        embedding_pep = embedding_pep.to(device)\n",
    "        embedding_prot = embedding_prot.to(device)\n",
    "        \n",
    "        # interaction_pep = interaction_pep.to(device)\n",
    "        # interaction_prot = interaction_prot.to(device)\n",
    "\n",
    "        positive_logits = self(\n",
    "            embedding_pep,\n",
    "            embedding_prot,\n",
    "            # interaction_pep,\n",
    "            # interaction_prot,\n",
    "            # int_prob = 0.0\n",
    "        )\n",
    "        \n",
    "        # Negative indexes\n",
    "        rows, cols = torch.triu_indices(embedding_prot.size(0), embedding_prot.size(0), offset=1)         \n",
    "        \n",
    "        negative_logits = self(embedding_pep[rows,:,:], \n",
    "                          embedding_prot[cols,:,:], \n",
    "                          int_prob=0.0)\n",
    "\n",
    "        # loss of predicting partner using peptide\n",
    "        positive_loss = F.binary_cross_entropy_with_logits(positive_logits, torch.ones_like(positive_logits).to(device))\n",
    " \n",
    "        # loss of predicting peptide using partner\n",
    "        negative_loss =  F.binary_cross_entropy_with_logits(negative_logits, torch.zeros_like(negative_logits).to(device))\n",
    "        \n",
    "        loss = (positive_loss + negative_loss) / 2\n",
    " \n",
    "        # del partner_prediction_loss, peptide_prediction_loss, embedding_pep, embedding_prot\n",
    "        torch.cuda.empty_cache()\n",
    "        return loss\n",
    "    \n",
    "    def validation_step(self, batch, device):\n",
    "        # Predict on random batches of training batch size\n",
    "        embedding_pep = batch[0]\n",
    "        embedding_prot = batch[1]\n",
    "        # interaction_pep = batch[2]\n",
    "        # interaction_prot = batch[3]\n",
    "        # print()\n",
    "        # print(\"validation_step\")\n",
    "        # print(\"embedding_pep.shape\", embedding_pep.shape)\n",
    "        # print(\"embedding_prot.shape\", embedding_prot.shape)\n",
    "        # print(\"interaction_pep.shape\", interaction_pep.shape)\n",
    "        # print(\"interaction_prot.shape\", interaction_prot.shape)\n",
    " \n",
    "        embedding_pep = embedding_pep.to(device)\n",
    "        embedding_prot = embedding_prot.to(device)\n",
    "        \n",
    "        # interaction_pep = interaction_pep.to(device)\n",
    "        # interaction_prot = interaction_prot.to(device)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "\n",
    "            positive_logits = self(\n",
    "                    embedding_pep,\n",
    "                    embedding_prot,\n",
    "                    # interaction_pep,\n",
    "                    # interaction_prot,\n",
    "                    # int_prob = 0.0\n",
    "                    )\n",
    "            \n",
    "            # loss of predicting partner using peptide\n",
    "            positive_loss = F.binary_cross_entropy_with_logits(positive_logits, torch.ones_like(positive_logits).to(device))\n",
    "            \n",
    "            # Negaive indexes\n",
    "            rows, cols = torch.triu_indices(embedding_prot.size(0), embedding_prot.size(0), offset=1) \n",
    "            \n",
    "\n",
    "            negative_logits = self(embedding_pep[rows,:,:], \n",
    "                              embedding_prot[cols,:,:], \n",
    "                              int_prob=0.0)\n",
    "                   \n",
    "            \n",
    "            negative_loss =  F.binary_cross_entropy_with_logits(negative_logits, torch.zeros_like(negative_logits).to(device))\n",
    "\n",
    "            loss = (positive_loss + negative_loss) / 2\n",
    "           \n",
    "            logit_matrix = torch.zeros((embedding_pep.size(0),embedding_pep.size(0)),device=self.device)\n",
    "            logit_matrix[rows, cols] = negative_logits\n",
    "            logit_matrix[cols, rows] = negative_logits\n",
    "            \n",
    "            # Fill diagonal with positive scores\n",
    "            diag_indices = torch.arange(embedding_pep.size(0), device=self.device)\n",
    "            logit_matrix[diag_indices, diag_indices] = positive_logits.squeeze()\n",
    "\n",
    "            labels = torch.arange(embedding_prot.size(0)).to(self.device)\n",
    "            peptide_predictions = logit_matrix.argmax(dim=0)\n",
    "            peptide_ranks = logit_matrix.argsort(dim=0).diag() + 1\n",
    "            peptide_mrr = (peptide_ranks).float().pow(-1).mean()\n",
    "            \n",
    "            # partner_accuracy = partner_predictions.eq(labels).float().mean()\n",
    "            peptide_accuracy = peptide_predictions.eq(labels).float().mean()\n",
    "    \n",
    "            k = 3\n",
    "            peptide_topk_accuracy = torch.any((logit_matrix.topk(k, dim=0).indices - labels.reshape(1, -1)) == 0, dim=0).sum() / logit_matrix.shape[0]\n",
    "            # partner_topk_accuracy = torch.any((logits.topk(k, dim=1).indices - labels.reshape(-1, 1)) == 0, dim=1).sum() / logits.shape[0]\n",
    "    \n",
    "            # del peptide_predictions, partner_predictions, logits, peptide_ranks, peptide_mrr, partner_ranks,partner_mrr, embedding_pep, embedding_prot\n",
    "            del logit_matrix,positive_logits,negative_logits,embedding_pep,embedding_prot\n",
    "\n",
    "            return loss, peptide_accuracy, peptide_topk_accuracy\n",
    "\n",
    "    def calculate_logit_matrix(self,embedding_pep,embedding_prot):\n",
    "        positive_logits = self(\n",
    "            embedding_pep,\n",
    "            embedding_prot)\n",
    "        \n",
    "        # Negaive indexes\n",
    "        rows, cols = torch.triu_indices(embedding_pep.size(0), embedding_pep.size(0), offset=1) \n",
    "        \n",
    "        negative_logits = self(embedding_pep[rows,:,:], \n",
    "                      embedding_prot[cols,:,:], \n",
    "                      int_prob=0.0)\n",
    "        \n",
    "        logit_matrix = torch.zeros((embedding_pep.size(0),embedding_pep.size(0)),device=self.device)\n",
    "        logit_matrix[rows, cols] = negative_logits\n",
    "        logit_matrix[cols, rows] = negative_logits\n",
    "        # Fill diagonal with positive scores\n",
    "        diag_indices = torch.arange(embedding_pep.size(0), device=self.device)\n",
    "        logit_matrix[diag_indices, diag_indices] = positive_logits.squeeze()\n",
    "        return logit_matrix\n",
    "    \n",
    "\n",
    "\n",
    "# Importing the modules\n",
    "from model_architectures import MiniCLIP, MiniCLIP_cross_attn\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "model = MiniCLIP_w_transformer_crossattn(embed_dimension=embedding_dimension,\n",
    "                                        num_recycles=number_of_recycles)\n",
    "model = model.to(device)\n",
    "## Testing information flow from batch\n",
    "# batch = next(iter(train_dataloader))\n",
    "# with torch.no_grad():\n",
    "#     loss = model.training_step(batch,device)\n",
    "#     loss, partner_accuracy, peptide_topk_accuracy = model.validation_step(batch,device)\n",
    "    \n",
    "\n",
    "# Printing model paramaeters and clearing cache\n",
    "# print_mem_consumption()\n",
    "model_parameters = filter(lambda p: p.requires_grad, model.parameters())\n",
    "params = sum([np.prod(p.size()) for p in model_parameters])\n",
    "# torch.cuda.empty_cache()\n",
    "# print_mem_consumption()\n",
    "\n",
    "# %% [markdown]\n",
    "# ## Training loop\n",
    "\n",
    "\n",
    "# %%\n",
    "\n",
    "if use_wandb:\n",
    "    import wandb\n",
    "    wandb.init(\n",
    "        # set the wandb project where this run will be logged\n",
    "        project=\"CLIP_binder\",\n",
    "        name=str(runID),\n",
    "        # track hyperparameters and run metadata\n",
    "        config={\n",
    "        \"learning_rate\": learning_rate,\n",
    "        \"architecture\": str(model.__repr__),\n",
    "        \"Batch size\":batch_size,\n",
    "        \"dataset\": \"Small-sample dataset of 5 (5000)\",\n",
    "        \"training-procedure\":\"new_binary_cross\",\n",
    "        }\n",
    "    )\n",
    "    # Watch the model with wandb to log gradients, weights, and biases\n",
    "    wandb.watch(model, log='all')\n",
    "else:\n",
    "    print(\"WandB Tracking not used \")\n",
    "    wandb = False\n",
    "\n",
    "# %%\n",
    "# set the optimizer\n",
    "optimizer = AdamW(model.parameters(), lr=learning_rate)\n",
    "# initialize accelerator for training\n",
    "accelerator = Accelerator()\n",
    "model, optimizer, train_dataloader, val_dataloader = accelerator.prepare(model, optimizer, train_dataloader, val_dataloader)\n",
    "\n",
    "\n",
    "import importlib\n",
    "import training_utils.train_utils as train_utils\n",
    "importlib.reload(train_utils)\n",
    "\n",
    "training_wrapper = train_utils.TrainWrapper(\n",
    "                                            model=model, \n",
    "                                            training_loader=train_dataloader, \n",
    "                                            validation_loader=val_dataloader, \n",
    "                                            test_dataset=test_dataset,\n",
    "                                            optimizer=optimizer, \n",
    "                                            EPOCHS=EPOCHS,\n",
    "                                            runID=runID, \n",
    "                                            device=device, \n",
    "                                            test_indexes_for_auROC=unique_test_indexes_nondimer, \n",
    "                                            model_save_steps=model_save_steps,\n",
    "                                            model_save_path=trained_model_dir, \n",
    "                                            v=True, \n",
    "                                            wandb_tracker=wandb\n",
    "                                            )\n",
    "\n",
    "training_wrapper.train_model()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "esm_gpu",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
