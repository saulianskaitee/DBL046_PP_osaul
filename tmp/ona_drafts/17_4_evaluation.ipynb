{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7f838074-e829-40d5-9b15-949a9bfe9708",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tesla V100-SXM2-32GB\n"
     ]
    }
   ],
   "source": [
    "import uuid, sys, os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import ast\n",
    "import math\n",
    "import random\n",
    "\n",
    "from sklearn import metrics\n",
    "from scipy import stats\n",
    "from collections import Counter\n",
    "\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"1\"\n",
    "os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"expandable_segments:True\"\n",
    "import torch\n",
    "torch.cuda.set_device(0)  # 0 == \"first visible\" -> actually GPU 2 on the node\n",
    "print(torch.cuda.get_device_name(0))\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader, TensorDataset, WeightedRandomSampler\n",
    "import pytorch_lightning as pl\n",
    "from torch.optim import AdamW\n",
    "\n",
    "torch.manual_seed(0)\n",
    "\n",
    "from accelerate import Accelerator\n",
    "torch.cuda.empty_cache()\n",
    "import training_utils.partitioning_utils as pat_utils\n",
    "from tqdm import trange"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2318b4f4-87e8-46d5-9a0b-88e789db1aa9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /zhome/c9/0/203261/.netrc\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33ms232958\u001b[0m (\u001b[33ms232958-danmarks-tekniske-universitet-dtu\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import requests\n",
    "requests.get(\"https://api.wandb.ai/status\").status_code\n",
    "\n",
    "import wandb\n",
    "wandb.login(key=\"f8a6d759fe657b095d56bddbdb4d586dfaebd468\", relogin=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a771b897-1647-4594-b648-41d2639d5282",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Setting a seed to have the same initiation of weights\n",
    "\n",
    "def set_seed(seed: int = 42):\n",
    "    # Python & NumPy\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    \n",
    "    # PyTorch\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)  # for multi-GPU\n",
    "\n",
    "    # CuDNN settings (for convolution etc.)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "    # (Optional) for some Python hashing randomness\n",
    "    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n",
    "\n",
    "SEED = 0\n",
    "set_seed(SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "12ceffc5-ad46-4e77-aee2-49d4743bd362",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch: 2.1.0+cu118\n",
      "Using device: cuda\n",
      "Current location: /zhome/c9/0/203261/DBL046_PP_osaul/DBL046_PP_osaul/tmp/ona_drafts\n"
     ]
    }
   ],
   "source": [
    "os.chdir(\"/zhome/c9/0/203261/DBL046_PP_osaul/DBL046_PP_osaul/tmp/ona_drafts\")\n",
    "# print(os.getcwd())\n",
    "\n",
    "print(\"PyTorch:\", torch.__version__)\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print('Using device:', device)\n",
    "print(\"Current location:\", os.getcwd())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fcfc6c69-e492-4bf1-a23e-e8ee41a27d4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model parameters\n",
    "memory_verbose = False\n",
    "use_wandb = True # Used to track loss in real-time without printing\n",
    "model_save_steps = 1\n",
    "train_frac = 1.0\n",
    "test_frac = 1.0\n",
    "\n",
    "embedding_dimension = 1152 #| 960 | 1152\n",
    "number_of_recycles = 2\n",
    "padding_value = -5000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b152a01f-a72c-4d75-9e1c-6b6f937515eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total memory:  34.072559616\n",
      "Reserved memory:  0.0\n",
      "Allocated memory:  0.0\n",
      "Free memory:  0.0\n"
     ]
    }
   ],
   "source": [
    "# ## Training variables\n",
    "runID = uuid.uuid4()\n",
    "\n",
    "## Output path\n",
    "trained_model_dir = f\"/work3/s232958/data/trained/PPint_retrain10%_0.4_Christian/251116_{runID}\"\n",
    "\n",
    "def print_mem_consumption():\n",
    "    # 1. Total memory available on the GPU (device 0)\n",
    "    t = torch.cuda.get_device_properties(0).total_memory\n",
    "    # 2. How much memory PyTorch has *reserved* from CUDA\n",
    "    r = torch.cuda.memory_reserved(0)\n",
    "    # 3. How much of that reserved memory is actually *used* by tensors\n",
    "    a = torch.cuda.memory_allocated(0)\n",
    "    # 4. Reserved but not currently allocated (so “free inside PyTorch’s pool”)\n",
    "    f = r - a\n",
    "\n",
    "    print(\"Total memory: \", t/1e9)      # total VRAM in GB\n",
    "    print(\"Reserved memory: \", r/1e9)   # PyTorch’s reserved pool in GB\n",
    "    print(\"Allocated memory: \", a//1e9) # actually in use (integer division)\n",
    "    print(\"Free memory: \", f/1e9)       # slack in the reserved pool in GB\n",
    "print_mem_consumption()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6a54147-26d3-4e49-b87a-b05209b00a7d",
   "metadata": {},
   "source": [
    "### Loading PPint dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c2815ec4-5467-4b95-8d4b-84a3efb3e93b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████| 24725/24725.0 [00:38<00:00, 650.61it/s]\n",
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████| 27834/27834 [00:00<00:00, 622334.00it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8\n",
      "0.2\n"
     ]
    }
   ],
   "source": [
    "path_to_mmseqs_clustering = \"/work3/s232958/data/PPint_DB/3_å_dataset5_singlefasta/clusterRes40\"\n",
    "all_seqs, clust, clust_keys = pat_utils.mmseqs_parser(path_to_mmseqs_clustering)\n",
    "\n",
    "path_to_interaction_df = \"/work3/s232958/data/PPint_DB/disordered_interfaces_no_cutoff_filtered_nonredundant80_3å_5.csv.gz\"\n",
    "disordered_interfaces_df = pd.read_csv(path_to_interaction_df,index_col=0).reset_index(drop=True)\n",
    "disordered_interfaces_df[\"PDB_chain_name\"] = (disordered_interfaces_df[\"PDB\"] + \"_\" + disordered_interfaces_df[\"chainname\"]).tolist()\n",
    "disordered_interfaces_df[\"index_num\"] = np.arange(len(disordered_interfaces_df))\n",
    "disordered_interfaces_df[\"chain_name_index\"] = [row[\"PDB_chain_name\"] + \"_\" + str(row[\"index_num\"]) for index, row in disordered_interfaces_df.iterrows()]\n",
    "disordered_interfaces_df = disordered_interfaces_df.set_index(\"PDB_interface_name\")\n",
    "disordered_interfaces_df[\"interface_residues\"] = disordered_interfaces_df[\"interface_residues\"].apply(lambda x: ast.literal_eval(x))\n",
    "# disordered_interfaces_df[\"inter_chain_hamming\"] = [1 - (Ldistance(seq.split(\"-\")[0], seq.split(\"-\")[1]))/np.max([len(seq.split(\"-\")[0]), len(seq.split(\"-\")[1])]) for seq in disordered_interfaces_df[\"protien_interface_sequences\"]]\n",
    "disordered_interfaces_df[\"dimer\"] = disordered_interfaces_df[\"inter_chain_hamming\"] > 0.60\n",
    "disordered_interfaces_df[\"clust_keys\"] = [clust_keys.get(row[\"chain_name_index\"]) for index, row in disordered_interfaces_df.iterrows()] \n",
    "\n",
    "pdb_interface_and_clust_keys = {index:disordered_interfaces_df.loc[index,\"clust_keys\"].values.tolist() for index in tqdm(disordered_interfaces_df.index.drop_duplicates(), total=len(disordered_interfaces_df)/2)}\n",
    "new_clusters, new_clusters_clustkeys = pat_utils.recluster_mmseqs_keys_to_non_overlapping_groups(pdb_interface_and_clust_keys)\n",
    "\n",
    "### Creating train and test datasets based on train and test-idexes\n",
    "train_indexes, test_indexes = pat_utils.run_train_test_partition(interaction_df=disordered_interfaces_df,\n",
    "                                                    clustering=new_clusters, # Clusters from Bidentate-graphs\n",
    "                                                    train_ratio=0.8, \n",
    "                                                    test_ratio=0.2, \n",
    "                                                    v=True, \n",
    "                                                    seed=0)\n",
    "\n",
    "disordered_interfaces_df[\"ID\"] = [row[\"PDB\"]+\"_\"+str(row[\"interface_index\"])+\"_\"+row[\"chainname\"] for __, row in disordered_interfaces_df.iterrows()]\n",
    "disordered_interfaces_df[\"PDB_interface_name\"] = disordered_interfaces_df.index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ee1383d9-091f-4faf-8a52-b305dcafef8f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train indeces number: 19781\n",
      "Test indeces number: 4944\n"
     ]
    }
   ],
   "source": [
    "print(f\"Train indeces number: {len(train_indexes)}\")\n",
    "print(f\"Test indeces number: {len(test_indexes)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "836e72c5-0b99-4bd0-9b5a-5573c217d0e5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>interface_id</th>\n",
       "      <th>seq1</th>\n",
       "      <th>seq2</th>\n",
       "      <th>ID1</th>\n",
       "      <th>ID2</th>\n",
       "      <th>dimer</th>\n",
       "      <th>seq_target_len</th>\n",
       "      <th>seq_binder_len</th>\n",
       "      <th>target_binder_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>6NZA_0</td>\n",
       "      <td>MNTVRSEKDSMGAIDVPADKLWGAQTQRSLEHFRISTEKMPTSLIH...</td>\n",
       "      <td>TVRSEKDSMGAIDVPADKLWGAQTQRSLEHFRISTEKMPTSLIHAL...</td>\n",
       "      <td>6NZA_0_A</td>\n",
       "      <td>6NZA_0_B</td>\n",
       "      <td>True</td>\n",
       "      <td>461</td>\n",
       "      <td>459</td>\n",
       "      <td>6NZA_0_A_6NZA_0_B</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>9JKA_1</td>\n",
       "      <td>VAAGATLALLSFLTPLAFLLLPPLLWREELEPCGTACEGLFISVAF...</td>\n",
       "      <td>VAAGATLALLSFLTPLAFLLLPPLLWREELEPCGTACEGLFISVAF...</td>\n",
       "      <td>9JKA_1_B</td>\n",
       "      <td>9JKA_1_C</td>\n",
       "      <td>True</td>\n",
       "      <td>362</td>\n",
       "      <td>362</td>\n",
       "      <td>9JKA_1_B_9JKA_1_C</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>8DQ6_1</td>\n",
       "      <td>PTLNLFTNIPVDAVTCSDILKDATKAVAKIIGKPESYVMILLNSGV...</td>\n",
       "      <td>PTLNLFTNIPVDAVTCSDILKDATKAVAKIIGKPESYVMILLNSGV...</td>\n",
       "      <td>8DQ6_1_B</td>\n",
       "      <td>8DQ6_1_C</td>\n",
       "      <td>True</td>\n",
       "      <td>109</td>\n",
       "      <td>97</td>\n",
       "      <td>8DQ6_1_B_8DQ6_1_C</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2YMZ_0</td>\n",
       "      <td>ARMFEMFNLDWKSGGTMKIKGHISEDAESFAINLGCKSSDLALHFN...</td>\n",
       "      <td>ARMFEMFNLDWKSGGTMKIKGHISEDAESFAINLGCKSSDLALHFN...</td>\n",
       "      <td>2YMZ_0_A</td>\n",
       "      <td>2YMZ_0_B</td>\n",
       "      <td>True</td>\n",
       "      <td>130</td>\n",
       "      <td>130</td>\n",
       "      <td>2YMZ_0_A_2YMZ_0_B</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>6IDB_0</td>\n",
       "      <td>DKICLGHHAVSNGTKVNTLTERGVEVVNATETVERTNIPRICSKGK...</td>\n",
       "      <td>GLFGAIAGFIENGWEGLIDGWYGFRHQNAQGEGTAADYKSTQSAID...</td>\n",
       "      <td>6IDB_0_A</td>\n",
       "      <td>6IDB_0_B</td>\n",
       "      <td>False</td>\n",
       "      <td>317</td>\n",
       "      <td>172</td>\n",
       "      <td>6IDB_0_A_6IDB_0_B</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  interface_id                                               seq1  \\\n",
       "0       6NZA_0  MNTVRSEKDSMGAIDVPADKLWGAQTQRSLEHFRISTEKMPTSLIH...   \n",
       "1       9JKA_1  VAAGATLALLSFLTPLAFLLLPPLLWREELEPCGTACEGLFISVAF...   \n",
       "2       8DQ6_1  PTLNLFTNIPVDAVTCSDILKDATKAVAKIIGKPESYVMILLNSGV...   \n",
       "3       2YMZ_0  ARMFEMFNLDWKSGGTMKIKGHISEDAESFAINLGCKSSDLALHFN...   \n",
       "4       6IDB_0  DKICLGHHAVSNGTKVNTLTERGVEVVNATETVERTNIPRICSKGK...   \n",
       "\n",
       "                                                seq2       ID1       ID2  \\\n",
       "0  TVRSEKDSMGAIDVPADKLWGAQTQRSLEHFRISTEKMPTSLIHAL...  6NZA_0_A  6NZA_0_B   \n",
       "1  VAAGATLALLSFLTPLAFLLLPPLLWREELEPCGTACEGLFISVAF...  9JKA_1_B  9JKA_1_C   \n",
       "2  PTLNLFTNIPVDAVTCSDILKDATKAVAKIIGKPESYVMILLNSGV...  8DQ6_1_B  8DQ6_1_C   \n",
       "3  ARMFEMFNLDWKSGGTMKIKGHISEDAESFAINLGCKSSDLALHFN...  2YMZ_0_A  2YMZ_0_B   \n",
       "4  GLFGAIAGFIENGWEGLIDGWYGFRHQNAQGEGTAADYKSTQSAID...  6IDB_0_A  6IDB_0_B   \n",
       "\n",
       "   dimer  seq_target_len  seq_binder_len   target_binder_id  \n",
       "0   True             461             459  6NZA_0_A_6NZA_0_B  \n",
       "1   True             362             362  9JKA_1_B_9JKA_1_C  \n",
       "2   True             109              97  8DQ6_1_B_8DQ6_1_C  \n",
       "3   True             130             130  2YMZ_0_A_2YMZ_0_B  \n",
       "4  False             317             172  6IDB_0_A_6IDB_0_B  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Creating new dataframe with pairs of proteins (PPints)\n",
    "grouped = {}\n",
    "for _, row in disordered_interfaces_df.iterrows():\n",
    "    iface = row[\"PDB_interface_name\"]\n",
    "    seq = row[\"sequence\"]\n",
    "    rid = row[\"ID\"]\n",
    "    dimer = row[\"dimer\"]\n",
    "    \n",
    "    if iface not in grouped:\n",
    "        grouped[iface] = {\n",
    "            \"sequences\": [],\n",
    "            \"IDs\": [],\n",
    "            \"dimer\": dimer,        # keep the dimer value for this interface\n",
    "        }\n",
    "    else:\n",
    "        # Optional: sanity-check it's consistent per interface\n",
    "        if grouped[iface][\"dimer\"] != dimer:\n",
    "            print(f\"Warning: multiple dimers for interface {iface}:\",\n",
    "                  grouped[iface]['dimer'], \"vs\", dimer)\n",
    "\n",
    "    grouped[iface][\"sequences\"].append(seq)\n",
    "    grouped[iface][\"IDs\"].append(rid)\n",
    "\n",
    "records = []\n",
    "for iface, vals in grouped.items():\n",
    "    seqs = vals[\"sequences\"]\n",
    "    ids = vals[\"IDs\"]\n",
    "    if len(seqs) >= 2 and len(ids) >= 2:\n",
    "        records.append({\n",
    "            \"interface_id\": iface,\n",
    "            \"seq1\": seqs[0],\n",
    "            \"seq2\": seqs[1],\n",
    "            \"ID1\": ids[0],\n",
    "            \"ID2\": ids[1],\n",
    "            \"dimer\": vals[\"dimer\"],   # <- add dimer to final record\n",
    "        })\n",
    "\n",
    "PPint_interactions_NEW = pd.DataFrame(records)\n",
    "PPint_interactions_NEW[\"seq_target_len\"] = [len(row.seq1) for __, row in PPint_interactions_NEW.iterrows()]\n",
    "PPint_interactions_NEW[\"seq_binder_len\"] = [len(row.seq2) for __, row in PPint_interactions_NEW.iterrows()]\n",
    "PPint_interactions_NEW[\"target_binder_id\"] = PPint_interactions_NEW[\"ID1\"] + \"_\" + PPint_interactions_NEW[\"ID2\"]\n",
    "\n",
    "PPint_interactions_NEW.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4fa4409c-0d65-455b-9db0-d67f9c205413",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sample random 10%\n",
    "train_indexes_sample = random.sample(train_indexes, int(len(train_indexes) * 0.1))\n",
    "test_indexes_sample = random.sample(test_indexes, int(len(test_indexes) * 0.1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "97ad8c32-1f55-43bc-9dd6-ae99594a7fa0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>interface_id</th>\n",
       "      <th>seq1</th>\n",
       "      <th>seq2</th>\n",
       "      <th>ID1</th>\n",
       "      <th>ID2</th>\n",
       "      <th>dimer</th>\n",
       "      <th>seq_target_len</th>\n",
       "      <th>seq_binder_len</th>\n",
       "      <th>target_binder_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>4POB_0</td>\n",
       "      <td>DHATVTVTDDSFQEDVVSSNKPVLVDFWATWCGPCKMVAPVLEEIA...</td>\n",
       "      <td>ATVTVTDDSFQEDVVSSNKPVLVDFWATWCGPCKMVAPVLEEIAKD...</td>\n",
       "      <td>4POB_0_A</td>\n",
       "      <td>4POB_0_B</td>\n",
       "      <td>True</td>\n",
       "      <td>107</td>\n",
       "      <td>105</td>\n",
       "      <td>4POB_0_A_4POB_0_B</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>7T6C_0</td>\n",
       "      <td>YYPFVRKALFQLDPERAHEFTFQQLRRITGTPFEALVRQKVPAKPV...</td>\n",
       "      <td>YYPFVRKALFQLDPERAHEFTFQQLRRITGTPFEALVRQKVPAKPV...</td>\n",
       "      <td>7T6C_0_A</td>\n",
       "      <td>7T6C_0_B</td>\n",
       "      <td>True</td>\n",
       "      <td>335</td>\n",
       "      <td>335</td>\n",
       "      <td>7T6C_0_A_7T6C_0_B</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>1EGP_0</td>\n",
       "      <td>LKSFPEVVGKTVDQAREYFTLHYPQYNVYFLPEGSPVTL</td>\n",
       "      <td>YNRVRVFYNPGTNVVNHVPHVG</td>\n",
       "      <td>1EGP_0_A</td>\n",
       "      <td>1EGP_0_B</td>\n",
       "      <td>False</td>\n",
       "      <td>39</td>\n",
       "      <td>22</td>\n",
       "      <td>1EGP_0_A_1EGP_0_B</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>62</th>\n",
       "      <td>7YH3_0</td>\n",
       "      <td>KVENPLLISLYSHYVEQILSETNSIDDANQKLRDLGKELGQQIYLN...</td>\n",
       "      <td>KVENPLLISLYSHYVEQILSETNSIDDANQKLRDLGKELGQQIYLN...</td>\n",
       "      <td>7YH3_0_A</td>\n",
       "      <td>7YH3_0_C</td>\n",
       "      <td>True</td>\n",
       "      <td>150</td>\n",
       "      <td>155</td>\n",
       "      <td>7YH3_0_A_7YH3_0_C</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>70</th>\n",
       "      <td>4WMO_1</td>\n",
       "      <td>GYRSCNEIKSSDSRAPDGIYTLATEDGESYQTFCDTTNGGGWTLVA...</td>\n",
       "      <td>GYRSCNEIKSSDSRAPDGIYTLATEDGESYQTFCDTTNGGGWTLVA...</td>\n",
       "      <td>4WMO_1_D</td>\n",
       "      <td>4WMO_1_E</td>\n",
       "      <td>True</td>\n",
       "      <td>271</td>\n",
       "      <td>271</td>\n",
       "      <td>4WMO_1_D_4WMO_1_E</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24455</th>\n",
       "      <td>2OYS_0</td>\n",
       "      <td>NKIFIYAGVRNHNSKTLEYTKRLSSIISSRNNVDISFRTPFNSELE...</td>\n",
       "      <td>NKIFIYAGVRNHNSKTLEYTKRLSSIISSRNNVDISFRTPFNSELE...</td>\n",
       "      <td>2OYS_0_A</td>\n",
       "      <td>2OYS_0_B</td>\n",
       "      <td>True</td>\n",
       "      <td>227</td>\n",
       "      <td>227</td>\n",
       "      <td>2OYS_0_A_2OYS_0_B</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24491</th>\n",
       "      <td>6XRF_1</td>\n",
       "      <td>TLYRLHEADLEIPDAWQDQSINIFKLPASGPAREASFVISRDASQG...</td>\n",
       "      <td>MDAQAAARLGDEIAHGFGVAAMVAGAVAGALIGAAVVAAATGGLAA...</td>\n",
       "      <td>6XRF_1_B</td>\n",
       "      <td>6XRF_1_C</td>\n",
       "      <td>False</td>\n",
       "      <td>140</td>\n",
       "      <td>57</td>\n",
       "      <td>6XRF_1_B_6XRF_1_C</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24507</th>\n",
       "      <td>5Z2L_0</td>\n",
       "      <td>GAFTGKTVLILGGSRGIGAAIVRRFVTDGANVRFTYAGSKDAAKRL...</td>\n",
       "      <td>GAFTGKTVLILGGSRGIGAAIVRRFVTDGANVRFTYAGSKDAAKRL...</td>\n",
       "      <td>5Z2L_0_A</td>\n",
       "      <td>5Z2L_0_B</td>\n",
       "      <td>True</td>\n",
       "      <td>239</td>\n",
       "      <td>243</td>\n",
       "      <td>5Z2L_0_A_5Z2L_0_B</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24617</th>\n",
       "      <td>4LRS_0</td>\n",
       "      <td>APRVRITDSTLRDGSHAMAHQFTEEQVRATVHALDAAGVEVIEVSH...</td>\n",
       "      <td>GKAVAAIVGPGNIGTDLLIKLQRSEHIEVRYMVGVDPASEGLARAR...</td>\n",
       "      <td>4LRS_0_A</td>\n",
       "      <td>4LRS_0_B</td>\n",
       "      <td>False</td>\n",
       "      <td>337</td>\n",
       "      <td>294</td>\n",
       "      <td>4LRS_0_A_4LRS_0_B</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24691</th>\n",
       "      <td>6MGN_0</td>\n",
       "      <td>RVRDINEAFRELGRMCQMHLKSDKAQTKLLILQQAVQVILGLEQQV...</td>\n",
       "      <td>MLYDMNGCYSRLKELVPTLPQNRKVSKVEILQHVIDYIRDLQLELNS</td>\n",
       "      <td>6MGN_0_A</td>\n",
       "      <td>6MGN_0_B</td>\n",
       "      <td>False</td>\n",
       "      <td>52</td>\n",
       "      <td>47</td>\n",
       "      <td>6MGN_0_A_6MGN_0_B</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>494 rows × 9 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      interface_id                                               seq1  \\\n",
       "7           4POB_0  DHATVTVTDDSFQEDVVSSNKPVLVDFWATWCGPCKMVAPVLEEIA...   \n",
       "12          7T6C_0  YYPFVRKALFQLDPERAHEFTFQQLRRITGTPFEALVRQKVPAKPV...   \n",
       "28          1EGP_0            LKSFPEVVGKTVDQAREYFTLHYPQYNVYFLPEGSPVTL   \n",
       "62          7YH3_0  KVENPLLISLYSHYVEQILSETNSIDDANQKLRDLGKELGQQIYLN...   \n",
       "70          4WMO_1  GYRSCNEIKSSDSRAPDGIYTLATEDGESYQTFCDTTNGGGWTLVA...   \n",
       "...            ...                                                ...   \n",
       "24455       2OYS_0  NKIFIYAGVRNHNSKTLEYTKRLSSIISSRNNVDISFRTPFNSELE...   \n",
       "24491       6XRF_1  TLYRLHEADLEIPDAWQDQSINIFKLPASGPAREASFVISRDASQG...   \n",
       "24507       5Z2L_0  GAFTGKTVLILGGSRGIGAAIVRRFVTDGANVRFTYAGSKDAAKRL...   \n",
       "24617       4LRS_0  APRVRITDSTLRDGSHAMAHQFTEEQVRATVHALDAAGVEVIEVSH...   \n",
       "24691       6MGN_0  RVRDINEAFRELGRMCQMHLKSDKAQTKLLILQQAVQVILGLEQQV...   \n",
       "\n",
       "                                                    seq2       ID1       ID2  \\\n",
       "7      ATVTVTDDSFQEDVVSSNKPVLVDFWATWCGPCKMVAPVLEEIAKD...  4POB_0_A  4POB_0_B   \n",
       "12     YYPFVRKALFQLDPERAHEFTFQQLRRITGTPFEALVRQKVPAKPV...  7T6C_0_A  7T6C_0_B   \n",
       "28                                YNRVRVFYNPGTNVVNHVPHVG  1EGP_0_A  1EGP_0_B   \n",
       "62     KVENPLLISLYSHYVEQILSETNSIDDANQKLRDLGKELGQQIYLN...  7YH3_0_A  7YH3_0_C   \n",
       "70     GYRSCNEIKSSDSRAPDGIYTLATEDGESYQTFCDTTNGGGWTLVA...  4WMO_1_D  4WMO_1_E   \n",
       "...                                                  ...       ...       ...   \n",
       "24455  NKIFIYAGVRNHNSKTLEYTKRLSSIISSRNNVDISFRTPFNSELE...  2OYS_0_A  2OYS_0_B   \n",
       "24491  MDAQAAARLGDEIAHGFGVAAMVAGAVAGALIGAAVVAAATGGLAA...  6XRF_1_B  6XRF_1_C   \n",
       "24507  GAFTGKTVLILGGSRGIGAAIVRRFVTDGANVRFTYAGSKDAAKRL...  5Z2L_0_A  5Z2L_0_B   \n",
       "24617  GKAVAAIVGPGNIGTDLLIKLQRSEHIEVRYMVGVDPASEGLARAR...  4LRS_0_A  4LRS_0_B   \n",
       "24691    MLYDMNGCYSRLKELVPTLPQNRKVSKVEILQHVIDYIRDLQLELNS  6MGN_0_A  6MGN_0_B   \n",
       "\n",
       "       dimer  seq_target_len  seq_binder_len   target_binder_id  \n",
       "7       True             107             105  4POB_0_A_4POB_0_B  \n",
       "12      True             335             335  7T6C_0_A_7T6C_0_B  \n",
       "28     False              39              22  1EGP_0_A_1EGP_0_B  \n",
       "62      True             150             155  7YH3_0_A_7YH3_0_C  \n",
       "70      True             271             271  4WMO_1_D_4WMO_1_E  \n",
       "...      ...             ...             ...                ...  \n",
       "24455   True             227             227  2OYS_0_A_2OYS_0_B  \n",
       "24491  False             140              57  6XRF_1_B_6XRF_1_C  \n",
       "24507   True             239             243  5Z2L_0_A_5Z2L_0_B  \n",
       "24617  False             337             294  4LRS_0_A_4LRS_0_B  \n",
       "24691  False              52              47  6MGN_0_A_6MGN_0_B  \n",
       "\n",
       "[494 rows x 9 columns]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Df_test = PPint_interactions_NEW[PPint_interactions_NEW.interface_id.isin(test_indexes_sample)]\n",
    "Df_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "599ddad9-516d-4856-a675-ac3b5f684fca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# interaction_Dict = {}\n",
    "\n",
    "# for _, row in Df_train.iterrows():\n",
    "#     key_prot, seq_prot = row['ID1'], row['seq1']\n",
    "#     key_pep, seq_pep = row['ID2'], row['seq2']\n",
    "#     interaction_Dict[key_prot] = seq_prot\n",
    "#     interaction_Dict[key_pep] = seq_pep\n",
    "\n",
    "# for _, row in Df_test.iterrows():\n",
    "#     key_prot, seq_prot = row['ID1'], row['seq1']\n",
    "#     key_pep, seq_pep = row['ID2'], row['seq2']\n",
    "#     interaction_Dict[key_prot] = seq_prot\n",
    "#     interaction_Dict[key_pep] = seq_pep\n",
    "\n",
    "# assert (len(list(interaction_Dict.items())) == (len(Df_test) + len(Df_train))*2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "7518790d-4e1c-4b8f-bda4-b79fbdeafa06",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/work3/s232958/tmp/ipykernel_2521746/783615165.py:16: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  state_dict = torch.load(weights_path, map_location=device)\n"
     ]
    }
   ],
   "source": [
    "# from pathlib import Path\n",
    "# from esm.models.esmc import ESMC\n",
    "# from esm.models.esmc import ESMC\n",
    "# from esm.sdk.api import ESMProtein, LogitsConfig\n",
    "# from esm.pretrained import get_esmc_model_tokenizers  \n",
    "\n",
    "# device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# tokenizer = get_esmc_model_tokenizers()\n",
    "# model = ESMC(\n",
    "#     d_model=1152,\n",
    "#     n_heads=18,\n",
    "#     n_layers=36,\n",
    "#     tokenizer=tokenizer,\n",
    "# ).eval()\n",
    "\n",
    "# weights_path = Path(\"/work3/s232958/models/esmc-600m-2024-12/data/weights/esmc_600m_2024_12_v0.pth\")\n",
    "# state_dict = torch.load(weights_path, map_location=device)\n",
    "\n",
    "# model.load_state_dict(state_dict)\n",
    "# client = model.to(device)  # or whatever variable you used\n",
    "# client.eval()\n",
    "\n",
    "# def calculate_ESM_pr_res_embeddings(sequence):\n",
    "#     protein = ESMProtein(sequence=sequence)\n",
    "#     protein_tensor = client.encode(protein)\n",
    "#     logits_output = client.logits(\n",
    "#     protein_tensor, LogitsConfig(sequence=True, return_embeddings=True)\n",
    "#     )\n",
    "#     return logits_output.embeddings.detach().cpu().numpy()\n",
    "\n",
    "# def to_numpy(x):\n",
    "#     try:\n",
    "#         return x.detach().cpu().numpy()\n",
    "#     except AttributeError:\n",
    "#         return np.asarray(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "d7e6fcd7-0d15-46fd-a1aa-ffe7fccc908e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Embedding PPint: 100%|██████████████████████████████████████████████████████████████████████████████████████████| 4944/4944 [04:17<00:00, 19.20it/s]\n"
     ]
    }
   ],
   "source": [
    "# path_to_output_embeddings = \"/work3/s232958/data/PPint_DB/embeddings_esmC\"\n",
    "\n",
    "# for name, seq in tqdm(interaction_Dict.items(), total=len(interaction_Dict.items()), desc=\"Embedding PPint\"):\n",
    "#     emb = calculate_ESM_pr_res_embeddings(seq)\n",
    "#     emb_np = to_numpy(emb)\n",
    "#     out_path = os.path.join(path_to_output_embeddings, f\"{name}.npy\")\n",
    "#     np.save(out_path, emb_np)\n",
    "#     # print(f\"Protein {name} embedded and saved to {out_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48a572b0-4361-48fd-8573-ea5182eac03f",
   "metadata": {},
   "source": [
    "#### CLIP_PPint_analysis_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "1f587290-f774-42ba-bc41-b24a0d3ac163",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "#Loading ESMC embeddings: 100%|███████████████████████████████████████████████████████████████████████████████████| 494/494 [00:10<00:00, 48.24it/s]\n"
     ]
    }
   ],
   "source": [
    "class CLIP_PPint_analysis_dataset(Dataset):\n",
    "    def __init__(\n",
    "        self,\n",
    "        dframe,\n",
    "        paths,\n",
    "        embedding_dim=1152,\n",
    "        embedding_pad_value=-5000.0,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.dframe = dframe.copy()\n",
    "        self.embedding_dim = int(embedding_dim)\n",
    "        self.emb_pad = float(embedding_pad_value)\n",
    "\n",
    "        # lengths\n",
    "        self.max_blen = self.dframe[\"seq_binder_len\"].max()\n",
    "        self.max_tlen = self.dframe[\"seq_target_len\"].max()\n",
    "\n",
    "        # paths\n",
    "        self.encoding_bpath, self.encoding_tpath = paths\n",
    "\n",
    "        # index & storage\n",
    "        self.dframe.set_index(\"target_binder_id\", inplace=True)\n",
    "        self.accessions = self.dframe.index.astype(str).tolist()\n",
    "        self.name_to_row = {name: i for i, name in enumerate(self.accessions)}\n",
    "        self.samples = []\n",
    "\n",
    "        for accession in tqdm(self.accessions, total=len(self.accessions), desc=\"#Loading ESMC embeddings\"):\n",
    "            parts = accession.split(\"_\") # e.g. accession 7S8T_5_F_7S8T_5_G\n",
    "            tgt_id = \"_\".join(parts[:3])\n",
    "            bnd_id = \"_\".join(parts[3:])\n",
    "\n",
    "            ### --- embeddings (pad to fixed lengths) --- ###\n",
    "            \n",
    "            # laod embeddings\n",
    "            t_emb = np.load(os.path.join(self.encoding_tpath, f\"{tgt_id}.npy\"))[0]     # [Lt, D]\n",
    "            b_emb = np.load(os.path.join(self.encoding_bpath, f\"{bnd_id}.npy\"))[0]     # [Lb, D]\n",
    "\n",
    "            # quich check whether embedding dimmension is as it suppose to be\n",
    "            if t_emb.shape[1] != self.embedding_dim or b_emb.shape[1] != self.embedding_dim:\n",
    "                raise ValueError(\"Embedding dim mismatch with 'embedding_dim'.\")\n",
    "\n",
    "            # add -5000 to all the padded target rows\n",
    "            if t_emb.shape[0] < self.max_tlen:\n",
    "                t_emb = np.concatenate([t_emb, np.full((self.max_tlen - t_emb.shape[0], t_emb.shape[1]), self.emb_pad, dtype=t_emb.dtype)], axis=0)\n",
    "            else:\n",
    "                t_emb = t_emb[: self.max_tlen] # no padding was used\n",
    "\n",
    "            # add -5000 to all the padded binder rows\n",
    "            if b_emb.shape[0] < self.max_blen:\n",
    "                b_emb = np.concatenate([b_emb, np.full((self.max_blen - b_emb.shape[0], b_emb.shape[1]), self.emb_pad, dtype=b_emb.dtype)], axis=0)\n",
    "            else:\n",
    "                b_emb = b_emb[: self.max_blen] # no padding was used\n",
    "\n",
    "            self.samples.append((b_emb, t_emb))\n",
    "\n",
    "    # ---- Dataset API ----\n",
    "    def __len__(self):\n",
    "        return len(self.samples)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        b_arr, t_arr = self.samples[idx]\n",
    "        binder_emb, target_emb = torch.from_numpy(b_arr).float(), torch.from_numpy(t_arr).float()\n",
    "        label = torch.tensor(1, dtype=torch.float32)  # single scalar labe\n",
    "        return binder_emb, target_emb, label\n",
    "\n",
    "    def _get_by_name(self, name):\n",
    "        # Single item -> return exactly what __getitem__ returns\n",
    "        if isinstance(name, str):\n",
    "            return self.__getitem__(self.name_to_row[name])\n",
    "        \n",
    "        # Multiple items -> fetch all\n",
    "        out = [self.__getitem__(self.name_to_row[n]) for n in list(name)]\n",
    "        b_list, t_list, lbl_list = zip(*out)\n",
    "    \n",
    "        # Stack embeddings\n",
    "        b  = torch.stack([torch.as_tensor(x) for x in b_list],  dim=0)  # [B, ...]\n",
    "        t  = torch.stack([torch.as_tensor(x) for x in t_list],  dim=0)  # [B, ...]\n",
    "    \n",
    "        # Stack labels\n",
    "        labels = torch.stack(lbl_list)  # [B]\n",
    "    \n",
    "        return b, t, labels\n",
    "\n",
    "bemb_path = \"/work3/s232958/data/PPint_DB/embeddings_esmC\"\n",
    "temb_path = \"/work3/s232958/data/PPint_DB/embeddings_esmC\"\n",
    "\n",
    "testing_Dataset = CLIP_PPint_analysis_dataset(\n",
    "    Df_test,\n",
    "    paths=[bemb_path, temb_path],\n",
    "    embedding_dim=1152\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73e53f5d-e653-47c0-97e0-9bbcdd9d2e1b",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "#### ESMC embedding meta-analysis dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "aa42385b-627f-474f-9af8-623fda68ccbb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/work3/s232958/tmp/ipykernel_2521746/3709663317.py:27: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  state_dict = torch.load(weights_path, map_location=device)\n",
      "Embedding Meta targets: 100%|███████████████████████████████████████████████████████████████████████████████████████| 16/16 [00:00<00:00, 19.89it/s]\n",
      "Embedding Meta binders: 100%|███████████████████████████████████████████████████████████████████████████████████| 3532/3532 [02:53<00:00, 20.41it/s]\n"
     ]
    }
   ],
   "source": [
    "# meta_targets, meta_binders = {}, {}\n",
    "\n",
    "# for _, row in interaction_df_shuffled.iterrows():\n",
    "#     key_prot, seq_prot = row['target_id'], row['seq_target']\n",
    "#     key_pep, seq_pep = row['binder_id'], row['seq_binder']\n",
    "#     if key_prot not in meta_targets.keys():\n",
    "#         meta_targets[key_prot] = seq_prot\n",
    "#     else:\n",
    "#         pass\n",
    "#     meta_binders[key_pep] = seq_pep\n",
    "\n",
    "# from pathlib import Path\n",
    "# from esm.models.esmc import ESMC\n",
    "# from esm.pretrained import get_esmc_model_tokenizers  \n",
    "\n",
    "# device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# tokenizer = get_esmc_model_tokenizers()\n",
    "# model = ESMC(\n",
    "#     d_model=1152,\n",
    "#     n_heads=18,\n",
    "#     n_layers=36,\n",
    "#     tokenizer=tokenizer,\n",
    "# ).eval()\n",
    "\n",
    "# weights_path = Path(\"/work3/s232958/models/esmc-600m-2024-12/data/weights/esmc_600m_2024_12_v0.pth\")\n",
    "# state_dict = torch.load(weights_path, map_location=device)\n",
    "\n",
    "# model.load_state_dict(state_dict)\n",
    "# client = model.to(device)  # or whatever variable you used\n",
    "# client.eval()\n",
    "\n",
    "# def calculate_ESM_pr_res_embeddings(sequence):\n",
    "#     protein = ESMProtein(sequence=sequence)\n",
    "#     protein_tensor = client.encode(protein)\n",
    "#     logits_output = client.logits(\n",
    "#     protein_tensor, LogitsConfig(sequence=True, return_embeddings=True)\n",
    "#     )\n",
    "#     return logits_output.embeddings.detach().cpu().numpy()\n",
    "\n",
    "# def to_numpy(x):\n",
    "#     try:\n",
    "#         return x.detach().cpu().numpy()\n",
    "#     except AttributeError:\n",
    "#         return np.asarray(x)\n",
    "\n",
    "# for name, seq in tqdm(meta_targets.items(), total=len(meta_targets.items()), desc=\"Embedding Meta targets\"):\n",
    "#     emb = calculate_ESM_pr_res_embeddings(seq)\n",
    "#     emb_np = to_numpy(emb)\n",
    "#     out_path = os.path.join(\"/work3/s232958/data/meta_analysis/targets_embeddings_esmC\", f\"{name}.npy\")\n",
    "#     np.save(out_path, emb_np)\n",
    "#     # print(f\"Protein {name} embedded and saved to {out_path}\")\n",
    "\n",
    "# for name, seq in tqdm(meta_binders.items(), total=len(meta_binders.items()), desc=\"Embedding Meta binders\"):\n",
    "#     emb = calculate_ESM_pr_res_embeddings(seq)\n",
    "#     emb_np = to_numpy(emb)\n",
    "#     out_path = os.path.join(\"/work3/s232958/data/meta_analysis/binders_embeddings_esmC\", f\"{name}.npy\")\n",
    "#     np.save(out_path, emb_np)\n",
    "#     # print(f\"Protein {name} embedded and saved to {out_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88dfa6cd-6dda-4e32-8c34-9d2dbbaa55cb",
   "metadata": {},
   "source": [
    "#### Loading MetaData"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "b1f0bee0-e259-44f9-ba13-aff0ec3c7686",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "#Loading ESM2 embeddings: 100%|████████████████████████████████████████████████████████████████████████████████| 3532/3532 [00:17<00:00, 197.37it/s]\n"
     ]
    }
   ],
   "source": [
    "class CLIP_PPint_MetaData(Dataset):\n",
    "    def __init__(\n",
    "        self,\n",
    "        dframe,\n",
    "        paths,\n",
    "        embedding_dim=1152,\n",
    "        embedding_pad_value=-5000.0\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.dframe = dframe.copy()\n",
    "        self.embedding_dim = int(embedding_dim)\n",
    "        self.emb_pad = float(embedding_pad_value)\n",
    "        self.max_blen = self.dframe[\"seq_binder_len\"].max()\n",
    "        self.max_tlen = self.dframe[\"seq_target_len\"].max()\n",
    "\n",
    "        # paths\n",
    "        self.encoding_bpath, self.encoding_tpath = paths\n",
    "\n",
    "        # index & storage\n",
    "        self.dframe.set_index(\"binder_id\", inplace=True)\n",
    "        self.accessions = self.dframe.index.astype(str).tolist()\n",
    "        self.name_to_row = {name: i for i, name in enumerate(self.accessions)}\n",
    "        self.samples = []\n",
    "\n",
    "        for accession in tqdm(self.accessions, total=len(self.accessions), desc=\"#Loading ESM2 embeddings\"):\n",
    "            lbl = torch.tensor(int(self.dframe.loc[accession, \"binder_label\"]))\n",
    "            parts = accession.split(\"_\") # e.g. accession 7S8T_5_F_7S8T_5_G\n",
    "            tgt_id = \"_\".join(parts[:-1])\n",
    "            bnd_id = accession\n",
    "\n",
    "            ### --- embeddings (pad to fixed lengths) --- ###\n",
    "            \n",
    "            # laod embeddings\n",
    "            t_emb = np.load(os.path.join(self.encoding_tpath, f\"{tgt_id}.npy\"))[0]     # [Lt, D]\n",
    "            b_emb = np.load(os.path.join(self.encoding_bpath, f\"{bnd_id}.npy\"))[0]     # [Lb, D]\n",
    "\n",
    "            # quich check whether embedding dimmension is as it suppose to be\n",
    "            if t_emb.shape[1] != self.embedding_dim or b_emb.shape[1] != self.embedding_dim:\n",
    "                raise ValueError(\"Embedding dim mismatch with 'embedding_dim'.\")\n",
    "\n",
    "            # add -5000 to all the padded target rows\n",
    "            if t_emb.shape[0] < self.max_tlen:\n",
    "                t_emb = np.concatenate([t_emb, np.full((self.max_tlen - t_emb.shape[0], t_emb.shape[1]), self.emb_pad, dtype=t_emb.dtype)], axis=0)\n",
    "            else:\n",
    "                t_emb = t_emb[: self.max_tlen] # no padding was used\n",
    "\n",
    "            # add -5000 to all the padded binder rows\n",
    "            if b_emb.shape[0] < self.max_blen:\n",
    "                b_emb = np.concatenate([b_emb, np.full((self.max_blen - b_emb.shape[0], b_emb.shape[1]), self.emb_pad, dtype=b_emb.dtype)], axis=0)\n",
    "            else:\n",
    "                b_emb = b_emb[: self.max_blen] # no padding was used\n",
    "\n",
    "            self.samples.append((b_emb, t_emb, lbl))\n",
    "\n",
    "    # ---- Dataset API ----\n",
    "    def __len__(self):\n",
    "        return len(self.samples)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        b_arr, t_arr, lbls = self.samples[idx]\n",
    "        binder_emb, target_emb = torch.from_numpy(b_arr).float(), torch.from_numpy(t_arr).float()\n",
    "        return binder_emb, target_emb, lbls\n",
    "\n",
    "    def _get_by_name(self, name):\n",
    "        # Single item -> return exactly what __getitem__ returns\n",
    "        if isinstance(name, str):\n",
    "            return self.__getitem__(self.name_to_row[name])\n",
    "        \n",
    "        # Multiple items -> fetch all\n",
    "        out = [self.__getitem__(self.name_to_row[n]) for n in list(name)]\n",
    "        b_list, t_list, lbl_list = zip(*out)\n",
    "    \n",
    "        # Stack embeddings\n",
    "        b  = torch.stack([torch.as_tensor(x) for x in b_list],  dim=0)  # [B, ...]\n",
    "        t  = torch.stack([torch.as_tensor(x) for x in t_list],  dim=0)  # [B, ...]\n",
    "    \n",
    "        # Stack labels\n",
    "        labels = torch.stack(lbl_list)  # [B]\n",
    "    \n",
    "        return b, t, labels\n",
    "\n",
    "bemb_path = \"/work3/s232958/data/meta_analysis/binders_embeddings_esmC\"\n",
    "temb_path = \"/work3/s232958/data/meta_analysis/targets_embeddings_esmC\"\n",
    "\n",
    "validation_Dataset = CLIP_PPint_MetaData(\n",
    "    # interaction_df_shuffled[:len(Df_test)],\n",
    "    interaction_df_shuffled,\n",
    "    paths=[bemb_path, temb_path],\n",
    "    embedding_dim=1152\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "509c9527-c9e9-4980-b273-70d66ae5464a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1, 0, 0, 0, 0])"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accessions_Meta = list(interaction_df_shuffled.binder_id)\n",
    "emb_b, emb_t, labels = validation_Dataset._get_by_name(accessions_Meta[:5])\n",
    "labels"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fadb2503-da3c-413a-b655-9d2d8e4ec911",
   "metadata": {},
   "source": [
    "### Train model from scratch with 10% of PPint dataset using old architecture (encodings only)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "0365067c-759c-4cdd-bfe1-5f35b2033fee",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_key_padding_mask(embeddings, padding_value=-5000, offset=10):\n",
    "    return (embeddings < (padding_value + offset)).all(dim=-1)\n",
    "\n",
    "def create_mean_of_non_masked(embeddings, padding_mask):\n",
    "    # Use masked select and mean to compute the mean of non-masked elements\n",
    "    # embeddings should be of shape (batch_size, seq_len, features)\n",
    "    seq_embeddings = []\n",
    "    for i in range(embeddings.shape[0]): # looping over all batch elements\n",
    "        non_masked_embeddings = embeddings[i][~padding_mask[i]] # shape [num_real_tokens, features]\n",
    "        if len(non_masked_embeddings) == 0:\n",
    "            print(\"You are masking all positions when creating sequence representation\")\n",
    "            sys.exit(1)\n",
    "        mean_embedding = non_masked_embeddings.mean(dim=0) # sequence is represented by the single vecotr [1152] [features]\n",
    "        seq_embeddings.append(mean_embedding)\n",
    "    return torch.stack(seq_embeddings)\n",
    "\n",
    "class MiniCLIP_w_transformer_crossattn(pl.LightningModule):\n",
    "\n",
    "    def __init__(self, padding_value = -5000, embed_dimension=embedding_dimension, num_recycles=2):\n",
    "\n",
    "        super().__init__()\n",
    "        self.num_recycles = num_recycles # how many times you iteratively refine embeddings with self- and cross-attention (ALPHA-Fold-style recycling).\n",
    "        self.padding_value = padding_value\n",
    "        self.embed_dimension = embed_dimension\n",
    "\n",
    "        self.logit_scale = nn.Parameter(torch.tensor(math.log(1/0.07)))  # ~CLIP init\n",
    "\n",
    "        self.transformerencoder =  nn.TransformerEncoderLayer(\n",
    "            d_model=self.embed_dimension,\n",
    "            nhead=8,\n",
    "            dropout=0.1,\n",
    "            batch_first=True,\n",
    "            dim_feedforward=self.embed_dimension\n",
    "            )\n",
    " \n",
    "        self.norm = nn.LayerNorm(self.embed_dimension)  # For residual additions\n",
    "\n",
    "        self.cross_attn = nn.MultiheadAttention(\n",
    "            embed_dim=self.embed_dimension,\n",
    "            num_heads=8,\n",
    "            dropout=0.1,\n",
    "            batch_first=True\n",
    "        )\n",
    "\n",
    "        self.prot_embedder = nn.Sequential(\n",
    "            nn.Linear(self.embed_dimension, 640),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(640, 320),\n",
    "        )\n",
    "        \n",
    "    def forward(self, pep_input, prot_input, label=None, pep_int_mask=None, prot_int_mask=None, int_prob=None, mem_save=True): # , pep_tokens, prot_tokens\n",
    "\n",
    "        pep_mask = create_key_padding_mask(embeddings=pep_input, padding_value=self.padding_value)\n",
    "        prot_mask = create_key_padding_mask(embeddings=prot_input, padding_value=self.padding_value)\n",
    " \n",
    "        # Initialize residual states\n",
    "        pep_emb = pep_input.clone()\n",
    "        prot_emb = prot_input.clone()\n",
    " \n",
    "        for _ in range(self.num_recycles):\n",
    "\n",
    "            # Transformer encoding with residual\n",
    "            pep_trans = self.transformerencoder(self.norm(pep_emb), src_key_padding_mask=pep_mask)\n",
    "            prot_trans = self.transformerencoder(self.norm(prot_emb), src_key_padding_mask=prot_mask)\n",
    "\n",
    "            # Cross-attention with residual\n",
    "            pep_cross, _ = self.cross_attn(query=self.norm(pep_trans), key=self.norm(prot_trans), value=self.norm(prot_trans), key_padding_mask=prot_mask)\n",
    "            prot_cross, _ = self.cross_attn(query=self.norm(prot_trans), key=self.norm(pep_trans), value=self.norm(pep_trans), key_padding_mask=pep_mask)\n",
    "            \n",
    "            # Additive update with residual connection\n",
    "            pep_emb = pep_emb + pep_trans  \n",
    "            prot_emb = prot_emb + prot_trans\n",
    "\n",
    "        pep_seq_coding = create_mean_of_non_masked(pep_emb, pep_mask)\n",
    "        prot_seq_coding = create_mean_of_non_masked(prot_emb, prot_mask)\n",
    "        \n",
    "        # Use self-attention outputs for embeddings\n",
    "        pep_seq_coding = F.normalize(self.prot_embedder(pep_seq_coding), dim=-1)\n",
    "        prot_seq_coding = F.normalize(self.prot_embedder(prot_seq_coding), dim=-1)\n",
    " \n",
    "        if mem_save:\n",
    "            torch.cuda.empty_cache()\n",
    "        \n",
    "        scale = torch.exp(self.logit_scale).clamp(max=100.0)\n",
    "        logits = scale * (pep_seq_coding * prot_seq_coding).sum(dim=-1)\n",
    "        \n",
    "        return logits\n",
    "\n",
    "    def training_step(self, batch, device):\n",
    "        embedding_pep, embedding_prot, labels = batch\n",
    "        embedding_pep, embedding_prot = embedding_pep.to(device), embedding_prot.to(device)\n",
    "        \n",
    "        positive_logits = self.forward(embedding_pep, embedding_prot)\n",
    "        \n",
    "        # Negative indexes\n",
    "        rows, cols = torch.triu_indices(embedding_prot.size(0), embedding_prot.size(0), offset=1)         \n",
    "        \n",
    "        negative_logits = self(embedding_pep[rows,:,:], \n",
    "                          embedding_prot[cols,:,:], \n",
    "                          int_prob=0.0)\n",
    "\n",
    "        # loss of predicting partner using peptide\n",
    "        positive_loss = F.binary_cross_entropy_with_logits(positive_logits, torch.ones_like(positive_logits).to(device))\n",
    " \n",
    "        # loss of predicting peptide using partner\n",
    "        negative_loss =  F.binary_cross_entropy_with_logits(negative_logits, torch.zeros_like(negative_logits).to(device))\n",
    "        \n",
    "        loss = (positive_loss + negative_loss) / 2\n",
    " \n",
    "        # del partner_prediction_loss, peptide_prediction_loss, embedding_pep, embedding_prot\n",
    "        torch.cuda.empty_cache()\n",
    "        return loss\n",
    "\n",
    "    def validation_step_PPint(self, batch, device):\n",
    "        # Predict on random batches of training batch size\n",
    "        embedding_pep, embedding_prot, labels = batch\n",
    "        embedding_pep, embedding_prot = embedding_pep.to(device), embedding_prot.to(device)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "\n",
    "            positive_logits = self(embedding_pep, embedding_prot)\n",
    "            \n",
    "            # loss of predicting partner using peptide\n",
    "            positive_loss = F.binary_cross_entropy_with_logits(positive_logits, torch.ones_like(positive_logits).to(device))\n",
    "            \n",
    "            # Negaive indexes\n",
    "            rows, cols = torch.triu_indices(embedding_prot.size(0), embedding_prot.size(0), offset=1)\n",
    "            \n",
    "            negative_logits = self(embedding_pep[rows,:,:], embedding_prot[cols,:,:], int_prob=0.0)\n",
    "    \n",
    "            negative_loss =  F.binary_cross_entropy_with_logits(negative_logits, torch.zeros_like(negative_logits).to(device))\n",
    "\n",
    "            loss = (positive_loss + negative_loss) / 2\n",
    "           \n",
    "            logit_matrix = torch.zeros((embedding_pep.size(0),embedding_pep.size(0)),device=self.device)\n",
    "            logit_matrix[rows, cols] = negative_logits\n",
    "            logit_matrix[cols, rows] = negative_logits\n",
    "            \n",
    "            # Fill diagonal with positive scores\n",
    "            diag_indices = torch.arange(embedding_pep.size(0), device=self.device)\n",
    "            logit_matrix[diag_indices, diag_indices] = positive_logits.squeeze()\n",
    "\n",
    "            labels = torch.arange(embedding_prot.size(0)).to(self.device)\n",
    "            peptide_predictions = logit_matrix.argmax(dim=0)\n",
    "            peptide_ranks = logit_matrix.argsort(dim=0).diag() + 1\n",
    "            peptide_mrr = (peptide_ranks).float().pow(-1).mean()\n",
    "            \n",
    "            # partner_accuracy = partner_predictions.eq(labels).float().mean()\n",
    "            peptide_accuracy = peptide_predictions.eq(labels).float().mean()\n",
    "    \n",
    "            k = 3\n",
    "            peptide_topk_accuracy = torch.any((logit_matrix.topk(k, dim=0).indices - labels.reshape(1, -1)) == 0, dim=0).sum() / logit_matrix.shape[0]\n",
    "    \n",
    "            del logit_matrix,positive_logits,negative_logits,embedding_pep,embedding_prot\n",
    "\n",
    "            return loss, peptide_accuracy, peptide_topk_accuracy\n",
    "    \n",
    "    def validation_step_MetaDataset(self, batch, device):\n",
    "        embedding_binder, embedding_target, labels = batch\n",
    "        embedding_binder = embedding_binder.to(device)\n",
    "        embedding_target = embedding_target.to(device)\n",
    "        labels = labels.to(device).float()\n",
    "    \n",
    "        with torch.no_grad():\n",
    "            logits = self.forward(embedding_binder, embedding_target)\n",
    "            logits = logits.float()\n",
    "            loss = F.binary_cross_entropy_with_logits(logits.view(-1), labels.view(-1))\n",
    "            return logits, loss\n",
    "\n",
    "    def calculate_logit_matrix(self,embedding_pep,embedding_prot):\n",
    "        rows, cols = torch.triu_indices(embedding_pep.size(0), embedding_pep.size(0), offset=1)\n",
    "        \n",
    "        positive_logits = self(embedding_pep, embedding_prot)\n",
    "        negative_logits = self(embedding_pep[rows,:,:], embedding_prot[cols,:,:], int_prob=0.0)\n",
    "        \n",
    "        logit_matrix = torch.zeros((embedding_pep.size(0),embedding_pep.size(0)),device=self.device)\n",
    "        logit_matrix[rows, cols] = negative_logits\n",
    "        logit_matrix[cols, rows] = negative_logits\n",
    "        \n",
    "        diag_indices = torch.arange(embedding_pep.size(0), device=self.device)\n",
    "        logit_matrix[diag_indices, diag_indices] = positive_logits.squeeze()\n",
    "        \n",
    "        return logit_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "826b9f34-ea1a-4334-8c94-0b13927bac5b",
   "metadata": {},
   "source": [
    "### Loading pre-trained model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4aec17f-ea91-4377-8bfc-75d6314eaddb",
   "metadata": {},
   "source": [
    "### ESM-C"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "24222398-c484-4e4b-9256-987a529c19ac",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MiniCLIP_w_transformer_crossattn(\n",
       "  (transformerencoder): TransformerEncoderLayer(\n",
       "    (self_attn): MultiheadAttention(\n",
       "      (out_proj): NonDynamicallyQuantizableLinear(in_features=1152, out_features=1152, bias=True)\n",
       "    )\n",
       "    (linear1): Linear(in_features=1152, out_features=1152, bias=True)\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "    (linear2): Linear(in_features=1152, out_features=1152, bias=True)\n",
       "    (norm1): LayerNorm((1152,), eps=1e-05, elementwise_affine=True)\n",
       "    (norm2): LayerNorm((1152,), eps=1e-05, elementwise_affine=True)\n",
       "    (dropout1): Dropout(p=0.1, inplace=False)\n",
       "    (dropout2): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (norm): LayerNorm((1152,), eps=1e-05, elementwise_affine=True)\n",
       "  (cross_attn): MultiheadAttention(\n",
       "    (out_proj): NonDynamicallyQuantizableLinear(in_features=1152, out_features=1152, bias=True)\n",
       "  )\n",
       "  (prot_embedder): Sequential(\n",
       "    (0): Linear(in_features=1152, out_features=640, bias=True)\n",
       "    (1): ReLU()\n",
       "    (2): Linear(in_features=640, out_features=320, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = MiniCLIP_w_transformer_crossattn(embed_dimension=embedding_dimension, num_recycles=number_of_recycles).to(\"cuda\")\n",
    "path = \"/work3/s232958/data/trained/PPint_retrain10%_0.4_Christian/251116_0e035ba9-3cbd-4aab-be0b-cc2e37723b27/0e035ba9-3cbd-4aab-be0b-cc2e37723b27_checkpoint_8/0e035ba9-3cbd-4aab-be0b-cc2e37723b27_checkpoint_epoch_8.pth\"\n",
    "checkpoint = torch.load(path, weights_only=False, map_location=torch.device('cpu'))\n",
    "# print(list(checkpoint[\"model_state_dict\"]))\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "model.load_state_dict(checkpoint['model_state_dict'])\n",
    "model.to(device)\n",
    "model.eval()  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "9ed1454e-d184-48ec-b66f-a27e9c03daad",
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_varlen(batch):\n",
    "    b_emb = torch.stack([x[0] for x in batch], dim=0)\n",
    "    t_emb = torch.stack([x[1] for x in batch], dim=0)\n",
    "    lbls = torch.tensor([x[2].float() for x in batch])\n",
    "    return b_emb, t_emb, lbls\n",
    "\n",
    "test_dataloader = DataLoader(testing_Dataset, batch_size=10, collate_fn=collate_varlen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "ec237c15-9668-48ff-8f95-3c474601b9ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "#Iterating through batched data: 50it [00:18,  2.77it/s]                                                                                            "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Positives: (494,)\n",
      "Negatives: (2211,)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "interaction_scores_pos = []\n",
    "interaction_scores_neg = []    \n",
    "\n",
    "for batch in tqdm(test_dataloader, total=round(len(Df_test)/10), desc=\"#Iterating through batched data\"):\n",
    "    b_emb, t_emb, lbls = batch\n",
    "    embedding_pep = b_emb.to(\"cuda\")\n",
    "    embedding_prot = t_emb.to(\"cuda\")\n",
    "\n",
    "    with torch.no_grad():\n",
    "\n",
    "        rows, cols = torch.triu_indices(embedding_pep.size(0), embedding_pep.size(0), offset=1)\n",
    "        positive_logits = model(embedding_pep, embedding_prot)\n",
    "        negative_logits = model(embedding_pep[rows,:,:], embedding_prot[cols,:,:], int_prob=0.0)\n",
    "        \n",
    "        logit_matrix = torch.zeros((embedding_pep.size(0),embedding_pep.size(0)),device=\"cuda\")\n",
    "        logit_matrix[rows, cols] = negative_logits\n",
    "        logit_matrix[cols, rows] = negative_logits\n",
    "        \n",
    "        diag_indices = torch.arange(embedding_pep.size(0), device=\"cuda\")\n",
    "        logit_matrix[diag_indices, diag_indices] = positive_logits.squeeze()\n",
    "        \n",
    "        # print(logit_matrix)\n",
    "        interaction_scores_pos.append(positive_logits)\n",
    "        interaction_scores_neg.append(negative_logits)\n",
    "\n",
    "# Convert list of tensors to single 1D tensors\n",
    "pos_logits = torch.cat(interaction_scores_pos).detach().cpu().numpy()\n",
    "neg_logits = torch.cat(interaction_scores_neg).detach().cpu().numpy()\n",
    "print(\"Positives:\", pos_logits.shape)\n",
    "print(\"Negatives:\", neg_logits.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "7b8210b6-7e6a-4f9b-8f63-5db6f32959f3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 600x400 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure(figsize=(6, 4))\n",
    "plt.hist(pos_logits, bins=50, alpha=0.6, label=\"Positive pairs\", density=True)\n",
    "plt.hist(neg_logits, bins=50, alpha=0.6, label=\"Negative pairs\", density=True)\n",
    "\n",
    "plt.xlabel(\"Logit score\")\n",
    "plt.ylabel(\"Density\")\n",
    "plt.title(\"Distribution of Positive vs Negative Interaction Scores\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06320982-3625-4b6e-9102-c9e822781a75",
   "metadata": {},
   "source": [
    "#### non-dimers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "35ded83c-5023-45e7-af80-1bd8b079e620",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "#Loading ESMC embeddings: 100%|██████████████████████████████████████████████████████████████████████████████████| 130/130 [00:01<00:00, 118.92it/s]\n"
     ]
    }
   ],
   "source": [
    "Df_test_non_dimer = Df_test[Df_test.dimer == False]\n",
    "\n",
    "non_dimers_Dataset = CLIP_PPint_analysis_dataset(\n",
    "    Df_test_non_dimer,\n",
    "    paths=[bemb_path, temb_path],\n",
    "    embedding_dim=1152\n",
    ")\n",
    "\n",
    "non_dimers_dataloader = DataLoader(testing_Dataset, batch_size=10, collate_fn=collate_varlen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "9e81400c-1e6d-4178-a22c-ee4e1129f9bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "#Iterating through batched data: 50it [00:04, 10.22it/s]                                                                                            \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([14.19217 , 14.252893,  8.744677, 14.229164, 14.252892, 14.241057,\n",
       "       10.360923, 14.211796, 14.252892, 11.652437, 14.19217 , 14.252893,\n",
       "        8.744677, 14.229164, 14.252892, 14.241057, 10.360923, 14.211796,\n",
       "       14.252892, 11.652437, 14.19217 , 14.252893,  8.744677, 14.229164,\n",
       "       14.252892, 14.241057, 10.360923, 14.211796, 14.252892, 11.652437,\n",
       "       14.19217 , 14.252893,  8.744677, 14.229164, 14.252892, 14.241057,\n",
       "       10.360923, 14.211796, 14.252892, 11.652437, 14.19217 , 14.252893,\n",
       "        8.744677, 14.229164, 14.252892, 14.241057, 10.360923, 14.211796,\n",
       "       14.252892, 11.652437, 14.19217 , 14.252893,  8.744677, 14.229164,\n",
       "       14.252892, 14.241057, 10.360923, 14.211796, 14.252892, 11.652437,\n",
       "       14.19217 , 14.252893,  8.744677, 14.229164, 14.252892, 14.241057,\n",
       "       10.360923, 14.211796, 14.252892, 11.652437, 14.19217 , 14.252893,\n",
       "        8.744677, 14.229164, 14.252892, 14.241057, 10.360923, 14.211796,\n",
       "       14.252892, 11.652437, 14.19217 , 14.252893,  8.744677, 14.229164,\n",
       "       14.252892, 14.241057, 10.360923, 14.211796, 14.252892, 11.652437,\n",
       "       14.19217 , 14.252893,  8.744677, 14.229164, 14.252892, 14.241057,\n",
       "       10.360923, 14.211796, 14.252892, 11.652437, 14.19217 , 14.252893,\n",
       "        8.744677, 14.229164, 14.252892, 14.241057, 10.360923, 14.211796,\n",
       "       14.252892, 11.652437, 14.19217 , 14.252893,  8.744677, 14.229164,\n",
       "       14.252892, 14.241057, 10.360923, 14.211796, 14.252892, 11.652437,\n",
       "       14.19217 , 14.252893,  8.744677, 14.229164, 14.252892, 14.241057,\n",
       "       10.360923, 14.211796, 14.252892, 11.652437, 14.19217 , 14.252893,\n",
       "        8.744677, 14.229164, 14.252892, 14.241057, 10.360923, 14.211796,\n",
       "       14.252892, 11.652437, 14.19217 , 14.252893,  8.744677, 14.229164,\n",
       "       14.252892, 14.241057, 10.360923, 14.211796, 14.252892, 11.652437,\n",
       "       14.19217 , 14.252893,  8.744677, 14.229164, 14.252892, 14.241057,\n",
       "       10.360923, 14.211796, 14.252892, 11.652437, 14.19217 , 14.252893,\n",
       "        8.744677, 14.229164, 14.252892, 14.241057, 10.360923, 14.211796,\n",
       "       14.252892, 11.652437, 14.19217 , 14.252893,  8.744677, 14.229164,\n",
       "       14.252892, 14.241057, 10.360923, 14.211796, 14.252892, 11.652437,\n",
       "       14.19217 , 14.252893,  8.744677, 14.229164, 14.252892, 14.241057,\n",
       "       10.360923, 14.211796, 14.252892, 11.652437, 14.19217 , 14.252893,\n",
       "        8.744677, 14.229164, 14.252892, 14.241057, 10.360923, 14.211796,\n",
       "       14.252892, 11.652437, 14.19217 , 14.252893,  8.744677, 14.229164,\n",
       "       14.252892, 14.241057, 10.360923, 14.211796, 14.252892, 11.652437,\n",
       "       14.19217 , 14.252893,  8.744677, 14.229164, 14.252892, 14.241057,\n",
       "       10.360923, 14.211796, 14.252892, 11.652437, 14.19217 , 14.252893,\n",
       "        8.744677, 14.229164, 14.252892, 14.241057, 10.360923, 14.211796,\n",
       "       14.252892, 11.652437, 14.19217 , 14.252893,  8.744677, 14.229164,\n",
       "       14.252892, 14.241057, 10.360923, 14.211796, 14.252892, 11.652437,\n",
       "       14.19217 , 14.252893,  8.744677, 14.229164, 14.252892, 14.241057,\n",
       "       10.360923, 14.211796, 14.252892, 11.652437, 14.19217 , 14.252893,\n",
       "        8.744677, 14.229164, 14.252892, 14.241057, 10.360923, 14.211796,\n",
       "       14.252892, 11.652437, 14.19217 , 14.252893,  8.744677, 14.229164,\n",
       "       14.252892, 14.241057, 10.360923, 14.211796, 14.252892, 11.652437,\n",
       "       14.19217 , 14.252893,  8.744677, 14.229164, 14.252892, 14.241057,\n",
       "       10.360923, 14.211796, 14.252892, 11.652437, 14.19217 , 14.252893,\n",
       "        8.744677, 14.229164, 14.252892, 14.241057, 10.360923, 14.211796,\n",
       "       14.252892, 11.652437, 14.19217 , 14.252893,  8.744677, 14.229164,\n",
       "       14.252892, 14.241057, 10.360923, 14.211796, 14.252892, 11.652437,\n",
       "       14.19217 , 14.252893,  8.744677, 14.229164, 14.252892, 14.241057,\n",
       "       10.360923, 14.211796, 14.252892, 11.652437, 14.19217 , 14.252893,\n",
       "        8.744677, 14.229164, 14.252892, 14.241057, 10.360923, 14.211796,\n",
       "       14.252892, 11.652437, 14.19217 , 14.252893,  8.744677, 14.229164,\n",
       "       14.252892, 14.241057, 10.360923, 14.211796, 14.252892, 11.652437,\n",
       "       14.19217 , 14.252893,  8.744677, 14.229164, 14.252892, 14.241057,\n",
       "       10.360923, 14.211796, 14.252892, 11.652437, 14.19217 , 14.252893,\n",
       "        8.744677, 14.229164, 14.252892, 14.241057, 10.360923, 14.211796,\n",
       "       14.252892, 11.652437, 14.19217 , 14.252893,  8.744677, 14.229164,\n",
       "       14.252892, 14.241057, 10.360923, 14.211796, 14.252892, 11.652437,\n",
       "       14.19217 , 14.252893,  8.744677, 14.229164, 14.252892, 14.241057,\n",
       "       10.360923, 14.211796, 14.252892, 11.652437, 14.19217 , 14.252893,\n",
       "        8.744677, 14.229164, 14.252892, 14.241057, 10.360923, 14.211796,\n",
       "       14.252892, 11.652437, 14.19217 , 14.252893,  8.744677, 14.229164,\n",
       "       14.252892, 14.241057, 10.360923, 14.211796, 14.252892, 11.652437,\n",
       "       14.19217 , 14.252893,  8.744677, 14.229164, 14.252892, 14.241057,\n",
       "       10.360923, 14.211796, 14.252892, 11.652437, 14.19217 , 14.252893,\n",
       "        8.744677, 14.229164, 14.252892, 14.241057, 10.360923, 14.211796,\n",
       "       14.252892, 11.652437, 14.19217 , 14.252893,  8.744677, 14.229164,\n",
       "       14.252892, 14.241057, 10.360923, 14.211796, 14.252892, 11.652437,\n",
       "       14.19217 , 14.252893,  8.744677, 14.229164, 14.252892, 14.241057,\n",
       "       10.360923, 14.211796, 14.252892, 11.652437, 14.19217 , 14.252893,\n",
       "        8.744677, 14.229164, 14.252892, 14.241057, 10.360923, 14.211796,\n",
       "       14.252892, 11.652437, 14.19217 , 14.252893,  8.744677, 14.229164,\n",
       "       14.252892, 14.241057, 10.360923, 14.211796, 14.252892, 11.652437,\n",
       "       14.19217 , 14.252893,  8.744677, 14.229164, 14.252892, 14.241057,\n",
       "       10.360923, 14.211796, 14.252892, 11.652437, 14.19217 , 14.252893,\n",
       "        8.744677, 14.229164, 14.252892, 14.241057, 10.360923, 14.211796,\n",
       "       14.252892, 11.652437, 14.19217 , 14.252893,  8.744677, 14.229164,\n",
       "       14.252892, 14.241057, 10.360923, 14.211796, 14.252892, 11.652437,\n",
       "       14.19217 , 14.252893,  8.744677, 14.229164, 14.252892, 14.241057,\n",
       "       10.360923, 14.211796, 14.252892, 11.652437, 14.19217 , 14.252893,\n",
       "        8.744677, 14.229164, 14.252892, 14.241057, 10.360923, 14.211796,\n",
       "       14.252892, 11.652437], dtype=float32)"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "interaction_scores = []\n",
    "for batch in tqdm(non_dimers_dataloader, total=round(len(Df_test_non_dimer)/10), desc=\"#Iterating through batched data\"):\n",
    "    b_emb, t_emb, lbls = batch\n",
    "    embedding_pep = b_emb.to(\"cuda\")\n",
    "    embedding_prot = t_emb.to(\"cuda\")\n",
    "\n",
    "    with torch.no_grad():\n",
    "        positive_logits = model(embedding_pep, embedding_prot)\n",
    "        interaction_scores.append(logits.unsqueeze(0))\n",
    "\n",
    "# Convert list of tensors to single 1D tensors\n",
    "predicted_interaction_scores = np.concatenate([batch_score.cpu().detach().numpy().reshape(-1,) for batch_score in interaction_scores])\n",
    "# interaction_probabilities = np.concatenate([torch.sigmoid(batch_score[0]).cpu().numpy() for batch_score in interaction_scores])\n",
    "\n",
    "predicted_interaction_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2ffcc8e-c045-46b6-a3a8-72b465e024af",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
