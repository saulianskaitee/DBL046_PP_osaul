{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3e5a860d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Kernel Python: /zhome/c9/0/203261/miniconda3/envs/esm_gpu/bin/python\n",
      "PyTorch: 2.5.1\n",
      "Using device: cpu\n",
      "Current location: /zhome/c9/0/203261/DBL046_PP_osaul/DBL046_PP_osaul/tmp/ona_drafts\n"
     ]
    }
   ],
   "source": [
    "import sys, os\n",
    "print(\"Kernel Python:\", sys.executable)\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "\n",
    "import torch\n",
    "print(\"PyTorch:\", torch.__version__)\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader, Subset\n",
    "import pytorch_lightning as pl\n",
    "\n",
    "from tqdm import tqdm\n",
    "# setting device on GPU if available, else CPU\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print('Using device:', device)\n",
    "print(\"Current location:\", os.getcwd())\n",
    "\n",
    "# from esm.models.esmc import ESMC\n",
    "# from esm.sdk.api import ESMProtein, LogitsConfig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "23fdabd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gaussian_kernel(x, sigma):\n",
    "    return np.exp(-x**2 / (2 * sigma**2))\n",
    "\n",
    "def transform_vector(vector, sigma):\n",
    "\n",
    "    interacting_indices = np.where(vector == 1)[0]   # positions where vector == 1\n",
    "    transformed_vector = np.zeros_like(vector, dtype=float)\n",
    "    \n",
    "    for i in range(len(vector)):\n",
    "        if vector[i] == 0:\n",
    "            distances = np.abs(interacting_indices - i)   # distance to all \"1\"s\n",
    "            min_distance = np.min(distances)              # closest \"1\"\n",
    "            transformed_vector[i] = gaussian_kernel(min_distance, sigma)\n",
    "        else:\n",
    "            transformed_vector[i] = 1.0\n",
    "    return transformed_vector\n",
    "\n",
    "def safe_shuffle(n, device):\n",
    "    shuffled = torch.randperm(n, device=device)\n",
    "    while torch.any(shuffled == torch.arange(n, device=device)):\n",
    "        shuffled = torch.randperm(n, device=device)\n",
    "    return shuffled\n",
    "\n",
    "def create_key_padding_mask(embeddings, padding_value=0, offset=10):\n",
    "    return (embeddings < (padding_value + offset)).all(dim=-1)\n",
    "\n",
    "def create_mean_of_non_masked(embeddings, padding_mask):\n",
    "    # Use masked select and mean to compute the mean of non-masked elements\n",
    "    # embeddings should be of shape (batch_size, seq_len, features)\n",
    "    seq_embeddings = []\n",
    "    for i in range(embeddings.shape[0]): # looping over all batch elements\n",
    "        non_masked_embeddings = embeddings[i][~padding_mask[i]] # shape [num_real_tokens, features]\n",
    "        if len(non_masked_embeddings) == 0:\n",
    "            print(\"You are masking all positions when creating sequence representation\")\n",
    "            sys.exit(1)\n",
    "        mean_embedding = non_masked_embeddings.mean(dim=0) # sequence is represented by the single vecotr [1152] [features]\n",
    "        seq_embeddings.append(mean_embedding)\n",
    "    return torch.stack(seq_embeddings)\n",
    "\n",
    "class MiniCLIP_w_transformer_crossattn(pl.LightningModule):\n",
    "\n",
    "    def __init__(self, padding_value = -5000, embed_dimension=1152, num_recycles=1):\n",
    "\n",
    "        super().__init__()\n",
    "        self.num_recycles = num_recycles # how many times you iteratively refine embeddings with self- and cross-attention (ALPHA-Fold-style recycling).\n",
    "        self.padding_value = padding_value\n",
    "        self.embed_dimension = embed_dimension\n",
    "\n",
    "        self.logit_scale = nn.Parameter(torch.tensor(math.log(1/0.07)))  # ~CLIP init\n",
    "\n",
    "        self.transformerencoder =  nn.TransformerEncoderLayer(\n",
    "            d_model=self.embed_dimension,\n",
    "            nhead=8,\n",
    "            dropout=0.1,\n",
    "            batch_first=True,\n",
    "            dim_feedforward=self.embed_dimension\n",
    "            )\n",
    " \n",
    "        self.norm = nn.LayerNorm(self.embed_dimension)  # For residual additions\n",
    "\n",
    "        self.cross_attn = nn.MultiheadAttention(\n",
    "            embed_dim=self.embed_dimension,\n",
    "            num_heads=8,\n",
    "            dropout=0.1,\n",
    "            batch_first=True\n",
    "        )\n",
    "\n",
    "        self.prot_embedder = nn.Sequential(\n",
    "            nn.Linear(self.embed_dimension, 640),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(640, 320),\n",
    "        )\n",
    "        \n",
    "    def forward(self, pep_input, prot_input, pep_int_mask=None, prot_int_mask=None, int_prob=None, mem_save=True): # , pep_tokens, prot_tokens\n",
    "\n",
    "        pep_mask = create_key_padding_mask(embeddings=pep_input,padding_value=self.padding_value)\n",
    "        prot_mask = create_key_padding_mask(embeddings=prot_input,padding_value=self.padding_value)\n",
    " \n",
    "        # Initialize residual states\n",
    "        # pep_emb = pep_input.clone()\n",
    "        # prot_emb = prot_input.clone()\n",
    "\n",
    "        pep_emb  = pep_input\n",
    "        prot_emb = prot_input\n",
    " \n",
    "        for _ in range(self.num_recycles):\n",
    "\n",
    "            # Transformer encoding with residual\n",
    "            pep_trans = self.transformerencoder(self.norm(pep_emb), src_key_padding_mask=pep_mask)\n",
    "            prot_trans = self.transformerencoder(self.norm(prot_emb), src_key_padding_mask=prot_mask)\n",
    "\n",
    "            # Cross-attention with residual\n",
    "            pep_cross, _ = self.cross_attn(query=self.norm(pep_trans), key=self.norm(prot_trans), value=self.norm(prot_trans), key_padding_mask=prot_mask)\n",
    "            prot_cross, _ = self.cross_attn(query=self.norm(prot_trans), key=self.norm(pep_trans), value=self.norm(pep_trans), key_padding_mask=pep_mask)\n",
    "            \n",
    "            # Additive update with residual connection\n",
    "            pep_emb = pep_emb + pep_cross  \n",
    "            prot_emb = prot_emb + prot_cross\n",
    "\n",
    "        pep_seq_coding = create_mean_of_non_masked(pep_emb, pep_mask)\n",
    "        prot_seq_coding = create_mean_of_non_masked(prot_emb, prot_mask)\n",
    "        \n",
    "        # Use self-attention outputs for embeddings\n",
    "        pep_seq_coding = F.normalize(self.prot_embedder(pep_seq_coding))\n",
    "        prot_seq_coding = F.normalize(self.prot_embedder(prot_seq_coding))\n",
    " \n",
    "        if mem_save:\n",
    "            torch.cuda.empty_cache()\n",
    "        \n",
    "        scale = torch.exp(self.logit_scale).clamp(max=100.0)\n",
    "        logits = scale * (pep_seq_coding * prot_seq_coding).sum(dim=-1) # Dot-Product for comparison\n",
    "        \n",
    "        return logits\n",
    "    \n",
    "\n",
    "    def training_step(self, batch, device):\n",
    "        # Passing the sequences to the models\n",
    " \n",
    "        embedding_pep = batch[0]\n",
    "        embedding_prot = batch[1]\n",
    " \n",
    "        embedding_pep = embedding_pep.to(device)\n",
    "        embedding_prot = embedding_prot.to(device)\n",
    "\n",
    "        positive_logits = self(embedding_pep, embedding_prot)\n",
    "        \n",
    "        rows, cols = torch.triu_indices(embedding_prot.size(0), embedding_prot.size(0), offset=1)         \n",
    "        \n",
    "        negative_logits = self(embedding_pep[rows,:,:], embedding_prot[cols,:,:], int_prob=0.0)\n",
    "\n",
    "        positive_loss = F.binary_cross_entropy_with_logits(positive_logits, torch.ones_like(positive_logits).to(device))\n",
    " \n",
    "        # loss of predicting peptide using partner\n",
    "        negative_loss =  F.binary_cross_entropy_with_logits(negative_logits, torch.zeros_like(negative_logits).to(device))\n",
    "        \n",
    "        loss = (positive_loss + negative_loss) / 2\n",
    " \n",
    "        torch.cuda.empty_cache()\n",
    "        return loss\n",
    "    \n",
    "    def validation_step(self, batch, device):\n",
    "        # Predict on random batches of training batch size\n",
    "        embedding_pep = batch[0]\n",
    "        embedding_prot = batch[1]\n",
    "        embedding_pep = embedding_pep.to(device)\n",
    "        embedding_prot = embedding_prot.to(device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "\n",
    "            positive_logits = self(\n",
    "                    embedding_pep,\n",
    "                    embedding_prot,\n",
    "                    # interaction_pep,\n",
    "                    # interaction_prot,\n",
    "                    # int_prob = 0.0\n",
    "                    )\n",
    "            \n",
    "            # loss of predicting partner using peptide\n",
    "            positive_loss = F.binary_cross_entropy_with_logits(positive_logits, torch.ones_like(positive_logits).to(device))\n",
    "            \n",
    "            # Negaive indexes\n",
    "            rows, cols = torch.triu_indices(embedding_prot.size(0), embedding_prot.size(0), offset=1) \n",
    "            \n",
    "\n",
    "            negative_logits = self(embedding_pep[rows,:,:], \n",
    "                              embedding_prot[cols,:,:], \n",
    "                              int_prob=0.0)\n",
    "                   \n",
    "            \n",
    "            negative_loss =  F.binary_cross_entropy_with_logits(negative_logits, torch.zeros_like(negative_logits).to(device))\n",
    "\n",
    "            loss = (positive_loss + negative_loss) / 2\n",
    "           \n",
    "            logit_matrix = torch.zeros((embedding_pep.size(0),embedding_pep.size(0)),device=self.device)\n",
    "            logit_matrix[rows, cols] = negative_logits\n",
    "            logit_matrix[cols, rows] = negative_logits\n",
    "            \n",
    "            # Fill diagonal with positive scores\n",
    "            diag_indices = torch.arange(embedding_pep.size(0), device=self.device)\n",
    "            logit_matrix[diag_indices, diag_indices] = positive_logits.squeeze()\n",
    "\n",
    "            labels = torch.arange(embedding_prot.size(0)).to(self.device)\n",
    "            peptide_predictions = logit_matrix.argmax(dim=0)\n",
    "            peptide_ranks = logit_matrix.argsort(dim=0).diag() + 1\n",
    "            peptide_mrr = (peptide_ranks).float().pow(-1).mean()\n",
    "            \n",
    "            # partner_accuracy = partner_predictions.eq(labels).float().mean()\n",
    "            peptide_accuracy = peptide_predictions.eq(labels).float().mean()\n",
    "    \n",
    "            k = 3\n",
    "            peptide_topk_accuracy = torch.any((logit_matrix.topk(k, dim=0).indices - labels.reshape(1, -1)) == 0, dim=0).sum() / logit_matrix.shape[0]\n",
    "            # partner_topk_accuracy = torch.any((logits.topk(k, dim=1).indices - labels.reshape(-1, 1)) == 0, dim=1).sum() / logits.shape[0]\n",
    "    \n",
    "            del logit_matrix,positive_logits,negative_logits,embedding_pep,embedding_prot\n",
    "\n",
    "            return loss, peptide_accuracy, peptide_topk_accuracy\n",
    "\n",
    "    def calculate_logit_matrix(self,embedding_pep,embedding_prot):\n",
    "        positive_logits = self(\n",
    "            embedding_pep,\n",
    "            embedding_prot)\n",
    "        \n",
    "        # Negaive indexes\n",
    "        rows, cols = torch.triu_indices(embedding_pep.size(0), embedding_pep.size(0), offset=1) \n",
    "        \n",
    "        negative_logits = self(embedding_pep[rows,:,:], \n",
    "                      embedding_prot[cols,:,:], \n",
    "                      int_prob=0.0)\n",
    "        \n",
    "        logit_matrix = torch.zeros((embedding_pep.size(0),embedding_pep.size(0)),device=self.device)\n",
    "        logit_matrix[rows, cols] = negative_logits\n",
    "        logit_matrix[cols, rows] = negative_logits\n",
    "        # Fill diagonal with positive scores\n",
    "        diag_indices = torch.arange(embedding_pep.size(0), device=self.device)\n",
    "        logit_matrix[diag_indices, diag_indices] = positive_logits.squeeze()\n",
    "        return logit_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "75d75ddc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MiniCLIP_w_transformer_crossattn(\n",
       "  (transformerencoder): TransformerEncoderLayer(\n",
       "    (self_attn): MultiheadAttention(\n",
       "      (out_proj): NonDynamicallyQuantizableLinear(in_features=1152, out_features=1152, bias=True)\n",
       "    )\n",
       "    (linear1): Linear(in_features=1152, out_features=1152, bias=True)\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "    (linear2): Linear(in_features=1152, out_features=1152, bias=True)\n",
       "    (norm1): LayerNorm((1152,), eps=1e-05, elementwise_affine=True)\n",
       "    (norm2): LayerNorm((1152,), eps=1e-05, elementwise_affine=True)\n",
       "    (dropout1): Dropout(p=0.1, inplace=False)\n",
       "    (dropout2): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (norm): LayerNorm((1152,), eps=1e-05, elementwise_affine=True)\n",
       "  (cross_attn): MultiheadAttention(\n",
       "    (out_proj): NonDynamicallyQuantizableLinear(in_features=1152, out_features=1152, bias=True)\n",
       "  )\n",
       "  (prot_embedder): Sequential(\n",
       "    (0): Linear(in_features=1152, out_features=640, bias=True)\n",
       "    (1): ReLU()\n",
       "    (2): Linear(in_features=640, out_features=320, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "path = '../PPI_PLM/models/CLIP_no_structural_information/a1d0549b-3f90-4ce2-b795-7bca2276cb07_checkpoint_4/a1d0549b-3f90-4ce2-b795-7bca2276cb07_checkpoint_epoch_4.pth'\n",
    "checkpoint = torch.load(path, weights_only=False, map_location=torch.device('cpu'))\n",
    "# print(list(checkpoint[\"model_state_dict\"]))\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "model = MiniCLIP_w_transformer_crossattn()\n",
    "model.load_state_dict(checkpoint['model_state_dict'])\n",
    "model.to(device)\n",
    "model.eval()  # or model.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "099fafee",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['VirB8', 'FGFR2', 'IL7Ra', 'InsulinR', 'EGFR', 'SARS_CoV2_RBD',\n",
       "       'Pdl1', 'EGFR_2', 'TrkA', 'IL10Ra', 'LTK', 'Mdm2', 'EGFR_3',\n",
       "       'sntx', 'sntx_2', 'IL2Ra'], dtype=object)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "interaction_df = pd.read_csv(\"../data/meta_analysis/interaction_df_metaanal.csv\", index_col = 0).drop(columns = [\"binder_id\", \"target_id\"]).rename(columns={\n",
    "    \"A_seq\" : \"binder_seq\",\n",
    "    \"B_seq\" : \"target_seq\"\n",
    "})\n",
    "interaction_df.target_id_mod.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e2a55526",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Targets df\n",
    "target_df = interaction_df[[\"target_id_mod\",\"target_seq\"]].rename(columns={\"target_seq\":\"sequence\", \"target_id_mod\" : \"ID\"})\n",
    "target_df[\"seq_len\"] = target_df[\"sequence\"].apply(len)\n",
    "target_df = target_df.drop_duplicates(subset=[\"ID\",\"sequence\"])\n",
    "target_df = target_df.set_index(\"ID\")\n",
    "\n",
    "# Binders df\n",
    "binder_df = interaction_df[[\"target_binder_ID\",\"binder_seq\"]].rename(columns={\"binder_seq\":\"sequence\", \"target_binder_ID\" : \"ID\"})\n",
    "binder_df[\"seq_len\"] = binder_df[\"sequence\"].apply(len)\n",
    "binder_df = binder_df.set_index(\"ID\")\n",
    "\n",
    "# target_df\n",
    "\n",
    "# Interaction Dict\n",
    "interaction_Dict = dict(enumerate(zip(interaction_df[\"target_id_mod\"], interaction_df[\"target_binder_ID\"]), start=1))\n",
    "# interaction_Dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "182a1da1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([16, 621, 1152])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max_len = int(target_df['sequence'].str.len().max())        # if strings\n",
    "torch.full((len(target_df), max_len, 1152), -5000, dtype=torch.float32).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "786bba76-81a0-48af-8bbe-ebde8d44eae4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sequence</th>\n",
       "      <th>seq_len</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ID</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>VirB8</th>\n",
       "      <td>ANPYISVANIMLQNYVKQREKYNYDTLKEQFTFIKNASTSIVYMQF...</td>\n",
       "      <td>138</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>FGFR2</th>\n",
       "      <td>RSPHRPILQAGLPANASTVVGGDVEFVCKVYSDAQPHIQWIKHVPY...</td>\n",
       "      <td>101</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>IL7Ra</th>\n",
       "      <td>DYSFSCYSQLEVNGSQHSLTCAFEDPDVNTTNLEFEICGALVEVKC...</td>\n",
       "      <td>193</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>InsulinR</th>\n",
       "      <td>EVCPGMDIRNNLTRLHELENCSVIEGHLQILLMFKTRPEDFRDLSF...</td>\n",
       "      <td>150</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>EGFR</th>\n",
       "      <td>RKVCNGIGIGEFKDSLSINATNIKHFKNCTSISGDLHILPVAFRGD...</td>\n",
       "      <td>191</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   sequence  seq_len\n",
       "ID                                                                  \n",
       "VirB8     ANPYISVANIMLQNYVKQREKYNYDTLKEQFTFIKNASTSIVYMQF...      138\n",
       "FGFR2     RSPHRPILQAGLPANASTVVGGDVEFVCKVYSDAQPHIQWIKHVPY...      101\n",
       "IL7Ra     DYSFSCYSQLEVNGSQHSLTCAFEDPDVNTTNLEFEICGALVEVKC...      193\n",
       "InsulinR  EVCPGMDIRNNLTRLHELENCSVIEGHLQILLMFKTRPEDFRDLSF...      150\n",
       "EGFR      RKVCNGIGIGEFKDSLSINATNIKHFKNCTSISGDLHILPVAFRGD...      191"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ffdc59cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CLIP_dataset(Dataset):\n",
    "    def __init__(self, sequence_df, esm_encoding_paths, embedding_dim=1152, padding_value=-5000):\n",
    "        super(CLIP_dataset, self).__init__()\n",
    "\n",
    "        self.sigma = 1\n",
    "        self.sequence_df = sequence_df\n",
    "        self.max_length = sequence_df[\"seq_len\"].max()\n",
    "        self.sequence_df[\"index_num\"] = np.arange(len(self.sequence_df))\n",
    "        self.esm_encoding_paths = esm_encoding_paths\n",
    "        num_samples = len(self.sequence_df)\n",
    "        self.x = torch.full((num_samples, self.max_length, embedding_dim), padding_value, dtype=torch.float32)\n",
    "        \n",
    "        # Load embeddings into the pre-allocated tensor\n",
    "        iterator = tqdm(self.sequence_df.index.tolist(), position=0, leave=True, total=num_samples, desc=\"# Reading in ESM-embeddings from folder\")\n",
    "        for i, accession in enumerate(iterator):\n",
    "            try:\n",
    "                embd = np.load(os.path.join(esm_encoding_paths, accession + \".npy\"))[0]\n",
    "                length_to_pad = self.max_length - len(embd)\n",
    "                if length_to_pad > 0:\n",
    "                    zero_padding = np.ones((length_to_pad, embd.shape[1])) * padding_value\n",
    "                    padded_array = np.concatenate((embd, zero_padding), axis=0)\n",
    "                else:\n",
    "                    padded_array = embd[:self.max_length] \n",
    "                self.x[i] = torch.tensor(padded_array, dtype=torch.float32)\n",
    "            except FileNotFoundError as e:\n",
    "                raise FileNotFoundError(f\"Embedding file {accession}.npy not found.\")\n",
    "                \n",
    "    def __len__(self):\n",
    "        return int(self.x.shape[0])\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        index1 = self.sequence_df.loc[index,\"index_num\"]\n",
    "        return self.x[index1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "86b6bf0e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "# Reading in ESM-embeddings from folder:   0%|          | 0/16 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "# Reading in ESM-embeddings from folder: 100%|██████████| 16/16 [00:00<00:00, 210.56it/s]\n",
      "# Reading in ESM-embeddings from folder: 100%|██████████| 3532/3532 [00:04<00:00, 750.22it/s]\n"
     ]
    }
   ],
   "source": [
    "targets_dataset = CLIP_dataset(target_df, esm_encoding_paths=\"../data/meta_analysis/targets_embeddings\", embedding_dim=1152)\n",
    "binders_dataset = CLIP_dataset(binder_df, esm_encoding_paths=\"../data/meta_analysis/binders_embeddings\", embedding_dim=1152)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9209614f-4991-4f7d-b257-775369d106c4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([250, 1152])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "binders_dataset[\"IL7Ra_4\"].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "50e73806-7fa3-4b54-9ae1-73dba9aeb092",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('VirB8', 'VirB8_1'),\n",
       " ('FGFR2', 'FGFR2_1'),\n",
       " ('FGFR2', 'FGFR2_2'),\n",
       " ('FGFR2', 'FGFR2_3'),\n",
       " ('FGFR2', 'FGFR2_4'),\n",
       " ('FGFR2', 'FGFR2_5'),\n",
       " ('FGFR2', 'FGFR2_6'),\n",
       " ('FGFR2', 'FGFR2_7'),\n",
       " ('FGFR2', 'FGFR2_8'),\n",
       " ('FGFR2', 'FGFR2_9')]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "interaction_lst = list(interaction_Dict.values())\n",
    "interaction_lst[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f4de8614-3744-4d1f-b740-d1bf82ec1a36",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([250, 1152])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Batching function:\n",
    "def batch_list(iterable, batch_size=1):\n",
    "    length = len(iterable)\n",
    "    for start in range(0, length, batch_size):\n",
    "        end = min(start + batch_size, length)\n",
    "        yield iterable[start:end]\n",
    "\n",
    "batch_size = 20\n",
    "batched_tb = batch_list(interaction_lst, batch_size)\n",
    "\n",
    "# num_batches = (len(interaction_lst) + batch_size - 1) // batch_size\n",
    "# print(num_batches)\n",
    "\n",
    "binders = [i[1] for i in interaction_lst[:20]]\n",
    "binders_embd = [binders_dataset[i] for i in binders]\n",
    "binders_embd[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "bcbd7932-4dc2-414a-8d39-7169acfdacdb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "#Iterating through batched data:   2%|▏         | 3/177 [00:19<19:01,  6.56s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[15], line 19\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[38;5;66;03m# binders_batch = next(iter(binders_dataloader)).to(device)\u001b[39;00m\n\u001b[1;32m     16\u001b[0m \u001b[38;5;66;03m# targets_batch = next(iter(targets_dataloader)).to(device)\u001b[39;00m\n\u001b[1;32m     18\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[0;32m---> 19\u001b[0m     logits \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbinders_batch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtargets_batch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     20\u001b[0m     \u001b[38;5;66;03m# CLIP_logits represents probailbities of interacting CLIP_logits[i] = prob of mb_batch[i] interacting with hp_index\u001b[39;00m\n\u001b[1;32m     21\u001b[0m     interaction_scores\u001b[38;5;241m.\u001b[39mappend(logits\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m0\u001b[39m))\n",
      "File \u001b[0;32m~/miniconda3/envs/esm_gpu/lib/python3.10/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/esm_gpu/lib/python3.10/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "Cell \u001b[0;32mIn[4], line 94\u001b[0m, in \u001b[0;36mMiniCLIP_w_transformer_crossattn.forward\u001b[0;34m(self, pep_input, prot_input, pep_int_mask, prot_int_mask, int_prob, mem_save)\u001b[0m\n\u001b[1;32m     92\u001b[0m \u001b[38;5;66;03m# Cross-attention with residual\u001b[39;00m\n\u001b[1;32m     93\u001b[0m pep_cross, _ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcross_attn(query\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnorm(pep_trans), key\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnorm(prot_trans), value\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnorm(prot_trans), key_padding_mask\u001b[38;5;241m=\u001b[39mprot_mask)\n\u001b[0;32m---> 94\u001b[0m prot_cross, _ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcross_attn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mquery\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnorm\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprot_trans\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkey\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnorm\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpep_trans\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalue\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnorm\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpep_trans\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkey_padding_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpep_mask\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     96\u001b[0m \u001b[38;5;66;03m# Additive update with residual connection\u001b[39;00m\n\u001b[1;32m     97\u001b[0m pep_emb \u001b[38;5;241m=\u001b[39m pep_emb \u001b[38;5;241m+\u001b[39m pep_cross  \n",
      "File \u001b[0;32m~/miniconda3/envs/esm_gpu/lib/python3.10/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/esm_gpu/lib/python3.10/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/miniconda3/envs/esm_gpu/lib/python3.10/site-packages/torch/nn/modules/activation.py:1368\u001b[0m, in \u001b[0;36mMultiheadAttention.forward\u001b[0;34m(self, query, key, value, key_padding_mask, need_weights, attn_mask, average_attn_weights, is_causal)\u001b[0m\n\u001b[1;32m   1342\u001b[0m     attn_output, attn_output_weights \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39mmulti_head_attention_forward(\n\u001b[1;32m   1343\u001b[0m         query,\n\u001b[1;32m   1344\u001b[0m         key,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1365\u001b[0m         is_causal\u001b[38;5;241m=\u001b[39mis_causal,\n\u001b[1;32m   1366\u001b[0m     )\n\u001b[1;32m   1367\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1368\u001b[0m     attn_output, attn_output_weights \u001b[38;5;241m=\u001b[39m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmulti_head_attention_forward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1369\u001b[0m \u001b[43m        \u001b[49m\u001b[43mquery\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1370\u001b[0m \u001b[43m        \u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1371\u001b[0m \u001b[43m        \u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1372\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43membed_dim\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1373\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnum_heads\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1374\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43min_proj_weight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1375\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43min_proj_bias\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1376\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias_k\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1377\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias_v\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1378\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43madd_zero_attn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1379\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdropout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1380\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mout_proj\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1381\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mout_proj\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1382\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtraining\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtraining\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1383\u001b[0m \u001b[43m        \u001b[49m\u001b[43mkey_padding_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkey_padding_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1384\u001b[0m \u001b[43m        \u001b[49m\u001b[43mneed_weights\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mneed_weights\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1385\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattn_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattn_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1386\u001b[0m \u001b[43m        \u001b[49m\u001b[43maverage_attn_weights\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maverage_attn_weights\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1387\u001b[0m \u001b[43m        \u001b[49m\u001b[43mis_causal\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_causal\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1388\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1389\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbatch_first \u001b[38;5;129;01mand\u001b[39;00m is_batched:\n\u001b[1;32m   1390\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m attn_output\u001b[38;5;241m.\u001b[39mtranspose(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m0\u001b[39m), attn_output_weights\n",
      "File \u001b[0;32m~/miniconda3/envs/esm_gpu/lib/python3.10/site-packages/torch/nn/functional.py:6237\u001b[0m, in \u001b[0;36mmulti_head_attention_forward\u001b[0;34m(query, key, value, embed_dim_to_check, num_heads, in_proj_weight, in_proj_bias, bias_k, bias_v, add_zero_attn, dropout_p, out_proj_weight, out_proj_bias, training, key_padding_mask, need_weights, attn_mask, use_separate_proj_weight, q_proj_weight, k_proj_weight, v_proj_weight, static_k, static_v, average_attn_weights, is_causal)\u001b[0m\n\u001b[1;32m   6232\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\n\u001b[1;32m   6233\u001b[0m     is_causal \u001b[38;5;129;01mand\u001b[39;00m attn_mask \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   6234\u001b[0m ), \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFIXME: is_causal not implemented for need_weights\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   6236\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m attn_mask \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m-> 6237\u001b[0m     attn_output_weights \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbaddbmm\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   6238\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattn_mask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mq_scaled\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mk\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtranspose\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m   6239\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   6240\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   6241\u001b[0m     attn_output_weights \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mbmm(q_scaled, k\u001b[38;5;241m.\u001b[39mtranspose(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m))\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Loading batches\n",
    "interaction_scores = []\n",
    "\n",
    "for batch in tqdm(batched_tb, total = round(len(interaction_df)/batch_size),  desc= \"#Iterating through batched data\"):\n",
    "    targets_acc = [taget_binder[0] for taget_binder in batch]\n",
    "    targets_embd = [targets_dataset[i] for i in targets_acc]\n",
    "    binders_acc = [taget_binder[1] for taget_binder in batch]\n",
    "    binders_embd = [binders_dataset[i] for i in binders_acc]\n",
    "\n",
    "    # binders_dataloader = DataLoader(binders_embd, batch_size=batch_size, shuffle=False)\n",
    "    # targets_dataloader = DataLoader(targets_embd, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "    binders_batch = torch.stack([binders_dataset[i] for i in binders_acc]).to(device)\n",
    "    targets_batch = torch.stack([targets_dataset[i] for i in targets_acc]).to(device)\n",
    "    # binders_batch = next(iter(binders_dataloader)).to(device)\n",
    "    # targets_batch = next(iter(targets_dataloader)).to(device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        logits = model(binders_batch, targets_batch)\n",
    "        # CLIP_logits represents probailbities of interacting CLIP_logits[i] = prob of mb_batch[i] interacting with hp_index\n",
    "        interaction_scores.append(logits.unsqueeze(0))\n",
    "        # print(interaction_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47c4d4ba-d17e-47be-85ce-298d23391473",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>binder_chain</th>\n",
       "      <th>target_chains</th>\n",
       "      <th>binder</th>\n",
       "      <th>binder_seq</th>\n",
       "      <th>target_seq</th>\n",
       "      <th>target_id_mod</th>\n",
       "      <th>target_binder_ID</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>A</td>\n",
       "      <td>[\"B\"]</td>\n",
       "      <td>False</td>\n",
       "      <td>LDFIVFAGPEKAIKFYKEMAKRNLEVKIWIDGDWAVVQVK</td>\n",
       "      <td>ANPYISVANIMLQNYVKQREKYNYDTLKEQFTFIKNASTSIVYMQF...</td>\n",
       "      <td>VirB8</td>\n",
       "      <td>VirB8_1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>A</td>\n",
       "      <td>[\"B\"]</td>\n",
       "      <td>False</td>\n",
       "      <td>SEQDETMHRIVRSVIQHAYKHNDEMAEYFAQNAAEIYKEQNKSEEA...</td>\n",
       "      <td>RSPHRPILQAGLPANASTVVGGDVEFVCKVYSDAQPHIQWIKHVPY...</td>\n",
       "      <td>FGFR2</td>\n",
       "      <td>FGFR2_1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>A</td>\n",
       "      <td>[\"B\"]</td>\n",
       "      <td>False</td>\n",
       "      <td>DYKQLKKHATKLLELAKKDPSSKRDLLRTAASYANKVLFEDSDPRA...</td>\n",
       "      <td>RSPHRPILQAGLPANASTVVGGDVEFVCKVYSDAQPHIQWIKHVPY...</td>\n",
       "      <td>FGFR2</td>\n",
       "      <td>FGFR2_2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>A</td>\n",
       "      <td>[\"B\"]</td>\n",
       "      <td>False</td>\n",
       "      <td>DEKEELERRANRVAFLAIQIQNEEYHRILAELYVQFMKAAENNDTE...</td>\n",
       "      <td>RSPHRPILQAGLPANASTVVGGDVEFVCKVYSDAQPHIQWIKHVPY...</td>\n",
       "      <td>FGFR2</td>\n",
       "      <td>FGFR2_3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>A</td>\n",
       "      <td>[\"B\"]</td>\n",
       "      <td>False</td>\n",
       "      <td>PDNKEKLMSIAVQLILRINEAARSEEQWRYANRAAFAAVEASSGSD...</td>\n",
       "      <td>RSPHRPILQAGLPANASTVVGGDVEFVCKVYSDAQPHIQWIKHVPY...</td>\n",
       "      <td>FGFR2</td>\n",
       "      <td>FGFR2_4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3527</th>\n",
       "      <td>A</td>\n",
       "      <td>[\"B\"]</td>\n",
       "      <td>False</td>\n",
       "      <td>DLRKYAAELVDRLAEKYNLDSDQYNALVRLASELVWQGKSKEEIEK...</td>\n",
       "      <td>ELCDDDPPEIPHATFKAMAYKEGTMLNCECKRGFRRIKSGSLYMLC...</td>\n",
       "      <td>IL2Ra</td>\n",
       "      <td>IL2Ra_62</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3528</th>\n",
       "      <td>A</td>\n",
       "      <td>[\"B\"]</td>\n",
       "      <td>False</td>\n",
       "      <td>SKEEIKKEAEELIEELKKKGYNLPLRILEFALKEIEETNSEKYYEQ...</td>\n",
       "      <td>ELCDDDPPEIPHATFKAMAYKEGTMLNCECKRGFRRIKSGSLYMLC...</td>\n",
       "      <td>IL2Ra</td>\n",
       "      <td>IL2Ra_63</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3529</th>\n",
       "      <td>A</td>\n",
       "      <td>[\"B\"]</td>\n",
       "      <td>False</td>\n",
       "      <td>SPEYKKFLELIKEAEAARKAGDLDKAKELLEKALELAKKMKAKSLI...</td>\n",
       "      <td>ELCDDDPPEIPHATFKAMAYKEGTMLNCECKRGFRRIKSGSLYMLC...</td>\n",
       "      <td>IL2Ra</td>\n",
       "      <td>IL2Ra_64</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3530</th>\n",
       "      <td>A</td>\n",
       "      <td>[\"B\"]</td>\n",
       "      <td>False</td>\n",
       "      <td>DPLLAYKLLKLSQKALEKAYAEDRERAEELLEEAEAALRSLGDEAG...</td>\n",
       "      <td>ELCDDDPPEIPHATFKAMAYKEGTMLNCECKRGFRRIKSGSLYMLC...</td>\n",
       "      <td>IL2Ra</td>\n",
       "      <td>IL2Ra_65</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3531</th>\n",
       "      <td>A</td>\n",
       "      <td>[\"B\"]</td>\n",
       "      <td>False</td>\n",
       "      <td>SEAARRARELFHEADELDKRGNPEEAEEVLREALELAREAGSPNLA...</td>\n",
       "      <td>ELCDDDPPEIPHATFKAMAYKEGTMLNCECKRGFRRIKSGSLYMLC...</td>\n",
       "      <td>IL2Ra</td>\n",
       "      <td>IL2Ra_66</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3532 rows × 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     binder_chain target_chains  binder  \\\n",
       "0               A         [\"B\"]   False   \n",
       "1               A         [\"B\"]   False   \n",
       "2               A         [\"B\"]   False   \n",
       "3               A         [\"B\"]   False   \n",
       "4               A         [\"B\"]   False   \n",
       "...           ...           ...     ...   \n",
       "3527            A         [\"B\"]   False   \n",
       "3528            A         [\"B\"]   False   \n",
       "3529            A         [\"B\"]   False   \n",
       "3530            A         [\"B\"]   False   \n",
       "3531            A         [\"B\"]   False   \n",
       "\n",
       "                                             binder_seq  \\\n",
       "0              LDFIVFAGPEKAIKFYKEMAKRNLEVKIWIDGDWAVVQVK   \n",
       "1     SEQDETMHRIVRSVIQHAYKHNDEMAEYFAQNAAEIYKEQNKSEEA...   \n",
       "2     DYKQLKKHATKLLELAKKDPSSKRDLLRTAASYANKVLFEDSDPRA...   \n",
       "3     DEKEELERRANRVAFLAIQIQNEEYHRILAELYVQFMKAAENNDTE...   \n",
       "4     PDNKEKLMSIAVQLILRINEAARSEEQWRYANRAAFAAVEASSGSD...   \n",
       "...                                                 ...   \n",
       "3527  DLRKYAAELVDRLAEKYNLDSDQYNALVRLASELVWQGKSKEEIEK...   \n",
       "3528  SKEEIKKEAEELIEELKKKGYNLPLRILEFALKEIEETNSEKYYEQ...   \n",
       "3529  SPEYKKFLELIKEAEAARKAGDLDKAKELLEKALELAKKMKAKSLI...   \n",
       "3530  DPLLAYKLLKLSQKALEKAYAEDRERAEELLEEAEAALRSLGDEAG...   \n",
       "3531  SEAARRARELFHEADELDKRGNPEEAEEVLREALELAREAGSPNLA...   \n",
       "\n",
       "                                             target_seq target_id_mod  \\\n",
       "0     ANPYISVANIMLQNYVKQREKYNYDTLKEQFTFIKNASTSIVYMQF...         VirB8   \n",
       "1     RSPHRPILQAGLPANASTVVGGDVEFVCKVYSDAQPHIQWIKHVPY...         FGFR2   \n",
       "2     RSPHRPILQAGLPANASTVVGGDVEFVCKVYSDAQPHIQWIKHVPY...         FGFR2   \n",
       "3     RSPHRPILQAGLPANASTVVGGDVEFVCKVYSDAQPHIQWIKHVPY...         FGFR2   \n",
       "4     RSPHRPILQAGLPANASTVVGGDVEFVCKVYSDAQPHIQWIKHVPY...         FGFR2   \n",
       "...                                                 ...           ...   \n",
       "3527  ELCDDDPPEIPHATFKAMAYKEGTMLNCECKRGFRRIKSGSLYMLC...         IL2Ra   \n",
       "3528  ELCDDDPPEIPHATFKAMAYKEGTMLNCECKRGFRRIKSGSLYMLC...         IL2Ra   \n",
       "3529  ELCDDDPPEIPHATFKAMAYKEGTMLNCECKRGFRRIKSGSLYMLC...         IL2Ra   \n",
       "3530  ELCDDDPPEIPHATFKAMAYKEGTMLNCECKRGFRRIKSGSLYMLC...         IL2Ra   \n",
       "3531  ELCDDDPPEIPHATFKAMAYKEGTMLNCECKRGFRRIKSGSLYMLC...         IL2Ra   \n",
       "\n",
       "     target_binder_ID  \n",
       "0             VirB8_1  \n",
       "1             FGFR2_1  \n",
       "2             FGFR2_2  \n",
       "3             FGFR2_3  \n",
       "4             FGFR2_4  \n",
       "...               ...  \n",
       "3527         IL2Ra_62  \n",
       "3528         IL2Ra_63  \n",
       "3529         IL2Ra_64  \n",
       "3530         IL2Ra_65  \n",
       "3531         IL2Ra_66  \n",
       "\n",
       "[3532 rows x 7 columns]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Labels for the predictions\n",
    "interaction_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "da8d1ca3-e9c9-4a98-9e7a-d5f52a7d0e18",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Length of values (60) does not match length of index (3532)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[19], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m predicted_interaction_scores \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mconcatenate([batch_score\u001b[38;5;241m.\u001b[39mcpu()\u001b[38;5;241m.\u001b[39mdetach()\u001b[38;5;241m.\u001b[39mnumpy()\u001b[38;5;241m.\u001b[39mreshape(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m,) \u001b[38;5;28;01mfor\u001b[39;00m batch_score \u001b[38;5;129;01min\u001b[39;00m interaction_scores])\n\u001b[1;32m      2\u001b[0m interaction_probabilities \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mconcatenate([torch\u001b[38;5;241m.\u001b[39msigmoid(batch_score[\u001b[38;5;241m0\u001b[39m])\u001b[38;5;241m.\u001b[39mcpu()\u001b[38;5;241m.\u001b[39mnumpy() \u001b[38;5;28;01mfor\u001b[39;00m batch_score \u001b[38;5;129;01min\u001b[39;00m interaction_scores])\n\u001b[0;32m----> 4\u001b[0m \u001b[43minteraction_df\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43minter_prob\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m \u001b[38;5;241m=\u001b[39m interaction_probabilities\n\u001b[1;32m      5\u001b[0m interaction_df[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpred_binder\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m interaction_df[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minter_prob\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.5\u001b[39m\n\u001b[1;32m      6\u001b[0m interaction_df[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mintr_scores\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m predicted_interaction_scores\n",
      "File \u001b[0;32m~/miniconda3/envs/esm_gpu/lib/python3.10/site-packages/pandas/core/frame.py:4311\u001b[0m, in \u001b[0;36mDataFrame.__setitem__\u001b[0;34m(self, key, value)\u001b[0m\n\u001b[1;32m   4308\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_setitem_array([key], value)\n\u001b[1;32m   4309\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   4310\u001b[0m     \u001b[38;5;66;03m# set column\u001b[39;00m\n\u001b[0;32m-> 4311\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_set_item\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/esm_gpu/lib/python3.10/site-packages/pandas/core/frame.py:4524\u001b[0m, in \u001b[0;36mDataFrame._set_item\u001b[0;34m(self, key, value)\u001b[0m\n\u001b[1;32m   4514\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_set_item\u001b[39m(\u001b[38;5;28mself\u001b[39m, key, value) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   4515\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   4516\u001b[0m \u001b[38;5;124;03m    Add series to DataFrame in specified column.\u001b[39;00m\n\u001b[1;32m   4517\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   4522\u001b[0m \u001b[38;5;124;03m    ensure homogeneity.\u001b[39;00m\n\u001b[1;32m   4523\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 4524\u001b[0m     value, refs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sanitize_column\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   4526\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m   4527\u001b[0m         key \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcolumns\n\u001b[1;32m   4528\u001b[0m         \u001b[38;5;129;01mand\u001b[39;00m value\u001b[38;5;241m.\u001b[39mndim \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m   4529\u001b[0m         \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(value\u001b[38;5;241m.\u001b[39mdtype, ExtensionDtype)\n\u001b[1;32m   4530\u001b[0m     ):\n\u001b[1;32m   4531\u001b[0m         \u001b[38;5;66;03m# broadcast across multiple columns if necessary\u001b[39;00m\n\u001b[1;32m   4532\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcolumns\u001b[38;5;241m.\u001b[39mis_unique \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcolumns, MultiIndex):\n",
      "File \u001b[0;32m~/miniconda3/envs/esm_gpu/lib/python3.10/site-packages/pandas/core/frame.py:5266\u001b[0m, in \u001b[0;36mDataFrame._sanitize_column\u001b[0;34m(self, value)\u001b[0m\n\u001b[1;32m   5263\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _reindex_for_setitem(value, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mindex)\n\u001b[1;32m   5265\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_list_like(value):\n\u001b[0;32m-> 5266\u001b[0m     \u001b[43mcom\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrequire_length_match\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   5267\u001b[0m arr \u001b[38;5;241m=\u001b[39m sanitize_array(value, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mindex, copy\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, allow_2d\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m   5268\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m   5269\u001b[0m     \u001b[38;5;28misinstance\u001b[39m(value, Index)\n\u001b[1;32m   5270\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m value\u001b[38;5;241m.\u001b[39mdtype \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mobject\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   5273\u001b[0m     \u001b[38;5;66;03m# TODO: Remove kludge in sanitize_array for string mode when enforcing\u001b[39;00m\n\u001b[1;32m   5274\u001b[0m     \u001b[38;5;66;03m# this deprecation\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/esm_gpu/lib/python3.10/site-packages/pandas/core/common.py:573\u001b[0m, in \u001b[0;36mrequire_length_match\u001b[0;34m(data, index)\u001b[0m\n\u001b[1;32m    569\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    570\u001b[0m \u001b[38;5;124;03mCheck the length of data matches the length of the index.\u001b[39;00m\n\u001b[1;32m    571\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    572\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(data) \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28mlen\u001b[39m(index):\n\u001b[0;32m--> 573\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    574\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLength of values \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    575\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m(\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(data)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m) \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    576\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdoes not match length of index \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    577\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m(\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(index)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m)\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    578\u001b[0m     )\n",
      "\u001b[0;31mValueError\u001b[0m: Length of values (60) does not match length of index (3532)"
     ]
    }
   ],
   "source": [
    "predicted_interaction_scores = np.concatenate([batch_score.cpu().detach().numpy().reshape(-1,) for batch_score in interaction_scores])\n",
    "interaction_probabilities = np.concatenate([torch.sigmoid(batch_score[0]).cpu().numpy() for batch_score in interaction_scores])\n",
    "\n",
    "interaction_df[\"inter_prob\"] = interaction_probabilities\n",
    "interaction_df[\"pred_binder\"] = interaction_df[\"inter_prob\"] >= 0.5\n",
    "interaction_df[\"intr_scores\"] = predicted_interaction_scores\n",
    "\n",
    "interaction_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75452514-1cb9-4ceb-bc96-fca81390de63",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1.2981, device='cuda:0')"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels = torch.as_tensor(interaction_df[\"binder\"].to_numpy(copy=False), dtype=torch.float32, device=device)\n",
    "logits = torch.as_tensor(predicted_interaction_scores, dtype=torch.float32, device=device)\n",
    "\n",
    "loss = F.binary_cross_entropy_with_logits(logits, labels)\n",
    "loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a305261e",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Could not interpret value `intr_scores` for `y`. An entry with this name does not appear in `data`.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[17], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43msns\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mboxplot\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minteraction_df\u001b[49m\u001b[43m,\u001b[49m\u001b[43mx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtarget_id_mod\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43my\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mintr_scores\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43mhue\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mbinder\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43mlegend\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m      2\u001b[0m plt\u001b[38;5;241m.\u001b[39mxticks(rotation\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m90\u001b[39m)\n\u001b[1;32m      3\u001b[0m plt\u001b[38;5;241m.\u001b[39mgrid()\n",
      "File \u001b[0;32m~/miniconda3/envs/esm_gpu/lib/python3.10/site-packages/seaborn/categorical.py:1597\u001b[0m, in \u001b[0;36mboxplot\u001b[0;34m(data, x, y, hue, order, hue_order, orient, color, palette, saturation, fill, dodge, width, gap, whis, linecolor, linewidth, fliersize, hue_norm, native_scale, log_scale, formatter, legend, ax, **kwargs)\u001b[0m\n\u001b[1;32m   1589\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mboxplot\u001b[39m(\n\u001b[1;32m   1590\u001b[0m     data\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;241m*\u001b[39m, x\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, y\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, hue\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, order\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, hue_order\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m   1591\u001b[0m     orient\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, color\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, palette\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, saturation\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m.75\u001b[39m, fill\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1594\u001b[0m     legend\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mauto\u001b[39m\u001b[38;5;124m\"\u001b[39m, ax\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs\n\u001b[1;32m   1595\u001b[0m ):\n\u001b[0;32m-> 1597\u001b[0m     p \u001b[38;5;241m=\u001b[39m \u001b[43m_CategoricalPlotter\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1598\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdata\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1599\u001b[0m \u001b[43m        \u001b[49m\u001b[43mvariables\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mdict\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhue\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhue\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1600\u001b[0m \u001b[43m        \u001b[49m\u001b[43morder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43morder\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1601\u001b[0m \u001b[43m        \u001b[49m\u001b[43morient\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43morient\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1602\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcolor\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcolor\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1603\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlegend\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlegend\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1604\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1606\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m ax \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   1607\u001b[0m         ax \u001b[38;5;241m=\u001b[39m plt\u001b[38;5;241m.\u001b[39mgca()\n",
      "File \u001b[0;32m~/miniconda3/envs/esm_gpu/lib/python3.10/site-packages/seaborn/categorical.py:67\u001b[0m, in \u001b[0;36m_CategoricalPlotter.__init__\u001b[0;34m(self, data, variables, order, orient, require_numeric, color, legend)\u001b[0m\n\u001b[1;32m     56\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m__init__\u001b[39m(\n\u001b[1;32m     57\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m     58\u001b[0m     data\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     64\u001b[0m     legend\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mauto\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     65\u001b[0m ):\n\u001b[0;32m---> 67\u001b[0m     \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvariables\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvariables\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     69\u001b[0m     \u001b[38;5;66;03m# This method takes care of some bookkeeping that is necessary because the\u001b[39;00m\n\u001b[1;32m     70\u001b[0m     \u001b[38;5;66;03m# original categorical plots (prior to the 2021 refactor) had some rules that\u001b[39;00m\n\u001b[1;32m     71\u001b[0m     \u001b[38;5;66;03m# don't fit exactly into VectorPlotter logic. It may be wise to have a second\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     76\u001b[0m     \u001b[38;5;66;03m# default VectorPlotter rules. If we do decide to make orient part of the\u001b[39;00m\n\u001b[1;32m     77\u001b[0m     \u001b[38;5;66;03m# _base variable assignment, we'll want to figure out how to express that.\u001b[39;00m\n\u001b[1;32m     78\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minput_format \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mwide\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m orient \u001b[38;5;129;01min\u001b[39;00m [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mh\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124my\u001b[39m\u001b[38;5;124m\"\u001b[39m]:\n",
      "File \u001b[0;32m~/miniconda3/envs/esm_gpu/lib/python3.10/site-packages/seaborn/_base.py:634\u001b[0m, in \u001b[0;36mVectorPlotter.__init__\u001b[0;34m(self, data, variables)\u001b[0m\n\u001b[1;32m    629\u001b[0m \u001b[38;5;66;03m# var_ordered is relevant only for categorical axis variables, and may\u001b[39;00m\n\u001b[1;32m    630\u001b[0m \u001b[38;5;66;03m# be better handled by an internal axis information object that tracks\u001b[39;00m\n\u001b[1;32m    631\u001b[0m \u001b[38;5;66;03m# such information and is set up by the scale_* methods. The analogous\u001b[39;00m\n\u001b[1;32m    632\u001b[0m \u001b[38;5;66;03m# information for numeric axes would be information about log scales.\u001b[39;00m\n\u001b[1;32m    633\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_var_ordered \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mx\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28;01mFalse\u001b[39;00m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124my\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28;01mFalse\u001b[39;00m}  \u001b[38;5;66;03m# alt., used DefaultDict\u001b[39;00m\n\u001b[0;32m--> 634\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43massign_variables\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvariables\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    636\u001b[0m \u001b[38;5;66;03m# TODO Lots of tests assume that these are called to initialize the\u001b[39;00m\n\u001b[1;32m    637\u001b[0m \u001b[38;5;66;03m# mappings to default values on class initialization. I'd prefer to\u001b[39;00m\n\u001b[1;32m    638\u001b[0m \u001b[38;5;66;03m# move away from that and only have a mapping when explicitly called.\u001b[39;00m\n\u001b[1;32m    639\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m var \u001b[38;5;129;01min\u001b[39;00m [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhue\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msize\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstyle\u001b[39m\u001b[38;5;124m\"\u001b[39m]:\n",
      "File \u001b[0;32m~/miniconda3/envs/esm_gpu/lib/python3.10/site-packages/seaborn/_base.py:679\u001b[0m, in \u001b[0;36mVectorPlotter.assign_variables\u001b[0;34m(self, data, variables)\u001b[0m\n\u001b[1;32m    674\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    675\u001b[0m     \u001b[38;5;66;03m# When dealing with long-form input, use the newer PlotData\u001b[39;00m\n\u001b[1;32m    676\u001b[0m     \u001b[38;5;66;03m# object (internal but introduced for the objects interface)\u001b[39;00m\n\u001b[1;32m    677\u001b[0m     \u001b[38;5;66;03m# to centralize / standardize data consumption logic.\u001b[39;00m\n\u001b[1;32m    678\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minput_format \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlong\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m--> 679\u001b[0m     plot_data \u001b[38;5;241m=\u001b[39m \u001b[43mPlotData\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvariables\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    680\u001b[0m     frame \u001b[38;5;241m=\u001b[39m plot_data\u001b[38;5;241m.\u001b[39mframe\n\u001b[1;32m    681\u001b[0m     names \u001b[38;5;241m=\u001b[39m plot_data\u001b[38;5;241m.\u001b[39mnames\n",
      "File \u001b[0;32m~/miniconda3/envs/esm_gpu/lib/python3.10/site-packages/seaborn/_core/data.py:58\u001b[0m, in \u001b[0;36mPlotData.__init__\u001b[0;34m(self, data, variables)\u001b[0m\n\u001b[1;32m     51\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m__init__\u001b[39m(\n\u001b[1;32m     52\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m     53\u001b[0m     data: DataSource,\n\u001b[1;32m     54\u001b[0m     variables: \u001b[38;5;28mdict\u001b[39m[\u001b[38;5;28mstr\u001b[39m, VariableSpec],\n\u001b[1;32m     55\u001b[0m ):\n\u001b[1;32m     57\u001b[0m     data \u001b[38;5;241m=\u001b[39m handle_data_source(data)\n\u001b[0;32m---> 58\u001b[0m     frame, names, ids \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_assign_variables\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvariables\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     60\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mframe \u001b[38;5;241m=\u001b[39m frame\n\u001b[1;32m     61\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnames \u001b[38;5;241m=\u001b[39m names\n",
      "File \u001b[0;32m~/miniconda3/envs/esm_gpu/lib/python3.10/site-packages/seaborn/_core/data.py:232\u001b[0m, in \u001b[0;36mPlotData._assign_variables\u001b[0;34m(self, data, variables)\u001b[0m\n\u001b[1;32m    230\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    231\u001b[0m         err \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn entry with this name does not appear in `data`.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m--> 232\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(err)\n\u001b[1;32m    234\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    235\u001b[0m \n\u001b[1;32m    236\u001b[0m     \u001b[38;5;66;03m# Otherwise, assume the value somehow represents data\u001b[39;00m\n\u001b[1;32m    237\u001b[0m \n\u001b[1;32m    238\u001b[0m     \u001b[38;5;66;03m# Ignore empty data structures\u001b[39;00m\n\u001b[1;32m    239\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(val, Sized) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(val) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "\u001b[0;31mValueError\u001b[0m: Could not interpret value `intr_scores` for `y`. An entry with this name does not appear in `data`."
     ]
    }
   ],
   "source": [
    "sns.boxplot(data=interaction_df,x=\"target_id_mod\",y=\"intr_scores\",hue=\"binder\",legend=True)\n",
    "plt.xticks(rotation=90)\n",
    "plt.grid()\n",
    "plt.show()\n",
    "\n",
    "#%%\n",
    "# import seaborn as sns\n",
    "# sns.boxplot(data=interaction_df,x=\"target_id\",y=\"af2_pae_interaction\",hue=\"binder\",legend=True)\n",
    "# plt.xticks(rotation=90)\n",
    "# plt.grid()\n",
    "# plt.show()\n",
    "\n",
    "\n",
    "# import seaborn as sns\n",
    "# sns.boxplot(data=interaction_df,x=\"target_id\",y=\"boltz_ipae\",hue=\"binder\",legend=True)\n",
    "# plt.xticks(rotation=90)\n",
    "# plt.grid()\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a67af889-6d9e-4a5e-8dda-2cb04ac0013a",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'interaction_probabilities' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[16], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m pred_binder \u001b[38;5;241m=\u001b[39m \u001b[43minteraction_probabilities\u001b[49m \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.5\u001b[39m\n\u001b[1;32m      2\u001b[0m true_label \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray(interaction_df[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbinder\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[1;32m      4\u001b[0m tp \u001b[38;5;241m=\u001b[39m ((pred_binder \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m) \u001b[38;5;241m&\u001b[39m (true_label \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m))\u001b[38;5;241m.\u001b[39msum()\u001b[38;5;241m.\u001b[39mitem()\n",
      "\u001b[0;31mNameError\u001b[0m: name 'interaction_probabilities' is not defined"
     ]
    }
   ],
   "source": [
    "pred_binder = interaction_probabilities >= 0.5\n",
    "true_label = np.array(interaction_df[\"binder\"])\n",
    "\n",
    "tp = ((pred_binder == 1) & (true_label == 1)).sum().item()\n",
    "tn = ((pred_binder == 0) & (true_label == 0)).sum().item()\n",
    "fp = ((pred_binder == 1) & (true_label == 0)).sum().item()\n",
    "fn = ((pred_binder == 0) & (true_label == 1)).sum().item()\n",
    "\n",
    "print(f\"TP:{tp}, TN:{tn}, FN:{fn}, FP:{fp}\")\n",
    "\n",
    "precision = tp / (tp + fp) if (tp + fp) > 0 else 0.0\n",
    "recall    = tp / (tp + fn) if (tp + fn) > 0 else 0.0\n",
    "correct = (true_label == pred_binder).sum()\n",
    "total   = len(true_label)\n",
    "accuracy = (correct / total).item()\n",
    "f1 = 2 * precision * recall / (precision + recall)\n",
    "\n",
    "print({\n",
    "    \"loss\": loss.item(),\n",
    "    \"accuracy\": accuracy,\n",
    "    \"precision\": precision,\n",
    "    \"recall\": recall,\n",
    "    \"f1\": f1\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9be274a4-ff30-40b1-b1ef-7ae9fae0670d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "\n",
    "cm = confusion_matrix(true_label, pred_binder)\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=cm)\n",
    "disp.plot()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e7698f7-73e0-4008-be5c-6e98258d2c1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "for target, subset_df in interaction_df.groupby(\"target_id_mod\"):\n",
    "    y_true = subset_df[\"binder\"].astype(int).values\n",
    "    y_scores = subset_df[\"CLIP_interaction_scores\"].values\n",
    "    # print(target)\n",
    "    # print(average_precision_score(y_true, y_scores))\n",
    "    # print((y_true == 1).sum() / len(y_true))\n",
    "    # print()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "esm_gpu",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
