{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7f838074-e829-40d5-9b15-949a9bfe9708",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tesla V100-SXM2-32GB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/work3/s232958/envs/esm_cuda/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: 'Could not load this library: /work3/s232958/envs/esm_cuda/lib/python3.10/site-packages/torchvision/image.so'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?\n",
      "  warn(\n",
      "/work3/s232958/envs/esm_cuda/lib/python3.10/site-packages/transformers/utils/generic.py:441: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.\n",
      "  _torch_pytree._register_pytree_node(\n"
     ]
    }
   ],
   "source": [
    "import uuid, sys, os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import ast\n",
    "import math\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn import metrics\n",
    "from scipy import stats\n",
    "from collections import Counter\n",
    "from sklearn.metrics import average_precision_score, precision_recall_curve, roc_auc_score, confusion_matrix, ConfusionMatrixDisplay, classification_report, roc_curve\n",
    "\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"3\"\n",
    "os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"expandable_segments:True\"\n",
    "import torch\n",
    "torch.cuda.set_device(0)  # 0 == \"first visible\" -> actually GPU 2 on the node\n",
    "print(torch.cuda.get_device_name(0))\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader, TensorDataset, WeightedRandomSampler\n",
    "import pytorch_lightning as pl\n",
    "from torch.optim import AdamW\n",
    "\n",
    "torch.manual_seed(0)\n",
    "\n",
    "from accelerate import Accelerator\n",
    "torch.cuda.empty_cache()\n",
    "import training_utils.partitioning_utils as pat_utils\n",
    "from tqdm import trange"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6583244-f400-4eff-955d-8b7a34a04038",
   "metadata": {},
   "source": [
    "## ESM C"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1f587290-f774-42ba-bc41-b24a0d3ac163",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "#Loading ESM2 embeddings and contacts: 100%|█████████████████████████████████████████| 494/494 [00:03<00:00, 135.96it/s]\n",
      "#Loading ESM2 embeddings and contacts: 100%|█████████████████████████████████████████| 122/122 [00:00<00:00, 153.25it/s]\n"
     ]
    }
   ],
   "source": [
    "Df_train = pd.read_csv(\"/work3/s232958/data/PPint_DB/PPint_train.csv\",index_col=0).reset_index(drop=True)\n",
    "Df_test = pd.read_csv(\"/work3/s232958/data/PPint_DB/PPint_test.csv\",index_col=0).reset_index(drop=True)\n",
    "\n",
    "class CLIP_PPint_dataclass(Dataset):\n",
    "    def __init__(\n",
    "        self,\n",
    "        dframe,\n",
    "        path,\n",
    "        embedding_dim=1280,\n",
    "        embedding_pad_value=-5000.0,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.dframe = dframe.copy()\n",
    "        self.embedding_dim = int(embedding_dim)\n",
    "        self.emb_pad = float(embedding_pad_value)\n",
    "\n",
    "        # lengths\n",
    "        self.max_blen = self.dframe[\"seq_binder_len\"].max()\n",
    "        self.max_tlen = self.dframe[\"seq_target_len\"].max()\n",
    "\n",
    "        # paths\n",
    "        self.encoding_path  = path\n",
    "\n",
    "        # index & storage\n",
    "        self.dframe.set_index(\"target_binder_id\", inplace=True)\n",
    "        self.accessions = self.dframe.index.astype(str).tolist()\n",
    "        self.name_to_row = {name: i for i, name in enumerate(self.accessions)}\n",
    "        self.samples = []\n",
    "\n",
    "        for accession in tqdm(self.accessions, total=len(self.accessions), desc=\"#Loading ESM2 embeddings and contacts\"):\n",
    "            parts = accession.split(\"_\") # e.g. accession 7S8T_5_F_7S8T_5_G\n",
    "            tgt_id = parts[0]+\"_\"+parts[2]\n",
    "            bnd_id = parts[-3]+\"_\"+parts[-1]\n",
    "\n",
    "            ### --- embeddings (pad to fixed lengths) --- ###\n",
    "            \n",
    "            # laod embeddings\n",
    "            t_emb = np.load(os.path.join(self.encoding_path, f\"{tgt_id}.npy\")).squeeze(0)     # [Lt, D]\n",
    "            b_emb = np.load(os.path.join(self.encoding_path, f\"{bnd_id}.npy\")).squeeze(0)     # [Lb, D]\n",
    "\n",
    "            # quich check whether embedding dimmension is as it suppose to be\n",
    "            if t_emb.shape[1] != self.embedding_dim or b_emb.shape[1] != self.embedding_dim:\n",
    "                raise ValueError(\"Embedding dim mismatch with 'embedding_dim'.\")\n",
    "\n",
    "            # add -5000 to all the padded target rows\n",
    "            if t_emb.shape[0] < self.max_tlen:\n",
    "                t_emb = np.concatenate([t_emb, np.full((self.max_tlen - t_emb.shape[0], t_emb.shape[1]), self.emb_pad, dtype=t_emb.dtype)], axis=0)\n",
    "            else:\n",
    "                t_emb = t_emb[: self.max_tlen] # no padding was used\n",
    "\n",
    "            # add -5000 to all the padded binder rows\n",
    "            if b_emb.shape[0] < self.max_blen:\n",
    "                b_emb = np.concatenate([b_emb, np.full((self.max_blen - b_emb.shape[0], b_emb.shape[1]), self.emb_pad, dtype=b_emb.dtype)], axis=0)\n",
    "            else:\n",
    "                b_emb = b_emb[: self.max_blen] # no padding was used\n",
    "\n",
    "            self.samples.append((b_emb, t_emb))\n",
    "\n",
    "    # ---- Dataset API ----\n",
    "    def __len__(self):\n",
    "        return len(self.samples)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        b_arr, t_arr = self.samples[idx]\n",
    "        binder_emb, target_emb = torch.from_numpy(b_arr).float(), torch.from_numpy(t_arr).float()\n",
    "        label = torch.tensor(1, dtype=torch.float32)  # single scalar labe\n",
    "        return binder_emb, target_emb, label\n",
    "\n",
    "    def _get_by_name(self, name):\n",
    "        # Single item -> return exactly what __getitem__ returns\n",
    "        if isinstance(name, str):\n",
    "            return self.__getitem__(self.name_to_row[name])\n",
    "        \n",
    "        # Multiple items -> fetch all\n",
    "        out = [self.__getitem__(self.name_to_row[n]) for n in list(name)]\n",
    "        b_list, t_list, lbl_list = zip(*out)\n",
    "    \n",
    "        # Stack embeddings\n",
    "        b  = torch.stack([torch.as_tensor(x) for x in b_list],  dim=0)  # [B, ...]\n",
    "        t  = torch.stack([torch.as_tensor(x) for x in t_list],  dim=0)  # [B, ...]\n",
    "    \n",
    "        # Stack labels\n",
    "        labels = torch.stack(lbl_list)  # [B]\n",
    "    \n",
    "        return b, t, labels\n",
    "\n",
    "emb_path = \"/work3/s232958/data/PPint_DB/embeddings_esmC\"\n",
    "\n",
    "testing_Dataset = CLIP_PPint_dataclass(\n",
    "    Df_test,\n",
    "    path=emb_path,\n",
    "    embedding_dim=1152\n",
    ")\n",
    "\n",
    "Df_test_non_dimer = Df_test[Df_test.dimer == False]\n",
    "\n",
    "non_dimers_Dataset = CLIP_PPint_dataclass(\n",
    "    Df_test_non_dimer,\n",
    "    path=emb_path,\n",
    "    embedding_dim=1152\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cf589e0e-f061-455b-92db-eaa97d91228f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>seq_binder</th>\n",
       "      <th>seq_target</th>\n",
       "      <th>target_id</th>\n",
       "      <th>binder_id</th>\n",
       "      <th>binder_label</th>\n",
       "      <th>seq_target_len</th>\n",
       "      <th>seq_binder_len</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>DIVEEAHKLLSRAMSEAMENDDPDKLRRANELYFKLEEALKNNDPK...</td>\n",
       "      <td>RSPHRPILQAGLPANASTVVGGDVEFVCKVYSDAQPHIQWIKHVPY...</td>\n",
       "      <td>FGFR2</td>\n",
       "      <td>FGFR2_124</td>\n",
       "      <td>True</td>\n",
       "      <td>101</td>\n",
       "      <td>62</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>SEELVEKVVEEILNSDLSNDQKILETHDRLMELHDQGKISKEEYYK...</td>\n",
       "      <td>LEEKKVCQGTSNKLTQLGTFEDHFLSLQRMFNNCEVVLGNLEITYV...</td>\n",
       "      <td>EGFR_2</td>\n",
       "      <td>EGFR_2_149</td>\n",
       "      <td>False</td>\n",
       "      <td>621</td>\n",
       "      <td>58</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>TINRVFHLHIQGDTEEARKAHEELVEEVRRWAEELAKRLNLTVRVT...</td>\n",
       "      <td>RSPHRPILQAGLPANASTVVGGDVEFVCKVYSDAQPHIQWIKHVPY...</td>\n",
       "      <td>FGFR2</td>\n",
       "      <td>FGFR2_339</td>\n",
       "      <td>False</td>\n",
       "      <td>101</td>\n",
       "      <td>65</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>DDLRKVERIASELAFFAAEQNDTKVAFTALELIHQLIRAIFHNDEE...</td>\n",
       "      <td>RSPHRPILQAGLPANASTVVGGDVEFVCKVYSDAQPHIQWIKHVPY...</td>\n",
       "      <td>FGFR2</td>\n",
       "      <td>FGFR2_1234</td>\n",
       "      <td>False</td>\n",
       "      <td>101</td>\n",
       "      <td>64</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>DEEVEELEELLEKAEDPRERAKLLRELAKLIRRDPRLRELATEVVA...</td>\n",
       "      <td>ELCDDDPPEIPHATFKAMAYKEGTMLNCECKRGFRRIKSGSLYMLC...</td>\n",
       "      <td>IL2Ra</td>\n",
       "      <td>IL2Ra_48</td>\n",
       "      <td>False</td>\n",
       "      <td>165</td>\n",
       "      <td>65</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3527</th>\n",
       "      <td>SEDELRELVKEIRKVAEKQGDKELRTLWIEAYDLLASLWYGAADEL...</td>\n",
       "      <td>TNLCPFGEVFNATRFASVYAWNRKRISNCVADYSVLYNSASFSTFK...</td>\n",
       "      <td>SARS_CoV2_RBD</td>\n",
       "      <td>SARS_CoV2_RBD_25</td>\n",
       "      <td>False</td>\n",
       "      <td>195</td>\n",
       "      <td>63</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3528</th>\n",
       "      <td>TEEEILKMLVELTAHMAGVPDVKVEIHNGTLRVTVNGDTREARSVL...</td>\n",
       "      <td>RSPHRPILQAGLPANASTVVGGDVEFVCKVYSDAQPHIQWIKHVPY...</td>\n",
       "      <td>FGFR2</td>\n",
       "      <td>FGFR2_2027</td>\n",
       "      <td>False</td>\n",
       "      <td>101</td>\n",
       "      <td>65</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3529</th>\n",
       "      <td>VEELKEARKLVEEVLRKKGDQIAEIWKDILEELEQRYQEGKLDPEE...</td>\n",
       "      <td>DYSFSCYSQLEVNGSQHSLTCAFEDPDVNTTNLEFEICGALVEVKC...</td>\n",
       "      <td>IL7Ra</td>\n",
       "      <td>IL7Ra_90</td>\n",
       "      <td>False</td>\n",
       "      <td>193</td>\n",
       "      <td>63</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3530</th>\n",
       "      <td>DAEEEIREIVEKLNDPLLREILRLLELAKEKGDPRLEAELYLAFEK...</td>\n",
       "      <td>RSPHRPILQAGLPANASTVVGGDVEFVCKVYSDAQPHIQWIKHVPY...</td>\n",
       "      <td>FGFR2</td>\n",
       "      <td>FGFR2_1605</td>\n",
       "      <td>False</td>\n",
       "      <td>101</td>\n",
       "      <td>65</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3531</th>\n",
       "      <td>DPEEIKQRAEEIKKEFQKKGVSPEIQFAIEQVIKYALEVGLSPKDI...</td>\n",
       "      <td>RSPHRPILQAGLPANASTVVGGDVEFVCKVYSDAQPHIQWIKHVPY...</td>\n",
       "      <td>FGFR2</td>\n",
       "      <td>FGFR2_1687</td>\n",
       "      <td>True</td>\n",
       "      <td>101</td>\n",
       "      <td>62</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3532 rows × 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             seq_binder  \\\n",
       "0     DIVEEAHKLLSRAMSEAMENDDPDKLRRANELYFKLEEALKNNDPK...   \n",
       "1     SEELVEKVVEEILNSDLSNDQKILETHDRLMELHDQGKISKEEYYK...   \n",
       "2     TINRVFHLHIQGDTEEARKAHEELVEEVRRWAEELAKRLNLTVRVT...   \n",
       "3     DDLRKVERIASELAFFAAEQNDTKVAFTALELIHQLIRAIFHNDEE...   \n",
       "4     DEEVEELEELLEKAEDPRERAKLLRELAKLIRRDPRLRELATEVVA...   \n",
       "...                                                 ...   \n",
       "3527  SEDELRELVKEIRKVAEKQGDKELRTLWIEAYDLLASLWYGAADEL...   \n",
       "3528  TEEEILKMLVELTAHMAGVPDVKVEIHNGTLRVTVNGDTREARSVL...   \n",
       "3529  VEELKEARKLVEEVLRKKGDQIAEIWKDILEELEQRYQEGKLDPEE...   \n",
       "3530  DAEEEIREIVEKLNDPLLREILRLLELAKEKGDPRLEAELYLAFEK...   \n",
       "3531  DPEEIKQRAEEIKKEFQKKGVSPEIQFAIEQVIKYALEVGLSPKDI...   \n",
       "\n",
       "                                             seq_target      target_id  \\\n",
       "0     RSPHRPILQAGLPANASTVVGGDVEFVCKVYSDAQPHIQWIKHVPY...          FGFR2   \n",
       "1     LEEKKVCQGTSNKLTQLGTFEDHFLSLQRMFNNCEVVLGNLEITYV...         EGFR_2   \n",
       "2     RSPHRPILQAGLPANASTVVGGDVEFVCKVYSDAQPHIQWIKHVPY...          FGFR2   \n",
       "3     RSPHRPILQAGLPANASTVVGGDVEFVCKVYSDAQPHIQWIKHVPY...          FGFR2   \n",
       "4     ELCDDDPPEIPHATFKAMAYKEGTMLNCECKRGFRRIKSGSLYMLC...          IL2Ra   \n",
       "...                                                 ...            ...   \n",
       "3527  TNLCPFGEVFNATRFASVYAWNRKRISNCVADYSVLYNSASFSTFK...  SARS_CoV2_RBD   \n",
       "3528  RSPHRPILQAGLPANASTVVGGDVEFVCKVYSDAQPHIQWIKHVPY...          FGFR2   \n",
       "3529  DYSFSCYSQLEVNGSQHSLTCAFEDPDVNTTNLEFEICGALVEVKC...          IL7Ra   \n",
       "3530  RSPHRPILQAGLPANASTVVGGDVEFVCKVYSDAQPHIQWIKHVPY...          FGFR2   \n",
       "3531  RSPHRPILQAGLPANASTVVGGDVEFVCKVYSDAQPHIQWIKHVPY...          FGFR2   \n",
       "\n",
       "             binder_id  binder_label  seq_target_len  seq_binder_len  \n",
       "0            FGFR2_124          True             101              62  \n",
       "1           EGFR_2_149         False             621              58  \n",
       "2            FGFR2_339         False             101              65  \n",
       "3           FGFR2_1234         False             101              64  \n",
       "4             IL2Ra_48         False             165              65  \n",
       "...                ...           ...             ...             ...  \n",
       "3527  SARS_CoV2_RBD_25         False             195              63  \n",
       "3528        FGFR2_2027         False             101              65  \n",
       "3529          IL7Ra_90         False             193              63  \n",
       "3530        FGFR2_1605         False             101              65  \n",
       "3531        FGFR2_1687          True             101              62  \n",
       "\n",
       "[3532 rows x 7 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "interaction_df = pd.read_csv(\"/work3/s232958/data/meta_analysis/interaction_df_metaanal.csv\")[[\"A_seq\", \"B_seq\", \"target_id_mod\", \"target_binder_ID\", \"binder\"]].rename(columns = {\n",
    "    \"A_seq\" : \"seq_binder\",\n",
    "    \"B_seq\" : \"seq_target\",\n",
    "    \"target_binder_ID\" : \"binder_id\",\n",
    "    \"target_id_mod\" : \"target_id\",\n",
    "    \"binder\" : \"binder_label\"\n",
    "})\n",
    "interaction_df[\"seq_target_len\"] = [len(seq) for seq in interaction_df[\"seq_target\"].tolist()]\n",
    "interaction_df[\"seq_binder_len\"] = [len(seq) for seq in interaction_df[\"seq_binder\"].tolist()]\n",
    "\n",
    "# Targets df\n",
    "target_df = interaction_df[[\"target_id\",\"seq_target\"]].rename(columns={\"seq_target\":\"sequence\", \"target_id\" : \"ID\"})\n",
    "target_df[\"seq_len\"] = target_df[\"sequence\"].apply(len)\n",
    "target_df = target_df.drop_duplicates(subset=[\"ID\",\"sequence\"])\n",
    "target_df = target_df.set_index(\"ID\")\n",
    "\n",
    "# Binders df\n",
    "binder_df = interaction_df[[\"binder_id\",\"seq_binder\"]].rename(columns={\"seq_binder\":\"sequence\", \"binder_id\" : \"ID\"})\n",
    "binder_df[\"seq_len\"] = binder_df[\"sequence\"].apply(len)\n",
    "binder_df = binder_df.set_index(\"ID\")\n",
    "\n",
    "# target_df\n",
    "\n",
    "# Interaction Dict\n",
    "interaction_Dict = dict(enumerate(zip(interaction_df[\"target_id\"], interaction_df[\"binder_id\"]), start=1))\n",
    "interaction_df_shuffled = interaction_df.sample(frac=1, random_state=0).reset_index(drop=True)\n",
    "interaction_df_shuffled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b1f0bee0-e259-44f9-ba13-aff0ec3c7686",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "#Loading ESM2 embeddings: 100%|████████████████████████████████████████████████████| 3532/3532 [00:21<00:00, 163.99it/s]\n"
     ]
    }
   ],
   "source": [
    "class CLIP_Meta_class(Dataset):\n",
    "    def __init__(\n",
    "        self,\n",
    "        dframe,\n",
    "        paths,\n",
    "        embedding_dim=1152,\n",
    "        embedding_pad_value=-5000.0\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.dframe = dframe.copy()\n",
    "        self.embedding_dim = int(embedding_dim)\n",
    "        self.emb_pad = float(embedding_pad_value)\n",
    "        self.max_blen = self.dframe[\"seq_binder_len\"].max()\n",
    "        self.max_tlen = self.dframe[\"seq_target_len\"].max()\n",
    "\n",
    "        # paths\n",
    "        self.encoding_bpath, self.encoding_tpath = paths\n",
    "\n",
    "        # index & storage\n",
    "        self.dframe.set_index(\"binder_id\", inplace=True)\n",
    "        self.accessions = self.dframe.index.astype(str).tolist()\n",
    "        self.name_to_row = {name: i for i, name in enumerate(self.accessions)}\n",
    "        self.samples = []\n",
    "\n",
    "        for accession in tqdm(self.accessions, total=len(self.accessions), desc=\"#Loading ESM2 embeddings\"):\n",
    "            lbl = torch.tensor(int(self.dframe.loc[accession, \"binder_label\"]))\n",
    "            parts = accession.split(\"_\") # e.g. accession 7S8T_5_F_7S8T_5_G\n",
    "            tgt_id = \"_\".join(parts[:-1])\n",
    "            bnd_id = accession\n",
    "\n",
    "            ### --- embeddings (pad to fixed lengths) --- ###\n",
    "            \n",
    "            # laod embeddings\n",
    "            t_emb = np.load(os.path.join(self.encoding_tpath, f\"{tgt_id}.npy\")).squeeze(0)     # [Lt, D]\n",
    "            b_emb = np.load(os.path.join(self.encoding_bpath, f\"{bnd_id}.npy\")).squeeze(0)      # [Lb, D]\n",
    "\n",
    "            # quich check whether embedding dimmension is as it suppose to be\n",
    "            if t_emb.shape[1] != self.embedding_dim or b_emb.shape[1] != self.embedding_dim:\n",
    "                raise ValueError(\"Embedding dim mismatch with 'embedding_dim'.\")\n",
    "\n",
    "            # add -5000 to all the padded target rows\n",
    "            if t_emb.shape[0] < self.max_tlen:\n",
    "                t_emb = np.concatenate([t_emb, np.full((self.max_tlen - t_emb.shape[0], t_emb.shape[1]), self.emb_pad, dtype=t_emb.dtype)], axis=0)\n",
    "            else:\n",
    "                t_emb = t_emb[: self.max_tlen] # no padding was used\n",
    "\n",
    "            # add -5000 to all the padded binder rows\n",
    "            if b_emb.shape[0] < self.max_blen:\n",
    "                b_emb = np.concatenate([b_emb, np.full((self.max_blen - b_emb.shape[0], b_emb.shape[1]), self.emb_pad, dtype=b_emb.dtype)], axis=0)\n",
    "            else:\n",
    "                b_emb = b_emb[: self.max_blen] # no padding was used\n",
    "\n",
    "            self.samples.append((b_emb, t_emb, lbl))\n",
    "\n",
    "    # ---- Dataset API ----\n",
    "    def __len__(self):\n",
    "        return len(self.samples)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        b_arr, t_arr, lbls = self.samples[idx]\n",
    "        binder_emb, target_emb = torch.from_numpy(b_arr).float(), torch.from_numpy(t_arr).float()\n",
    "        return binder_emb, target_emb, lbls\n",
    "\n",
    "    def _get_by_name(self, name):\n",
    "        # Single item -> return exactly what __getitem__ returns\n",
    "        if isinstance(name, str):\n",
    "            return self.__getitem__(self.name_to_row[name])\n",
    "        \n",
    "        # Multiple items -> fetch all\n",
    "        out = [self.__getitem__(self.name_to_row[n]) for n in list(name)]\n",
    "        b_list, t_list, lbl_list = zip(*out)\n",
    "    \n",
    "        # Stack embeddings\n",
    "        b  = torch.stack([torch.as_tensor(x) for x in b_list],  dim=0)  # [B, ...]\n",
    "        t  = torch.stack([torch.as_tensor(x) for x in t_list],  dim=0)  # [B, ...]\n",
    "    \n",
    "        # Stack labels\n",
    "        labels = torch.stack(lbl_list)  # [B]\n",
    "    \n",
    "        return b, t, labels\n",
    "\n",
    "bemb_path = \"/work3/s232958/data/meta_analysis/embeddings_esmC_binders\"\n",
    "temb_path = \"/work3/s232958/data/meta_analysis/embeddings_esmC_targets\"\n",
    "\n",
    "validation_Dataset = CLIP_Meta_class(\n",
    "    # interaction_df_shuffled[:len(Df_test)],\n",
    "    interaction_df_shuffled,\n",
    "    paths=[bemb_path, temb_path],\n",
    "    embedding_dim=1152\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "509c9527-c9e9-4980-b273-70d66ae5464a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1, 0, 0, 0, 0])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accessions_Meta = list(interaction_df_shuffled.binder_id)\n",
    "emb_b, emb_t, labels = validation_Dataset._get_by_name(accessions_Meta[:5])\n",
    "labels"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fadb2503-da3c-413a-b655-9d2d8e4ec911",
   "metadata": {},
   "source": [
    "### Loading Pre-trained Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6ff6f708-e974-4047-8b31-bb4baf32d0b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_dimension = 1152"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0365067c-759c-4cdd-bfe1-5f35b2033fee",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_key_padding_mask(embeddings, padding_value=-5000, offset=10):\n",
    "    return (embeddings < (padding_value + offset)).all(dim=-1)\n",
    "\n",
    "def create_mean_of_non_masked(embeddings, padding_mask):\n",
    "    # Use masked select and mean to compute the mean of non-masked elements\n",
    "    # embeddings should be of shape (batch_size, seq_len, features)\n",
    "    seq_embeddings = []\n",
    "    for i in range(embeddings.shape[0]): # looping over all batch elements\n",
    "        non_masked_embeddings = embeddings[i][~padding_mask[i]] # shape [num_real_tokens, features]\n",
    "        if len(non_masked_embeddings) == 0:\n",
    "            print(\"You are masking all positions when creating sequence representation\")\n",
    "            sys.exit(1)\n",
    "        mean_embedding = non_masked_embeddings.mean(dim=0) # sequence is represented by the single vecotr [1152] [features]\n",
    "        seq_embeddings.append(mean_embedding)\n",
    "    return torch.stack(seq_embeddings)\n",
    "\n",
    "class MiniCLIP_w_transformer_crossattn(pl.LightningModule):\n",
    "\n",
    "    def __init__(self, padding_value = -5000, embed_dimension=embedding_dimension, num_recycles=2):\n",
    "\n",
    "        super().__init__()\n",
    "        self.num_recycles = num_recycles # how many times you iteratively refine embeddings with self- and cross-attention (ALPHA-Fold-style recycling).\n",
    "        self.padding_value = padding_value\n",
    "        self.embed_dimension = embed_dimension\n",
    "\n",
    "        self.logit_scale = nn.Parameter(torch.tensor(math.log(1/0.07)))  # ~CLIP init\n",
    "\n",
    "        self.transformerencoder =  nn.TransformerEncoderLayer(\n",
    "            d_model=self.embed_dimension,\n",
    "            nhead=8,\n",
    "            dropout=0.1,\n",
    "            batch_first=True,\n",
    "            dim_feedforward=self.embed_dimension\n",
    "            )\n",
    " \n",
    "        self.norm = nn.LayerNorm(self.embed_dimension)  # For residual additions\n",
    "\n",
    "        self.cross_attn = nn.MultiheadAttention(\n",
    "            embed_dim=self.embed_dimension,\n",
    "            num_heads=8,\n",
    "            dropout=0.1,\n",
    "            batch_first=True\n",
    "        )\n",
    "\n",
    "        self.prot_embedder = nn.Sequential(\n",
    "            nn.Linear(self.embed_dimension, 640),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(640, 320),\n",
    "        )\n",
    "        \n",
    "    def forward(self, pep_input, prot_input, label=None, pep_int_mask=None, prot_int_mask=None, int_prob=None, mem_save=True): # , pep_tokens, prot_tokens\n",
    "\n",
    "        pep_mask = create_key_padding_mask(embeddings=pep_input, padding_value=self.padding_value)\n",
    "        prot_mask = create_key_padding_mask(embeddings=prot_input, padding_value=self.padding_value)\n",
    " \n",
    "        # Initialize residual states\n",
    "        pep_emb = pep_input.clone()\n",
    "        prot_emb = prot_input.clone()\n",
    " \n",
    "        for _ in range(self.num_recycles):\n",
    "\n",
    "            # Transformer encoding with residual\n",
    "            pep_trans = self.transformerencoder(self.norm(pep_emb), src_key_padding_mask=pep_mask)\n",
    "            prot_trans = self.transformerencoder(self.norm(prot_emb), src_key_padding_mask=prot_mask)\n",
    "\n",
    "            # Cross-attention with residual\n",
    "            pep_cross, _ = self.cross_attn(query=self.norm(pep_trans), key=self.norm(prot_trans), value=self.norm(prot_trans), key_padding_mask=prot_mask)\n",
    "            prot_cross, _ = self.cross_attn(query=self.norm(prot_trans), key=self.norm(pep_trans), value=self.norm(pep_trans), key_padding_mask=pep_mask)\n",
    "            \n",
    "            # Additive update with residual connection\n",
    "            pep_emb = pep_emb + pep_trans  \n",
    "            prot_emb = prot_emb + prot_trans\n",
    "\n",
    "        pep_seq_coding = create_mean_of_non_masked(pep_emb, pep_mask)\n",
    "        prot_seq_coding = create_mean_of_non_masked(prot_emb, prot_mask)\n",
    "        \n",
    "        # Use self-attention outputs for embeddings\n",
    "        pep_seq_coding = F.normalize(self.prot_embedder(pep_seq_coding), dim=-1)\n",
    "        prot_seq_coding = F.normalize(self.prot_embedder(prot_seq_coding), dim=-1)\n",
    " \n",
    "        if mem_save:\n",
    "            torch.cuda.empty_cache()\n",
    "        \n",
    "        scale = torch.exp(self.logit_scale).clamp(max=100.0)\n",
    "        logits = scale * (pep_seq_coding * prot_seq_coding).sum(dim=-1)\n",
    "        \n",
    "        return logits\n",
    "\n",
    "    def training_step(self, batch, device):\n",
    "        embedding_pep, embedding_prot, labels = batch\n",
    "        embedding_pep, embedding_prot = embedding_pep.to(device), embedding_prot.to(device)\n",
    "        \n",
    "        positive_logits = self.forward(embedding_pep, embedding_prot)\n",
    "        \n",
    "        # Negative indexes\n",
    "        rows, cols = torch.triu_indices(embedding_prot.size(0), embedding_prot.size(0), offset=1)         \n",
    "        \n",
    "        negative_logits = self(embedding_pep[rows,:,:], \n",
    "                          embedding_prot[cols,:,:], \n",
    "                          int_prob=0.0)\n",
    "\n",
    "        # loss of predicting partner using peptide\n",
    "        positive_loss = F.binary_cross_entropy_with_logits(positive_logits, torch.ones_like(positive_logits).to(device))\n",
    " \n",
    "        # loss of predicting peptide using partner\n",
    "        negative_loss =  F.binary_cross_entropy_with_logits(negative_logits, torch.zeros_like(negative_logits).to(device))\n",
    "        \n",
    "        loss = (positive_loss + negative_loss) / 2\n",
    " \n",
    "        # del partner_prediction_loss, peptide_prediction_loss, embedding_pep, embedding_prot\n",
    "        torch.cuda.empty_cache()\n",
    "        return loss\n",
    "\n",
    "    def validation_step_PPint(self, batch, device):\n",
    "        # Predict on random batches of training batch size\n",
    "        embedding_pep, embedding_prot, labels = batch\n",
    "        embedding_pep, embedding_prot = embedding_pep.to(device), embedding_prot.to(device)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "\n",
    "            positive_logits = self(embedding_pep, embedding_prot)\n",
    "            \n",
    "            # loss of predicting partner using peptide\n",
    "            positive_loss = F.binary_cross_entropy_with_logits(positive_logits, torch.ones_like(positive_logits).to(device))\n",
    "            \n",
    "            # Negaive indexes\n",
    "            rows, cols = torch.triu_indices(embedding_prot.size(0), embedding_prot.size(0), offset=1)\n",
    "            \n",
    "            negative_logits = self(embedding_pep[rows,:,:], embedding_prot[cols,:,:], int_prob=0.0)\n",
    "    \n",
    "            negative_loss =  F.binary_cross_entropy_with_logits(negative_logits, torch.zeros_like(negative_logits).to(device))\n",
    "\n",
    "            loss = (positive_loss + negative_loss) / 2\n",
    "           \n",
    "            logit_matrix = torch.zeros((embedding_pep.size(0),embedding_pep.size(0)),device=self.device)\n",
    "            logit_matrix[rows, cols] = negative_logits\n",
    "            logit_matrix[cols, rows] = negative_logits\n",
    "            \n",
    "            # Fill diagonal with positive scores\n",
    "            diag_indices = torch.arange(embedding_pep.size(0), device=self.device)\n",
    "            logit_matrix[diag_indices, diag_indices] = positive_logits.squeeze()\n",
    "\n",
    "            labels = torch.arange(embedding_prot.size(0)).to(self.device)\n",
    "            peptide_predictions = logit_matrix.argmax(dim=0)\n",
    "            peptide_ranks = logit_matrix.argsort(dim=0).diag() + 1\n",
    "            peptide_mrr = (peptide_ranks).float().pow(-1).mean()\n",
    "            \n",
    "            # partner_accuracy = partner_predictions.eq(labels).float().mean()\n",
    "            peptide_accuracy = peptide_predictions.eq(labels).float().mean()\n",
    "    \n",
    "            k = 3\n",
    "            peptide_topk_accuracy = torch.any((logit_matrix.topk(k, dim=0).indices - labels.reshape(1, -1)) == 0, dim=0).sum() / logit_matrix.shape[0]\n",
    "    \n",
    "            del logit_matrix,positive_logits,negative_logits,embedding_pep,embedding_prot\n",
    "\n",
    "            return loss, peptide_accuracy, peptide_topk_accuracy\n",
    "    \n",
    "    def validation_step_MetaDataset(self, batch, device):\n",
    "        embedding_binder, embedding_target, labels = batch\n",
    "        embedding_binder = embedding_binder.to(device)\n",
    "        embedding_target = embedding_target.to(device)\n",
    "        labels = labels.to(device).float()\n",
    "    \n",
    "        with torch.no_grad():\n",
    "            logits = self.forward(embedding_binder, embedding_target)\n",
    "            logits = logits.float()\n",
    "            loss = F.binary_cross_entropy_with_logits(logits.view(-1), labels.view(-1))\n",
    "            return logits, loss\n",
    "\n",
    "    def calculate_logit_matrix(self,embedding_pep,embedding_prot):\n",
    "        rows, cols = torch.triu_indices(embedding_pep.size(0), embedding_pep.size(0), offset=1)\n",
    "        \n",
    "        positive_logits = self(embedding_pep, embedding_prot)\n",
    "        negative_logits = self(embedding_pep[rows,:,:], embedding_prot[cols,:,:], int_prob=0.0)\n",
    "        \n",
    "        logit_matrix = torch.zeros((embedding_pep.size(0),embedding_pep.size(0)),device=self.device)\n",
    "        logit_matrix[rows, cols] = negative_logits\n",
    "        logit_matrix[cols, rows] = negative_logits\n",
    "        \n",
    "        diag_indices = torch.arange(embedding_pep.size(0), device=self.device)\n",
    "        logit_matrix[diag_indices, diag_indices] = positive_logits.squeeze()\n",
    "        \n",
    "        return logit_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "97b2045e-38fb-42d3-8d19-ce1af03c1181",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['4333f167-963b-421b-aabe-a9fd5889311d_checkpoint_epoch_6.pth']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.listdir(\"/work3/s232958/data/trained/original_architecture/4333f167-963b-421b-aabe-a9fd5889311d/4333f167-963b-421b-aabe-a9fd5889311d_checkpoint_6\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "24222398-c484-4e4b-9256-987a529c19ac",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MiniCLIP_w_transformer_crossattn(\n",
       "  (transformerencoder): TransformerEncoderLayer(\n",
       "    (self_attn): MultiheadAttention(\n",
       "      (out_proj): NonDynamicallyQuantizableLinear(in_features=1152, out_features=1152, bias=True)\n",
       "    )\n",
       "    (linear1): Linear(in_features=1152, out_features=1152, bias=True)\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "    (linear2): Linear(in_features=1152, out_features=1152, bias=True)\n",
       "    (norm1): LayerNorm((1152,), eps=1e-05, elementwise_affine=True)\n",
       "    (norm2): LayerNorm((1152,), eps=1e-05, elementwise_affine=True)\n",
       "    (dropout1): Dropout(p=0.1, inplace=False)\n",
       "    (dropout2): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (norm): LayerNorm((1152,), eps=1e-05, elementwise_affine=True)\n",
       "  (cross_attn): MultiheadAttention(\n",
       "    (out_proj): NonDynamicallyQuantizableLinear(in_features=1152, out_features=1152, bias=True)\n",
       "  )\n",
       "  (prot_embedder): Sequential(\n",
       "    (0): Linear(in_features=1152, out_features=640, bias=True)\n",
       "    (1): ReLU()\n",
       "    (2): Linear(in_features=640, out_features=320, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = MiniCLIP_w_transformer_crossattn(embed_dimension=1152, num_recycles=2).to(\"cuda\")\n",
    "path = \"/work3/s232958/data/trained/original_architecture/4333f167-963b-421b-aabe-a9fd5889311d/4333f167-963b-421b-aabe-a9fd5889311d_checkpoint_6/4333f167-963b-421b-aabe-a9fd5889311d_checkpoint_epoch_6.pth\"\n",
    "checkpoint = torch.load(path, weights_only=False, map_location=torch.device('cpu'))\n",
    "# print(list(checkpoint[\"model_state_dict\"]))\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "model.load_state_dict(checkpoint['model_state_dict'])\n",
    "model.to(device)\n",
    "model.eval()  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9ed1454e-d184-48ec-b66f-a27e9c03daad",
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_varlen(batch):\n",
    "    b_emb = torch.stack([x[0] for x in batch], dim=0)\n",
    "    t_emb = torch.stack([x[1] for x in batch], dim=0)\n",
    "    lbls = torch.tensor([x[2].float() for x in batch])\n",
    "    return b_emb, t_emb, lbls\n",
    "\n",
    "test_dataloader = DataLoader(testing_Dataset, batch_size=10, collate_fn=collate_varlen)\n",
    "non_dimers_dataloader = DataLoader(non_dimers_Dataset, batch_size=10, collate_fn=collate_varlen)\n",
    "validation_dataloader = DataLoader(validation_Dataset, batch_size=10, shuffle=False, drop_last = False, collate_fn=collate_varlen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ec237c15-9668-48ff-8f95-3c474601b9ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "#Iterating through batched data: 50it [00:18,  2.64it/s]                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Positives: (494,)\n",
      "Negatives: (2211,)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "interaction_scores_pos = []\n",
    "interaction_scores_neg = []    \n",
    "\n",
    "for batch in tqdm(test_dataloader, total=round(len(Df_test)/10), desc=\"#Iterating through batched data\"):\n",
    "    b_emb, t_emb, lbls = batch\n",
    "    embedding_pep = b_emb.to(\"cuda\")\n",
    "    embedding_prot = t_emb.to(\"cuda\")\n",
    "\n",
    "    with torch.no_grad():\n",
    "\n",
    "        rows, cols = torch.triu_indices(embedding_pep.size(0), embedding_pep.size(0), offset=1)\n",
    "        positive_logits = model(embedding_pep, embedding_prot)\n",
    "        negative_logits = model(embedding_pep[rows,:,:], embedding_prot[cols,:,:], int_prob=0.0)\n",
    "        \n",
    "        logit_matrix = torch.zeros((embedding_pep.size(0),embedding_pep.size(0)),device=\"cuda\")\n",
    "        logit_matrix[rows, cols] = negative_logits\n",
    "        logit_matrix[cols, rows] = negative_logits\n",
    "        \n",
    "        diag_indices = torch.arange(embedding_pep.size(0), device=\"cuda\")\n",
    "        logit_matrix[diag_indices, diag_indices] = positive_logits.squeeze()\n",
    "        \n",
    "        # print(logit_matrix)\n",
    "        interaction_scores_pos.append(positive_logits)\n",
    "        interaction_scores_neg.append(negative_logits)\n",
    "\n",
    "# Convert list of tensors to single 1D tensors\n",
    "pos_logits = torch.cat(interaction_scores_pos).detach().cpu().numpy()\n",
    "neg_logits = torch.cat(interaction_scores_neg).detach().cpu().numpy()\n",
    "print(\"Positives:\", pos_logits.shape)\n",
    "print(\"Negatives:\", neg_logits.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7b8210b6-7e6a-4f9b-8f63-5db6f32959f3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 600x400 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize=(6, 4))\n",
    "plt.hist(pos_logits, bins=50, alpha=0.6, label=\"Positive pairs\", density=True)\n",
    "plt.hist(neg_logits, bins=50, alpha=0.6, label=\"Negative pairs\", density=True)\n",
    "plt.xlabel(\"Logit score\")\n",
    "plt.ylabel(\"Density\")\n",
    "plt.title(\"Distribution of Positive vs Negative Interaction Scores\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06320982-3625-4b6e-9102-c9e822781a75",
   "metadata": {},
   "source": [
    "#### non-dimers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9e81400c-1e6d-4178-a22c-ee4e1129f9bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "#Iterating through batched data: 13it [00:01, 11.10it/s]                                                                \n"
     ]
    }
   ],
   "source": [
    "interaction_scores = []\n",
    "\n",
    "for batch in tqdm(non_dimers_dataloader, total=round(len(Df_test_non_dimer)/10), desc=\"#Iterating through batched data\"):\n",
    "    b_emb, t_emb, lbls = batch\n",
    "    embedding_pep = b_emb.to(\"cuda\")\n",
    "    embedding_prot = t_emb.to(\"cuda\")\n",
    "\n",
    "    with torch.no_grad():\n",
    "        positive_logits = model(embedding_pep, embedding_prot)\n",
    "        interaction_scores.append(positive_logits.unsqueeze(0))\n",
    "\n",
    "# Convert list of tensors to single 1D tensors\n",
    "predicted_interaction_scores = np.concatenate([batch_score.cpu().detach().numpy().reshape(-1,) for batch_score in interaction_scores])\n",
    "# interaction_probabilities = np.concatenate([torch.sigmoid(batch_score[0]).cpu().numpy() for batch_score in interaction_scores])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8be84d78-c6b3-45fd-8274-f2f4f0bafd4e",
   "metadata": {},
   "source": [
    "#### meta-analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "833362b0-8b44-445b-a540-bff8ffb26ac9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "#Iterating through batched data:  82%|███████████████████████████████████████▍        | 290/353 [00:26<00:06,  9.09it/s]"
     ]
    }
   ],
   "source": [
    "# Loading batches\n",
    "interaction_scores = []\n",
    "\n",
    "for batch in tqdm(validation_dataloader, total = round(len(interaction_df_shuffled)/10),  desc= \"#Iterating through batched data\"):\n",
    "    b_emb, t_emb, lbls = batch\n",
    "    embedding_pep = b_emb.to(\"cuda\")\n",
    "    embedding_prot = t_emb.to(\"cuda\")\n",
    "\n",
    "    with torch.no_grad():\n",
    "        positive_logits = model(embedding_pep, embedding_prot)\n",
    "        interaction_scores.append(positive_logits.unsqueeze(0))\n",
    "\n",
    "predicted_interaction_scores = np.concatenate([batch_score.cpu().detach().numpy().reshape(-1,) for batch_score in interaction_scores])\n",
    "interaction_probabilities = np.concatenate([torch.sigmoid(batch_score[0]).cpu().numpy() for batch_score in interaction_scores])\n",
    "\n",
    "pos_logits, neg_logits = [], []\n",
    "for i, row in interaction_df_shuffled.iterrows():\n",
    "    logit = predicted_interaction_scores[i]\n",
    "    if row.binder_label == False:\n",
    "        neg_logits.append(logit)\n",
    "    elif row.binder_label == True:\n",
    "        pos_logits.append(logit)\n",
    "\n",
    "plt.figure(figsize=(5, 4))\n",
    "\n",
    "plt.hist(neg_logits, bins=50, alpha=0.6, label=\"Negative pairs\", density=True)\n",
    "plt.hist(pos_logits, bins=50, alpha=0.6, label=\"Positive pairs\", density=True)\n",
    "\n",
    "plt.xlabel(\"Logit score\")\n",
    "plt.ylabel(\"Density\")\n",
    "plt.title(\"Distribution of Positive vs Negative Interaction Scores\")\n",
    "\n",
    "# --- Simple grid behind ---\n",
    "plt.grid(True, linestyle=\"--\", alpha=0.5)\n",
    "\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89c71e84-c25b-4700-8178-2ed8c0dad997",
   "metadata": {},
   "source": [
    "## ESM 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb19353a-d0f2-4302-b82d-e74c6236f04f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CLIP_PPint_class(Dataset):\n",
    "    def __init__(\n",
    "        self,\n",
    "        dframe,\n",
    "        path,\n",
    "        embedding_dim=1280,\n",
    "        embedding_pad_value=-5000.0,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.dframe = dframe.copy()\n",
    "        self.embedding_dim = int(embedding_dim)\n",
    "        self.emb_pad = float(embedding_pad_value)\n",
    "\n",
    "        # lengths\n",
    "        self.max_blen = self.dframe[\"seq_binder_len\"].max()\n",
    "        self.max_tlen = self.dframe[\"seq_target_len\"].max()\n",
    "\n",
    "        # paths\n",
    "        self.encoding_path  = path\n",
    "\n",
    "        # index & storage\n",
    "        self.dframe.set_index(\"target_binder_id\", inplace=True)\n",
    "        self.accessions = self.dframe.index.astype(str).tolist()\n",
    "        self.name_to_row = {name: i for i, name in enumerate(self.accessions)}\n",
    "        self.samples = []\n",
    "\n",
    "        for accession in tqdm(self.accessions, total=len(self.accessions), desc=\"#Loading ESM2 embeddings and contacts\"):\n",
    "            parts = accession.split(\"_\") # e.g. accession 7S8T_5_F_7S8T_5_G\n",
    "            tgt_id = parts[0]+\"_\"+parts[2]\n",
    "            bnd_id = parts[-3]+\"_\"+parts[-1]\n",
    "\n",
    "            ### --- embeddings (pad to fixed lengths) --- ###\n",
    "            \n",
    "            # laod embeddings\n",
    "            t_emb = np.load(os.path.join(self.encoding_path, f\"{tgt_id}.npy\"))     # [Lt, D]\n",
    "            b_emb = np.load(os.path.join(self.encoding_path, f\"{bnd_id}.npy\"))     # [Lb, D]\n",
    "\n",
    "            # quich check whether embedding dimmension is as it suppose to be\n",
    "            if t_emb.shape[1] != self.embedding_dim or b_emb.shape[1] != self.embedding_dim:\n",
    "                raise ValueError(\"Embedding dim mismatch with 'embedding_dim'.\")\n",
    "\n",
    "            # add -5000 to all the padded target rows\n",
    "            if t_emb.shape[0] < self.max_tlen:\n",
    "                t_emb = np.concatenate([t_emb, np.full((self.max_tlen - t_emb.shape[0], t_emb.shape[1]), self.emb_pad, dtype=t_emb.dtype)], axis=0)\n",
    "            else:\n",
    "                t_emb = t_emb[: self.max_tlen] # no padding was used\n",
    "\n",
    "            # add -5000 to all the padded binder rows\n",
    "            if b_emb.shape[0] < self.max_blen:\n",
    "                b_emb = np.concatenate([b_emb, np.full((self.max_blen - b_emb.shape[0], b_emb.shape[1]), self.emb_pad, dtype=b_emb.dtype)], axis=0)\n",
    "            else:\n",
    "                b_emb = b_emb[: self.max_blen] # no padding was used\n",
    "\n",
    "            self.samples.append((b_emb, t_emb))\n",
    "\n",
    "    # ---- Dataset API ----\n",
    "    def __len__(self):\n",
    "        return len(self.samples)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        b_arr, t_arr = self.samples[idx]\n",
    "        binder_emb, target_emb = torch.from_numpy(b_arr).float(), torch.from_numpy(t_arr).float()\n",
    "        label = torch.tensor(1, dtype=torch.float32)  # single scalar labe\n",
    "        return binder_emb, target_emb, label\n",
    "\n",
    "    def _get_by_name(self, name):\n",
    "        # Single item -> return exactly what __getitem__ returns\n",
    "        if isinstance(name, str):\n",
    "            return self.__getitem__(self.name_to_row[name])\n",
    "        \n",
    "        # Multiple items -> fetch all\n",
    "        out = [self.__getitem__(self.name_to_row[n]) for n in list(name)]\n",
    "        b_list, t_list, lbl_list = zip(*out)\n",
    "    \n",
    "        # Stack embeddings\n",
    "        b  = torch.stack([torch.as_tensor(x) for x in b_list],  dim=0)  # [B, ...]\n",
    "        t  = torch.stack([torch.as_tensor(x) for x in t_list],  dim=0)  # [B, ...]\n",
    "    \n",
    "        # Stack labels\n",
    "        labels = torch.stack(lbl_list)  # [B]\n",
    "    \n",
    "        return b, t, labels\n",
    "\n",
    "emb_path = \"/work3/s232958/data/PPint_DB/embeddings_esm2\"\n",
    "\n",
    "training_Dataset = CLIP_PPint_class(\n",
    "    Df_train,\n",
    "    path=emb_path,\n",
    "    embedding_dim=1280\n",
    ")\n",
    "\n",
    "testing_Dataset = CLIP_PPint_class(\n",
    "    Df_test,\n",
    "    path=emb_path,\n",
    "    embedding_dim=1280\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a996b3f-3d59-438e-a898-7fbd88ff9bf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CLIP_Meta_class(Dataset):\n",
    "    def __init__(\n",
    "        self,\n",
    "        dframe,\n",
    "        paths,\n",
    "        embedding_dim=1280,\n",
    "        embedding_pad_value=-5000.0\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.dframe = dframe.copy()\n",
    "        self.embedding_dim = int(embedding_dim)\n",
    "        self.emb_pad = float(embedding_pad_value)\n",
    "        self.max_blen = self.dframe[\"seq_binder_len\"].max()\n",
    "        self.max_tlen = self.dframe[\"seq_target_len\"].max()\n",
    "\n",
    "        # paths\n",
    "        self.encoding_bpath, self.encoding_tpath = paths\n",
    "\n",
    "        # index & storage\n",
    "        self.dframe.set_index(\"binder_id\", inplace=True)\n",
    "        self.accessions = self.dframe.index.astype(str).tolist()\n",
    "        self.name_to_row = {name: i for i, name in enumerate(self.accessions)}\n",
    "        self.samples = []\n",
    "\n",
    "        for accession in tqdm(self.accessions, total=len(self.accessions), desc=\"#Loading ESM2 embeddings\"):\n",
    "            lbl = torch.tensor(int(self.dframe.loc[accession, \"binder_label\"]))\n",
    "            parts = accession.split(\"_\") # e.g. accession 7S8T_5_F_7S8T_5_G\n",
    "            tgt_id = \"_\".join(parts[:-1])\n",
    "            bnd_id = accession\n",
    "\n",
    "            ### --- embeddings (pad to fixed lengths) --- ###\n",
    "            \n",
    "            # laod embeddings\n",
    "            t_emb = np.load(os.path.join(self.encoding_tpath, f\"{tgt_id}.npy\"))     # [Lt, D]\n",
    "            b_emb = np.load(os.path.join(self.encoding_bpath, f\"{bnd_id}.npy\"))     # [Lb, D]\n",
    "\n",
    "            # quich check whether embedding dimmension is as it suppose to be\n",
    "            if t_emb.shape[1] != self.embedding_dim or b_emb.shape[1] != self.embedding_dim:\n",
    "                raise ValueError(\"Embedding dim mismatch with 'embedding_dim'.\")\n",
    "\n",
    "            # add -5000 to all the padded target rows\n",
    "            if t_emb.shape[0] < self.max_tlen:\n",
    "                t_emb = np.concatenate([t_emb, np.full((self.max_tlen - t_emb.shape[0], t_emb.shape[1]), self.emb_pad, dtype=t_emb.dtype)], axis=0)\n",
    "            else:\n",
    "                t_emb = t_emb[: self.max_tlen] # no padding was used\n",
    "\n",
    "            # add -5000 to all the padded binder rows\n",
    "            if b_emb.shape[0] < self.max_blen:\n",
    "                b_emb = np.concatenate([b_emb, np.full((self.max_blen - b_emb.shape[0], b_emb.shape[1]), self.emb_pad, dtype=b_emb.dtype)], axis=0)\n",
    "            else:\n",
    "                b_emb = b_emb[: self.max_blen] # no padding was used\n",
    "\n",
    "            self.samples.append((b_emb, t_emb, lbl))\n",
    "\n",
    "    # ---- Dataset API ----\n",
    "    def __len__(self):\n",
    "        return len(self.samples)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        b_arr, t_arr, lbls = self.samples[idx]\n",
    "        binder_emb, target_emb = torch.from_numpy(b_arr).float(), torch.from_numpy(t_arr).float()\n",
    "        return binder_emb, target_emb, lbls\n",
    "\n",
    "    def _get_by_name(self, name):\n",
    "        # Single item -> return exactly what __getitem__ returns\n",
    "        if isinstance(name, str):\n",
    "            return self.__getitem__(self.name_to_row[name])\n",
    "        \n",
    "        # Multiple items -> fetch all\n",
    "        out = [self.__getitem__(self.name_to_row[n]) for n in list(name)]\n",
    "        b_list, t_list, lbl_list = zip(*out)\n",
    "    \n",
    "        # Stack embeddings\n",
    "        b  = torch.stack([torch.as_tensor(x) for x in b_list],  dim=0)  # [B, ...]\n",
    "        t  = torch.stack([torch.as_tensor(x) for x in t_list],  dim=0)  # [B, ...]\n",
    "    \n",
    "        # Stack labels\n",
    "        labels = torch.stack(lbl_list)  # [B]\n",
    "    \n",
    "        return b, t, labels\n",
    "\n",
    "bemb_path = \"/work3/s232958/data/meta_analysis/embeddings_esm2_binders\"\n",
    "temb_path = \"/work3/s232958/data/meta_analysis/embeddings_esm2_targets\"\n",
    "\n",
    "validation_Dataset = CLIP_Meta_class(\n",
    "    # interaction_df_shuffled[:len(Df_test)],\n",
    "    interaction_df_shuffled,\n",
    "    paths=[bemb_path, temb_path],\n",
    "    embedding_dim=1280\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30f5b133-fc38-4b51-a3e0-23dbfd0a1f16",
   "metadata": {},
   "source": [
    "### Loading Pre-trained Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfbe90b1-b384-44a0-93e7-56b991342b00",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_key_padding_mask(embeddings, padding_value=-5000, offset=10):\n",
    "    \"\"\"\n",
    "    Purpose: return vector indicating which rows are not padded (don't have values = -5000)\n",
    "    \"\"\"\n",
    "    return (embeddings < (padding_value + offset)).all(dim=-1)\n",
    "\n",
    "def create_mean_of_non_masked(embeddings, padding_mask):\n",
    "    # Use masked select and mean to compute the mean of non-masked elements\n",
    "    # embeddings should be of shape (batch_size, seq_len, features)\n",
    "    seq_embeddings = []\n",
    "    for i in range(embeddings.shape[0]): # looping over all batch elements\n",
    "        non_masked_embeddings = embeddings[i][~padding_mask[i]] # shape [num_real_tokens, features]\n",
    "        if len(non_masked_embeddings) == 0:\n",
    "            print(\"You are masking all positions when creating sequence representation\")\n",
    "            sys.exit(1)\n",
    "        mean_embedding = non_masked_embeddings.mean(dim=0) # sequence is represented by the single vecotr [1280] [features]\n",
    "        seq_embeddings.append(mean_embedding)\n",
    "    return torch.stack(seq_embeddings)\n",
    "\n",
    "def get_sinusoid_encoding(num_tokens, token_len):\n",
    "    \"\"\"\n",
    "    Purpose: positional encoding having the same dimensions as token matrix\n",
    "    Standard Vaswani sinusoidal positional encoding for a sequence of length L=num_tokens and dim D=token_len.\n",
    "    Returns: [L, D] tensor (no batch dim).\n",
    "    \"\"\"\n",
    "    device = torch.device(\"cpu\")\n",
    "    L, D = num_tokens, token_len\n",
    "    pos = torch.arange(L, dtype=torch.float32, device=device).unsqueeze(1)      # [L,1]\n",
    "    i   = torch.arange(D, dtype=torch.float32, device=device).unsqueeze(0)      # [1,D]\n",
    "    div = torch.pow(10000.0, (2 * torch.floor(i/2)) / D)                        # [1,D]\n",
    "    angles = pos / div                                                          # [L,D]\n",
    "    pe = torch.zeros(L, D, dtype=torch.float32, device=device)\n",
    "    pe[:, 0::2] = torch.sin(angles[:, 0::2])\n",
    "    pe[:, 1::2] = torch.cos(angles[:, 1::2])\n",
    "    return pe\n",
    "\n",
    "def pad_tokens_to_L(tokens: torch.Tensor, L_target: int):\n",
    "    if tokens.dim() == 3:\n",
    "        B, Ls, D = tokens.shape\n",
    "        if Ls >= L_target:\n",
    "            return tokens[:, :L_target, :]\n",
    "        pad = tokens.new_zeros(B, L_target - Ls, D)\n",
    "        return torch.cat([tokens, pad], dim=1)\n",
    "    elif tokens.dim() == 2:\n",
    "        Ls, D = tokens.shape\n",
    "        if Ls >= L_target:\n",
    "            return tokens[:L_target]\n",
    "        pad = tokens.new_zeros(L_target - Ls, D)\n",
    "        return torch.cat([tokens, pad], dim=0)\n",
    "    else:\n",
    "        raise ValueError(f\"Unexpected tokens.dim()={tokens.dim()} (wanted 2 or 3)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5abd97ce-ebbb-4e34-bfd0-7076d70c2445",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_key_padding_mask(embeddings, padding_value=-5000, offset=10):\n",
    "    return (embeddings < (padding_value + offset)).all(dim=-1)\n",
    "\n",
    "def create_mean_of_non_masked(embeddings, padding_mask):\n",
    "    # Use masked select and mean to compute the mean of non-masked elements\n",
    "    # embeddings should be of shape (batch_size, seq_len, features)\n",
    "    seq_embeddings = []\n",
    "    for i in range(embeddings.shape[0]): # looping over all batch elements\n",
    "        non_masked_embeddings = embeddings[i][~padding_mask[i]] # shape [num_real_tokens, features]\n",
    "        if len(non_masked_embeddings) == 0:\n",
    "            print(\"You are masking all positions when creating sequence representation\")\n",
    "            sys.exit(1)\n",
    "        mean_embedding = non_masked_embeddings.mean(dim=0) # sequence is represented by the single vecotr [1152] [features]\n",
    "        seq_embeddings.append(mean_embedding)\n",
    "    return torch.stack(seq_embeddings)\n",
    "\n",
    "class MiniCLIP_w_transformer_crossattn(pl.LightningModule):\n",
    "\n",
    "    def __init__(self, padding_value = -5000, embed_dimension=embedding_dimension, num_recycles=2):\n",
    "\n",
    "        super().__init__()\n",
    "        self.num_recycles = num_recycles # how many times you iteratively refine embeddings with self- and cross-attention (ALPHA-Fold-style recycling).\n",
    "        self.padding_value = padding_value\n",
    "        self.embed_dimension = embed_dimension\n",
    "\n",
    "        self.logit_scale = nn.Parameter(torch.tensor(math.log(1/0.07)))  # ~CLIP init\n",
    "\n",
    "        self.transformerencoder =  nn.TransformerEncoderLayer(\n",
    "            d_model=self.embed_dimension,\n",
    "            nhead=8,\n",
    "            dropout=0.1,\n",
    "            batch_first=True,\n",
    "            dim_feedforward=self.embed_dimension\n",
    "            )\n",
    " \n",
    "        self.norm = nn.LayerNorm(self.embed_dimension)  # For residual additions\n",
    "\n",
    "        self.cross_attn = nn.MultiheadAttention(\n",
    "            embed_dim=self.embed_dimension,\n",
    "            num_heads=8,\n",
    "            dropout=0.1,\n",
    "            batch_first=True\n",
    "        )\n",
    "\n",
    "        self.prot_embedder = nn.Sequential(\n",
    "            nn.Linear(self.embed_dimension, 640),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(640, 320),\n",
    "        )\n",
    "        \n",
    "    def forward(self, pep_input, prot_input, label=None, pep_int_mask=None, prot_int_mask=None, int_prob=None, mem_save=True): # , pep_tokens, prot_tokens\n",
    "\n",
    "        pep_mask = create_key_padding_mask(embeddings=pep_input, padding_value=self.padding_value)\n",
    "        prot_mask = create_key_padding_mask(embeddings=prot_input, padding_value=self.padding_value)\n",
    " \n",
    "        # Initialize residual states\n",
    "        pep_emb = pep_input.clone()\n",
    "        prot_emb = prot_input.clone()\n",
    " \n",
    "        for _ in range(self.num_recycles):\n",
    "\n",
    "            # Transformer encoding with residual\n",
    "            pep_trans = self.transformerencoder(self.norm(pep_emb), src_key_padding_mask=pep_mask)\n",
    "            prot_trans = self.transformerencoder(self.norm(prot_emb), src_key_padding_mask=prot_mask)\n",
    "\n",
    "            # Cross-attention with residual\n",
    "            pep_cross, _ = self.cross_attn(query=self.norm(pep_trans), key=self.norm(prot_trans), value=self.norm(prot_trans), key_padding_mask=prot_mask)\n",
    "            prot_cross, _ = self.cross_attn(query=self.norm(prot_trans), key=self.norm(pep_trans), value=self.norm(pep_trans), key_padding_mask=pep_mask)\n",
    "            \n",
    "            # Additive update with residual connection\n",
    "            pep_emb = pep_emb + pep_trans  \n",
    "            prot_emb = prot_emb + prot_trans\n",
    "\n",
    "        pep_seq_coding = create_mean_of_non_masked(pep_emb, pep_mask)\n",
    "        prot_seq_coding = create_mean_of_non_masked(prot_emb, prot_mask)\n",
    "        \n",
    "        # Use self-attention outputs for embeddings\n",
    "        pep_seq_coding = F.normalize(self.prot_embedder(pep_seq_coding), dim=-1)\n",
    "        prot_seq_coding = F.normalize(self.prot_embedder(prot_seq_coding), dim=-1)\n",
    " \n",
    "        if mem_save:\n",
    "            torch.cuda.empty_cache()\n",
    "        \n",
    "        scale = torch.exp(self.logit_scale).clamp(max=100.0)\n",
    "        logits = scale * (pep_seq_coding * prot_seq_coding).sum(dim=-1)\n",
    "        \n",
    "        return logits\n",
    "\n",
    "    def training_step(self, batch, device):\n",
    "        embedding_pep, embedding_prot, labels = batch\n",
    "        embedding_pep, embedding_prot = embedding_pep.to(device), embedding_prot.to(device)\n",
    "        \n",
    "        positive_logits = self.forward(embedding_pep, embedding_prot)\n",
    "        \n",
    "        # Negative indexes\n",
    "        rows, cols = torch.triu_indices(embedding_prot.size(0), embedding_prot.size(0), offset=1)         \n",
    "        \n",
    "        negative_logits = self(embedding_pep[rows,:,:], \n",
    "                          embedding_prot[cols,:,:], \n",
    "                          int_prob=0.0)\n",
    "\n",
    "        # loss of predicting partner using peptide\n",
    "        positive_loss = F.binary_cross_entropy_with_logits(positive_logits, torch.ones_like(positive_logits).to(device))\n",
    " \n",
    "        # loss of predicting peptide using partner\n",
    "        negative_loss =  F.binary_cross_entropy_with_logits(negative_logits, torch.zeros_like(negative_logits).to(device))\n",
    "        \n",
    "        loss = (positive_loss + negative_loss) / 2\n",
    " \n",
    "        # del partner_prediction_loss, peptide_prediction_loss, embedding_pep, embedding_prot\n",
    "        torch.cuda.empty_cache()\n",
    "        return loss\n",
    "\n",
    "    def validation_step_PPint(self, batch, device):\n",
    "        # Predict on random batches of training batch size\n",
    "        embedding_pep, embedding_prot, labels = batch\n",
    "        embedding_pep, embedding_prot = embedding_pep.to(device), embedding_prot.to(device)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "\n",
    "            positive_logits = self(embedding_pep, embedding_prot)\n",
    "            \n",
    "            # loss of predicting partner using peptide\n",
    "            positive_loss = F.binary_cross_entropy_with_logits(positive_logits, torch.ones_like(positive_logits).to(device))\n",
    "            \n",
    "            # Negaive indexes\n",
    "            rows, cols = torch.triu_indices(embedding_prot.size(0), embedding_prot.size(0), offset=1)\n",
    "            \n",
    "            negative_logits = self(embedding_pep[rows,:,:], embedding_prot[cols,:,:], int_prob=0.0)\n",
    "    \n",
    "            negative_loss =  F.binary_cross_entropy_with_logits(negative_logits, torch.zeros_like(negative_logits).to(device))\n",
    "\n",
    "            loss = (positive_loss + negative_loss) / 2\n",
    "           \n",
    "            logit_matrix = torch.zeros((embedding_pep.size(0),embedding_pep.size(0)),device=self.device)\n",
    "            logit_matrix[rows, cols] = negative_logits\n",
    "            logit_matrix[cols, rows] = negative_logits\n",
    "            \n",
    "            # Fill diagonal with positive scores\n",
    "            diag_indices = torch.arange(embedding_pep.size(0), device=self.device)\n",
    "            logit_matrix[diag_indices, diag_indices] = positive_logits.squeeze()\n",
    "\n",
    "            labels = torch.arange(embedding_prot.size(0)).to(self.device)\n",
    "            peptide_predictions = logit_matrix.argmax(dim=0)\n",
    "            peptide_ranks = logit_matrix.argsort(dim=0).diag() + 1\n",
    "            peptide_mrr = (peptide_ranks).float().pow(-1).mean()\n",
    "            \n",
    "            # partner_accuracy = partner_predictions.eq(labels).float().mean()\n",
    "            peptide_accuracy = peptide_predictions.eq(labels).float().mean()\n",
    "    \n",
    "            k = 3\n",
    "            peptide_topk_accuracy = torch.any((logit_matrix.topk(k, dim=0).indices - labels.reshape(1, -1)) == 0, dim=0).sum() / logit_matrix.shape[0]\n",
    "    \n",
    "            del logit_matrix,positive_logits,negative_logits,embedding_pep,embedding_prot\n",
    "\n",
    "            return loss, peptide_accuracy, peptide_topk_accuracy\n",
    "    \n",
    "    def validation_step_MetaDataset(self, batch, device):\n",
    "        embedding_binder, embedding_target, labels = batch\n",
    "        embedding_binder = embedding_binder.to(device)\n",
    "        embedding_target = embedding_target.to(device)\n",
    "        labels = labels.to(device).float()\n",
    "    \n",
    "        with torch.no_grad():\n",
    "            logits = self.forward(embedding_binder, embedding_target)\n",
    "            logits = logits.float()\n",
    "            loss = F.binary_cross_entropy_with_logits(logits.view(-1), labels.view(-1))\n",
    "            return logits, loss\n",
    "\n",
    "    def calculate_logit_matrix(self,embedding_pep,embedding_prot):\n",
    "        rows, cols = torch.triu_indices(embedding_pep.size(0), embedding_pep.size(0), offset=1)\n",
    "        \n",
    "        positive_logits = self(embedding_pep, embedding_prot)\n",
    "        negative_logits = self(embedding_pep[rows,:,:], embedding_prot[cols,:,:], int_prob=0.0)\n",
    "        \n",
    "        logit_matrix = torch.zeros((embedding_pep.size(0),embedding_pep.size(0)),device=self.device)\n",
    "        logit_matrix[rows, cols] = negative_logits\n",
    "        logit_matrix[cols, rows] = negative_logits\n",
    "        \n",
    "        diag_indices = torch.arange(embedding_pep.size(0), device=self.device)\n",
    "        logit_matrix[diag_indices, diag_indices] = positive_logits.squeeze()\n",
    "        \n",
    "        return logit_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71cff534-579f-4fcf-9b85-176089d173f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "testing_dataloader = DataLoader(testing_Dataset, batch_size=10, shuffle=False)\n",
    "validation_dataloader = DataLoader(validation_Dataset, batch_size=10, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf1144ef-6dc4-4ead-9575-2508aa2c8e75",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.listdir(\"/work3/s232958/data/trained/original_architecture/63a90678-26de-48fc-844f-737d34fa60a3\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2ffcc8e-c045-46b6-a3a8-72b465e024af",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = MiniCLIP_w_transformer_crossattn(embed_dimension=1280, num_recycles=2).to(\"cuda\")\n",
    "path = \"/work3/s232958/data/trained/original_architecture/63a90678-26de-48fc-844f-737d34fa60a3/63a90678-26de-48fc-844f-737d34fa60a3_checkpoint_6/63a90678-26de-48fc-844f-737d34fa60a3_checkpoint_epoch_6.pth\"\n",
    "checkpoint = torch.load(path, weights_only=False, map_location=torch.device('cpu'))\n",
    "# print(list(checkpoint[\"model_state_dict\"]))\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "model.load_state_dict(checkpoint['model_state_dict'])\n",
    "model.to(device)\n",
    "model.eval()  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "907bb456-be62-4be5-98cd-154dd04c76a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "interaction_scores_pos = []\n",
    "interaction_scores_neg = []    \n",
    "\n",
    "for batch in tqdm(testing_dataloader, total=round(len(Df_test)/10), desc=\"#Iterating through batched data\"):\n",
    "    b_emb, t_emb, lbls = batch\n",
    "    embedding_pep = b_emb.to(\"cuda\")\n",
    "    embedding_prot = t_emb.to(\"cuda\")\n",
    "\n",
    "    with torch.no_grad():\n",
    "\n",
    "        rows, cols = torch.triu_indices(embedding_pep.size(0), embedding_pep.size(0), offset=1)\n",
    "        positive_logits = model(embedding_pep, embedding_prot)\n",
    "        negative_logits = model(embedding_pep[rows,:,:], embedding_prot[cols,:,:], int_prob=0.0)\n",
    "        \n",
    "        logit_matrix = torch.zeros((embedding_pep.size(0),embedding_pep.size(0)),device=\"cuda\")\n",
    "        logit_matrix[rows, cols] = negative_logits\n",
    "        logit_matrix[cols, rows] = negative_logits\n",
    "        \n",
    "        diag_indices = torch.arange(embedding_pep.size(0), device=\"cuda\")\n",
    "        logit_matrix[diag_indices, diag_indices] = positive_logits.squeeze()\n",
    "        \n",
    "        # print(logit_matrix)\n",
    "        interaction_scores_pos.append(positive_logits)\n",
    "        interaction_scores_neg.append(negative_logits)\n",
    "\n",
    "# Convert list of tensors to single 1D tensors\n",
    "pos_logits = torch.cat(interaction_scores_pos).detach().cpu().numpy()\n",
    "neg_logits = torch.cat(interaction_scores_neg).detach().cpu().numpy()\n",
    "print(\"Positives:\", pos_logits.shape)\n",
    "print(\"Negatives:\", neg_logits.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "840b0512-1531-49b2-b642-192dd5c2dd44",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(6, 4))\n",
    "plt.hist(pos_logits, bins=50, alpha=0.6, label=\"Positive pairs\", density=True)\n",
    "plt.hist(neg_logits, bins=50, alpha=0.6, label=\"Negative pairs\", density=True)\n",
    "\n",
    "plt.xlabel(\"Logit score\")\n",
    "plt.ylabel(\"Density\")\n",
    "plt.title(\"Distribution of Positive vs Negative Interaction Scores\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea238def-e1a8-4b48-b268-e7e1aefe90c4",
   "metadata": {},
   "source": [
    "#### non-dimers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e60cfc88-0928-415f-a8bc-05ada2c4cf6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "Df_test_non_dimer = Df_test[Df_test.dimer == False]\n",
    "\n",
    "non_dimers_Dataset = CLIP_PPint_class(\n",
    "    Df_test_non_dimer,\n",
    "    path=emb_path,\n",
    "    embedding_dim=1280)\n",
    "\n",
    "non_dimers_dataloader = DataLoader(non_dimers_Dataset, batch_size=10, shuffle = False)\n",
    "\n",
    "interaction_scores = []\n",
    "for batch in tqdm(non_dimers_dataloader, total=round(len(Df_test_non_dimer)/10), desc=\"#Iterating through batched data\"):\n",
    "    b_emb, t_emb, lbls = batch\n",
    "    embedding_pep = b_emb.to(\"cuda\")\n",
    "    embedding_prot = t_emb.to(\"cuda\")\n",
    "\n",
    "    with torch.no_grad():\n",
    "        positive_logits = model(embedding_pep, embedding_prot)\n",
    "        interaction_scores.append(positive_logits.unsqueeze(0))\n",
    "\n",
    "# Convert list of tensors to single 1D tensors\n",
    "predicted_interaction_scores = np.concatenate([batch_score.cpu().detach().numpy().reshape(-1,) for batch_score in interaction_scores])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6572da55-b740-4a9c-b206-812605483ac6",
   "metadata": {},
   "source": [
    "#### meta-analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e42cd761-b45c-4a80-b094-a0a8af7e243e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading batches\n",
    "interaction_scores = []\n",
    "\n",
    "for batch in tqdm(validation_dataloader, total = round(len(interaction_df_shuffled)/10),  desc= \"#Iterating through batched data\"):\n",
    "    b_emb, t_emb, lbls = batch\n",
    "    embedding_pep = b_emb.to(\"cuda\")\n",
    "    embedding_prot = t_emb.to(\"cuda\")\n",
    "\n",
    "    with torch.no_grad():\n",
    "        positive_logits = model(embedding_pep, embedding_prot)\n",
    "        interaction_scores.append(positive_logits.unsqueeze(0))\n",
    "\n",
    "predicted_interaction_scores = np.concatenate([batch_score.cpu().detach().numpy().reshape(-1,) for batch_score in interaction_scores])\n",
    "interaction_probabilities = np.concatenate([torch.sigmoid(batch_score[0]).cpu().numpy() for batch_score in interaction_scores])\n",
    "\n",
    "pos_logits, neg_logits = [], []\n",
    "for i, row in interaction_df_shuffled.iterrows():\n",
    "    logit = predicted_interaction_scores[i]\n",
    "    if row.binder_label == False:\n",
    "        neg_logits.append(logit)\n",
    "    elif row.binder_label == True:\n",
    "        pos_logits.append(logit)\n",
    "\n",
    "plt.figure(figsize=(5, 4))\n",
    "\n",
    "plt.hist(neg_logits, bins=50, alpha=0.6, label=\"Negative pairs\", density=True)\n",
    "plt.hist(pos_logits, bins=50, alpha=0.6, label=\"Positive pairs\", density=True)\n",
    "\n",
    "plt.xlabel(\"Logit score\")\n",
    "plt.ylabel(\"Density\")\n",
    "plt.title(\"Distribution of Positive vs Negative Interaction Scores\")\n",
    "\n",
    "# --- Simple grid behind ---\n",
    "plt.grid(True, linestyle=\"--\", alpha=0.5)\n",
    "\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7aee40cf-b10f-4825-bdce-3a4663a6e983",
   "metadata": {},
   "outputs": [],
   "source": [
    "interaction_df_shuffled[\"inter_prob\"] = interaction_probabilities\n",
    "interaction_df_shuffled[\"pred_binder\"] = interaction_df_shuffled[\"inter_prob\"] >= 0.5\n",
    "interaction_df_shuffled[\"intr_scores\"] = predicted_interaction_scores\n",
    "\n",
    "pred_labels = interaction_probabilities >= 0.5\n",
    "true_labels = np.array(interaction_df_shuffled[\"binder_label\"])\n",
    "\n",
    "true_positives = ((pred_labels == 1) & (true_labels == 1)).sum().item()\n",
    "true_negatives = ((pred_labels == 0) & (true_labels == 0)).sum().item()\n",
    "false_positives = ((pred_labels == 1) & (true_labels == 0)).sum().item()\n",
    "false_negatives = ((pred_labels == 0) & (true_labels == 1)).sum().item()\n",
    "\n",
    "predicted_positives = true_positives + false_positives\n",
    "all_real_positives = true_positives + false_negatives\n",
    "\n",
    "print(classification_report(true_labels, pred_labels, digits = 4))\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(8, 4))\n",
    "\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=confusion_matrix(true_labels, pred_labels))\n",
    "disp.plot(ax=axes[0])\n",
    "axes[0].set_title(\"Confusion Matrix\")\n",
    "\n",
    "TPR = true_positives / (true_positives + true_negatives) # how good the model is at predicting the positive class when the actual outcome is positive.\n",
    "# sensitivity = true_positives / (true_positives + false_negatives) # the same as TPR\n",
    "FPR = false_positives / (false_positives + true_negatives) # how often a positive class is predicted when the actual outcome is negative.\n",
    "# specificity = true_negatives / (true_negatives + false_positives) # FPR = 1 - specificity\n",
    "\n",
    "fpr, tpr, thresholds = roc_curve(true_labels, interaction_probabilities)\n",
    "auc = roc_auc_score(true_labels, interaction_probabilities)\n",
    "print('AUC: %.3f' % auc)\n",
    "\n",
    "axes[1].plot(fpr, tpr, linewidth=2)\n",
    "axes[1].plot([0, 1], [0, 1], linestyle=\"--\", linewidth=1)  # diagonal reference\n",
    "axes[1].set_xlabel('False Positive Rate')\n",
    "axes[1].set_ylabel('True Positive Rate')\n",
    "axes[1].set_xlim(0, 1)\n",
    "axes[1].set_ylim(0, 1)\n",
    "axes[1].set_title('ROC Curve')\n",
    "\n",
    "# show the plot\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92ec06df-3b0d-41c0-91ec-5676d0943188",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ESM CUDA Env",
   "language": "python",
   "name": "esm_cuda"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
