{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7f838074-e829-40d5-9b15-949a9bfe9708",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tesla V100-SXM2-32GB\n"
     ]
    }
   ],
   "source": [
    "import uuid, sys, os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import ast\n",
    "import math\n",
    "import random\n",
    "\n",
    "from sklearn import metrics\n",
    "from scipy import stats\n",
    "from collections import Counter\n",
    "\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"2\"\n",
    "os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"expandable_segments:True\"\n",
    "import torch\n",
    "torch.cuda.set_device(0)  # 0 == \"first visible\" -> actually GPU 2 on the node\n",
    "print(torch.cuda.get_device_name(0))\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader, TensorDataset, WeightedRandomSampler\n",
    "import pytorch_lightning as pl\n",
    "from torch.optim import AdamW\n",
    "\n",
    "torch.manual_seed(0)\n",
    "\n",
    "from accelerate import Accelerator\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a771b897-1647-4594-b648-41d2639d5282",
   "metadata": {},
   "outputs": [],
   "source": [
    "SEED = 0\n",
    "os.environ[\"PYTHONHASHSEED\"] = str(SEED)\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "torch.cuda.manual_seed_all(SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b1323865-da15-44ce-92e3-6e026ece0afd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/work3/s232958/envs/esm_gpu/lib/python3.10/site-packages/pydantic/_internal/_generate_schema.py:2249: UnsupportedFieldAttributeWarning: The 'repr' attribute with value False was provided to the `Field()` function, which has no effect in the context it was used. 'repr' is field-specific metadata, and can only be attached to a model field using `Annotated` metadata or by assignment. This may have happened because an `Annotated` type alias using the `type` statement was used, or if the `Field()` function was attached to a single member of a union type.\n",
      "  warnings.warn(\n",
      "/work3/s232958/envs/esm_gpu/lib/python3.10/site-packages/pydantic/_internal/_generate_schema.py:2249: UnsupportedFieldAttributeWarning: The 'frozen' attribute with value True was provided to the `Field()` function, which has no effect in the context it was used. 'frozen' is field-specific metadata, and can only be attached to a model field using `Annotated` metadata or by assignment. This may have happened because an `Annotated` type alias using the `type` statement was used, or if the `Field()` function was attached to a single member of a union type.\n",
      "  warnings.warn(\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33ms232958\u001b[0m (\u001b[33ms232958-danmarks-tekniske-universitet-dtu\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import wandb\n",
    "wandb.login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "12ceffc5-ad46-4e77-aee2-49d4743bd362",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch: 2.5.1\n",
      "Using device: cuda\n",
      "Current location: /zhome/c9/0/203261/DBL046_PP_osaul/DBL046_PP_osaul/tmp/ona_drafts\n"
     ]
    }
   ],
   "source": [
    "os.chdir(\"/zhome/c9/0/203261/DBL046_PP_osaul/DBL046_PP_osaul/tmp/ona_drafts\")\n",
    "# print(os.getcwd())\n",
    "\n",
    "print(\"PyTorch:\", torch.__version__)\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print('Using device:', device)\n",
    "print(\"Current location:\", os.getcwd())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fcfc6c69-e492-4bf1-a23e-e8ee41a27d4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model parameters\n",
    "memory_verbose = False\n",
    "use_wandb = True # Used to track loss in real-time without printing\n",
    "model_save_steps = 1\n",
    "train_frac = 1.0\n",
    "test_frac = 1.0\n",
    "\n",
    "embedding_dimension = 1280 #| 960 | 1152\n",
    "number_of_recycles = 2\n",
    "padding_value = -5000\n",
    "\n",
    "# batch_size = 20\n",
    "learning_rate = 2e-5\n",
    "EPOCHS = 15"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b152a01f-a72c-4d75-9e1c-6b6f937515eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total memory:  34.072559616\n",
      "Reserved memory:  0.0\n",
      "Allocated memory:  0.0\n",
      "Free memory:  0.0\n"
     ]
    }
   ],
   "source": [
    "## Output path\n",
    "trained_model_dir = \"/zhome/c9/0/203261/DBL046_PP_osaul/DBL046_PP_osaul/tmp/ona_drafts\"\n",
    "\n",
    "## Embeddings paths\n",
    "binders_embeddings = \"/zhome/c9/0/203261/DBL046_PP_osaul/DBL046_PP_osaul/tmp/data/PPint_DB/binders_embeddings_esm2\"\n",
    "targets_embeddings = \"/zhome/c9/0/203261/DBL046_PP_osaul/DBL046_PP_osaul/tmp/data/PPint_DB/targets_embeddings_esm2\"\n",
    "\n",
    "## Contact maps paths\n",
    "binders_contacts = \"/zhome/c9/0/203261/DBL046_PP_osaul/DBL046_PP_osaul/tmp/data/PPint_DB/binders_contacts\"\n",
    "targets_contacts = \"/zhome/c9/0/203261/DBL046_PP_osaul/DBL046_PP_osaul/tmp/data/PPint_DB/targets_contacts\"\n",
    "\n",
    "# ## Training variables\n",
    "runID = uuid.uuid4()\n",
    "\n",
    "def print_mem_consumption():\n",
    "    # 1. Total memory available on the GPU (device 0)\n",
    "    t = torch.cuda.get_device_properties(0).total_memory\n",
    "    # 2. How much memory PyTorch has *reserved* from CUDA\n",
    "    r = torch.cuda.memory_reserved(0)\n",
    "    # 3. How much of that reserved memory is actually *used* by tensors\n",
    "    a = torch.cuda.memory_allocated(0)\n",
    "    # 4. Reserved but not currently allocated (so “free inside PyTorch’s pool”)\n",
    "    f = r - a\n",
    "\n",
    "    print(\"Total memory: \", t/1e9)      # total VRAM in GB\n",
    "    print(\"Reserved memory: \", r/1e9)   # PyTorch’s reserved pool in GB\n",
    "    print(\"Allocated memory: \", a//1e9) # actually in use (integer division)\n",
    "    print(\"Free memory: \", f/1e9)       # slack in the reserved pool in GB\n",
    "print_mem_consumption()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6a54147-26d3-4e49-b87a-b05209b00a7d",
   "metadata": {},
   "source": [
    "### Loading PPint dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "89207aed-ed9e-4618-8591-ad4a5b4bd3e9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>seq_target</th>\n",
       "      <th>seq_binder</th>\n",
       "      <th>target_id</th>\n",
       "      <th>binder_id</th>\n",
       "      <th>target_binder_id</th>\n",
       "      <th>seq_target_len</th>\n",
       "      <th>seq_binder_len</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>SLTKTERTIIVSMWAKISTQADTIGTETLERLFLSHPQTKTYFPHF...</td>\n",
       "      <td>VHLTDAEKAAVSGLWGKVNADEVGGEALGRLLVVYPWTQRYFDSFG...</td>\n",
       "      <td>1JEB_2</td>\n",
       "      <td>1JEB_2</td>\n",
       "      <td>1JEB_2_1JEB_2</td>\n",
       "      <td>141</td>\n",
       "      <td>146</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>TTTLAFKFQHGVIAAVDSRASAGSYISALRVNKVIEINPYLLGTMS...</td>\n",
       "      <td>DRGVNTFSPEGRLFQVEYAIEAIKLGSTAIGIQTSEGVCLAVEKRI...</td>\n",
       "      <td>7B12_23</td>\n",
       "      <td>7B12_23</td>\n",
       "      <td>7B12_23_7B12_23</td>\n",
       "      <td>203</td>\n",
       "      <td>230</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ITHLPPEVMLSIFSYLNPQELCRCSQVSMKWSQLTKTGSLWKHLYP...</td>\n",
       "      <td>PSIKLQSSDGEIFEVDVEIAKQSVTIKTMLEDLGDPVPLPNVNAAI...</td>\n",
       "      <td>6VCD_1</td>\n",
       "      <td>6VCD_1</td>\n",
       "      <td>6VCD_1_6VCD_1</td>\n",
       "      <td>255</td>\n",
       "      <td>135</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>NAKDVLGLTLLEKTLKERLNLKDAIIVSGDSDQSPWVKKEGRAAVA...</td>\n",
       "      <td>AKDVLGLTLLEKTLKERLNLKDAIIVSGDSDQSPWVKKEGRAAVAC...</td>\n",
       "      <td>2OKG_0</td>\n",
       "      <td>2OKG_0</td>\n",
       "      <td>2OKG_0_2OKG_0</td>\n",
       "      <td>241</td>\n",
       "      <td>243</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>DIVMSQSPSSLAVSVGEKVTMSCKSSQSLLYNNNQKNYLAWYQQKP...</td>\n",
       "      <td>VTLKESGPGILQPSQTLSLTCSFSGFSLSTYGMGVGWIRQPSGKGL...</td>\n",
       "      <td>3MBX_0</td>\n",
       "      <td>3MBX_0</td>\n",
       "      <td>3MBX_0_3MBX_0</td>\n",
       "      <td>220</td>\n",
       "      <td>229</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2467</th>\n",
       "      <td>REIPLKVLVKAVLFACMLMRKTMASRVRVTILFATETGKSEALAWD...</td>\n",
       "      <td>QLTEEQIAEFKEAFSLFDKDGDGTITTKELGTVMRSLGQNPTEAEL...</td>\n",
       "      <td>3HR4_0</td>\n",
       "      <td>3HR4_0</td>\n",
       "      <td>3HR4_0_3HR4_0</td>\n",
       "      <td>189</td>\n",
       "      <td>145</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2468</th>\n",
       "      <td>NKYRFIDVQPLTGVLGAEITGVDLREPLDDSTWNEILDAFHTYQVI...</td>\n",
       "      <td>KYRFIDVQPLTGVLGAEITGVDLREPLDDSTWNEILDAFHTYQVIY...</td>\n",
       "      <td>6D3M_0</td>\n",
       "      <td>6D3M_0</td>\n",
       "      <td>6D3M_0_6D3M_0</td>\n",
       "      <td>286</td>\n",
       "      <td>285</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2469</th>\n",
       "      <td>TMKIAYLGPSGSFTHNVALHAFPAADLLPFENITEVIKAYESKQVC...</td>\n",
       "      <td>TMKIAYLGPSGSFTHNVALHAFPAADLLPFENITEVIKAYESKQVC...</td>\n",
       "      <td>4LUB_0</td>\n",
       "      <td>4LUB_0</td>\n",
       "      <td>4LUB_0_4LUB_0</td>\n",
       "      <td>188</td>\n",
       "      <td>190</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2470</th>\n",
       "      <td>ERDEVGARKNAVDEEIERLSQPGDQRLNALAERFGGVLLSEIYDDV...</td>\n",
       "      <td>EPVTIVLSQGWVRSAKGHDIDAPGLNYKAGDSFKAAVKGKSNQPVV...</td>\n",
       "      <td>4MN4_2</td>\n",
       "      <td>4MN4_2</td>\n",
       "      <td>4MN4_2_4MN4_2</td>\n",
       "      <td>154</td>\n",
       "      <td>236</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2471</th>\n",
       "      <td>QPVQSPQAVILREGEDAIINCSSSKALYSVHWYRQKHGEAPIFLMI...</td>\n",
       "      <td>AGVAQSPRYKIIEKRQSVAFWCNPISGHATLYWYQQILGQGPKLLI...</td>\n",
       "      <td>5WKF_2</td>\n",
       "      <td>5WKF_2</td>\n",
       "      <td>5WKF_2_5WKF_2</td>\n",
       "      <td>198</td>\n",
       "      <td>244</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2472 rows × 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             seq_target  \\\n",
       "0     SLTKTERTIIVSMWAKISTQADTIGTETLERLFLSHPQTKTYFPHF...   \n",
       "1     TTTLAFKFQHGVIAAVDSRASAGSYISALRVNKVIEINPYLLGTMS...   \n",
       "2     ITHLPPEVMLSIFSYLNPQELCRCSQVSMKWSQLTKTGSLWKHLYP...   \n",
       "3     NAKDVLGLTLLEKTLKERLNLKDAIIVSGDSDQSPWVKKEGRAAVA...   \n",
       "4     DIVMSQSPSSLAVSVGEKVTMSCKSSQSLLYNNNQKNYLAWYQQKP...   \n",
       "...                                                 ...   \n",
       "2467  REIPLKVLVKAVLFACMLMRKTMASRVRVTILFATETGKSEALAWD...   \n",
       "2468  NKYRFIDVQPLTGVLGAEITGVDLREPLDDSTWNEILDAFHTYQVI...   \n",
       "2469  TMKIAYLGPSGSFTHNVALHAFPAADLLPFENITEVIKAYESKQVC...   \n",
       "2470  ERDEVGARKNAVDEEIERLSQPGDQRLNALAERFGGVLLSEIYDDV...   \n",
       "2471  QPVQSPQAVILREGEDAIINCSSSKALYSVHWYRQKHGEAPIFLMI...   \n",
       "\n",
       "                                             seq_binder target_id binder_id  \\\n",
       "0     VHLTDAEKAAVSGLWGKVNADEVGGEALGRLLVVYPWTQRYFDSFG...    1JEB_2    1JEB_2   \n",
       "1     DRGVNTFSPEGRLFQVEYAIEAIKLGSTAIGIQTSEGVCLAVEKRI...   7B12_23   7B12_23   \n",
       "2     PSIKLQSSDGEIFEVDVEIAKQSVTIKTMLEDLGDPVPLPNVNAAI...    6VCD_1    6VCD_1   \n",
       "3     AKDVLGLTLLEKTLKERLNLKDAIIVSGDSDQSPWVKKEGRAAVAC...    2OKG_0    2OKG_0   \n",
       "4     VTLKESGPGILQPSQTLSLTCSFSGFSLSTYGMGVGWIRQPSGKGL...    3MBX_0    3MBX_0   \n",
       "...                                                 ...       ...       ...   \n",
       "2467  QLTEEQIAEFKEAFSLFDKDGDGTITTKELGTVMRSLGQNPTEAEL...    3HR4_0    3HR4_0   \n",
       "2468  KYRFIDVQPLTGVLGAEITGVDLREPLDDSTWNEILDAFHTYQVIY...    6D3M_0    6D3M_0   \n",
       "2469  TMKIAYLGPSGSFTHNVALHAFPAADLLPFENITEVIKAYESKQVC...    4LUB_0    4LUB_0   \n",
       "2470  EPVTIVLSQGWVRSAKGHDIDAPGLNYKAGDSFKAAVKGKSNQPVV...    4MN4_2    4MN4_2   \n",
       "2471  AGVAQSPRYKIIEKRQSVAFWCNPISGHATLYWYQQILGQGPKLLI...    5WKF_2    5WKF_2   \n",
       "\n",
       "     target_binder_id  seq_target_len  seq_binder_len  \n",
       "0       1JEB_2_1JEB_2             141             146  \n",
       "1     7B12_23_7B12_23             203             230  \n",
       "2       6VCD_1_6VCD_1             255             135  \n",
       "3       2OKG_0_2OKG_0             241             243  \n",
       "4       3MBX_0_3MBX_0             220             229  \n",
       "...               ...             ...             ...  \n",
       "2467    3HR4_0_3HR4_0             189             145  \n",
       "2468    6D3M_0_6D3M_0             286             285  \n",
       "2469    4LUB_0_4LUB_0             188             190  \n",
       "2470    4MN4_2_4MN4_2             154             236  \n",
       "2471    5WKF_2_5WKF_2             198             244  \n",
       "\n",
       "[2472 rows x 7 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "PPint_interaactions_df = pd.read_csv(\"/zhome/c9/0/203261/DBL046_PP_osaul/DBL046_PP_osaul/tmp/data/PPint_DB/PPint_interactions.csv\")\n",
    "PPint_interaactions_df[\"seq_target_len\"] = [len(seq) for seq in PPint_interaactions_df[\"seq_target\"].tolist()]\n",
    "PPint_interaactions_df[\"seq_binder_len\"] = [len(seq) for seq in PPint_interaactions_df[\"seq_binder\"].tolist()]\n",
    "# PPint_interaactions_df[\"index\"] = [i for i in PPint_interaactions_df[\"target_binder_id\"].tolist()]\n",
    "PPint_interaactions_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fbf36b61-a8cd-4142-b65f-f32883362e33",
   "metadata": {},
   "outputs": [],
   "source": [
    "Df_val = PPint_interaactions_df.sample(n=round(len(PPint_interaactions_df) * 0.2), random_state=0)\n",
    "Df_train = PPint_interaactions_df.drop(Df_val.index)\n",
    "Df_val[\"dimer\"] = Df_val[\"seq_target\"] == Df_val[\"seq_binder\"]\n",
    "indices_non_dimers_val = Df_val[~Df_val[\"dimer\"]].index.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1f587290-f774-42ba-bc41-b24a0d3ac163",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CLIP_PPint_analysis_dataset(Dataset):\n",
    "    def __init__(\n",
    "        self,\n",
    "        dframe,\n",
    "        paths,\n",
    "        embedding_dim=1280,\n",
    "        padding_value=-5000.0,\n",
    "        fixed_max_blen=None,\n",
    "        fixed_max_tlen=None,\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        self.dframe = dframe.copy()\n",
    "\n",
    "        # either use provided global maxes, or fall back to per-split maxes\n",
    "        if fixed_max_blen is None:\n",
    "            self.max_blen = int(self.dframe[\"seq_binder_len\"].max())\n",
    "        else:\n",
    "            self.max_blen = int(fixed_max_blen)\n",
    "\n",
    "        if fixed_max_tlen is None:\n",
    "            self.max_tlen = int(self.dframe[\"seq_target_len\"].max())\n",
    "        else:\n",
    "            self.max_tlen = int(fixed_max_tlen)\n",
    "\n",
    "        # unpack paths\n",
    "        self.encoding_bpath = paths[0]  # binder embeddings\n",
    "        self.encoding_tpath = paths[1]  # target embeddings\n",
    "        self.contacts_bpath = paths[2]  # binder contact maps\n",
    "        self.contacts_tpath = paths[3]  # target contact maps\n",
    "\n",
    "        self.dframe.set_index(\"target_binder_id\", inplace=True)\n",
    "        self.accessions = self.dframe.index.astype(str).tolist()\n",
    "        self.name_to_row = {name: i for i, name in enumerate(self.accessions)}\n",
    "        self.samples = []\n",
    "\n",
    "        iterator = tqdm(\n",
    "            self.accessions,\n",
    "            position=0,\n",
    "            total=len(self.accessions),\n",
    "            desc=\"#Loading ESM2 embeddings and contacts\"\n",
    "        )\n",
    "\n",
    "        for accession in iterator:\n",
    "            parts = accession.split(\"_\")\n",
    "            if len(parts) < 4:\n",
    "                raise ValueError(\n",
    "                    f\"Expected target_binder_id to have at least 4 underscore-separated parts, got {accession}\"\n",
    "                )\n",
    "\n",
    "            target_id = parts[0] + \"_\" + parts[1]\n",
    "            binder_id = parts[2] + \"_\" + parts[3]\n",
    "\n",
    "            tname = f\"t_{target_id}\"\n",
    "            bname = f\"b_{binder_id}\"\n",
    "\n",
    "            # --- Embeddings ---\n",
    "            tnpy_path = os.path.join(self.encoding_tpath, f\"{tname}.npy\")\n",
    "            bnpy_path = os.path.join(self.encoding_bpath, f\"{bname}.npy\")\n",
    "\n",
    "            if not os.path.exists(tnpy_path):\n",
    "                raise FileNotFoundError(f\"Missing target embedding file: {tnpy_path}\")\n",
    "            if not os.path.exists(bnpy_path):\n",
    "                raise FileNotFoundError(f\"Missing binder embedding file: {bnpy_path}\")\n",
    "\n",
    "            tembd = np.load(tnpy_path)  # [Lt, emb_dim]\n",
    "            bembd = np.load(bnpy_path)  # [Lb, emb_dim]\n",
    "\n",
    "            # pad/crop target embedding to [max_tlen, emb_dim]\n",
    "            if tembd.shape[0] < self.max_tlen:\n",
    "                t_pad_len = self.max_tlen - tembd.shape[0]\n",
    "                t_pad = np.full(\n",
    "                    (t_pad_len, tembd.shape[1]),\n",
    "                    padding_value,\n",
    "                    dtype=tembd.dtype\n",
    "                )\n",
    "                t_final = np.concatenate([tembd, t_pad], axis=0)\n",
    "            else:\n",
    "                t_final = tembd[: self.max_tlen]\n",
    "\n",
    "            # pad/crop binder embedding to [max_blen, emb_dim]\n",
    "            if bembd.shape[0] < self.max_blen:\n",
    "                b_pad_len = self.max_blen - bembd.shape[0]\n",
    "                b_pad = np.full(\n",
    "                    (b_pad_len, bembd.shape[1]),\n",
    "                    padding_value,\n",
    "                    dtype=bembd.dtype\n",
    "                )\n",
    "                b_final = np.concatenate([bembd, b_pad], axis=0)\n",
    "            else:\n",
    "                b_final = bembd[: self.max_blen]\n",
    "\n",
    "            # --- Contact maps ---\n",
    "            tcont_path = os.path.join(self.contacts_tpath, f\"{tname}.npy\")\n",
    "            bcont_path = os.path.join(self.contacts_bpath, f\"{bname}.npy\")\n",
    "\n",
    "            if not os.path.exists(tcont_path):\n",
    "                raise FileNotFoundError(f\"Missing target contact map file: {tcont_path}\")\n",
    "            if not os.path.exists(bcont_path):\n",
    "                raise FileNotFoundError(f\"Missing binder contact map file: {bcont_path}\")\n",
    "\n",
    "            tcont = np.load(tcont_path)  # [Lt, Lt]\n",
    "            bcont = np.load(bcont_path)  # [Lb, Lb]\n",
    "\n",
    "            # pad/crop target contact to [max_tlen, max_tlen]\n",
    "            if tcont.shape[0] < self.max_tlen:\n",
    "                Lt = tcont.shape[0]\n",
    "                tcont_padded = np.full(\n",
    "                    (self.max_tlen, self.max_tlen),\n",
    "                    padding_value,\n",
    "                    dtype=tcont.dtype\n",
    "                )\n",
    "                tcont_padded[:Lt, :Lt] = tcont\n",
    "                tcont_final = tcont_padded\n",
    "            else:\n",
    "                tcont_final = tcont[: self.max_tlen, : self.max_tlen]\n",
    "\n",
    "            # pad/crop binder contact to [max_blen, max_blen]\n",
    "            if bcont.shape[0] < self.max_blen:\n",
    "                Lb = bcont.shape[0]\n",
    "                bcont_padded = np.full(\n",
    "                    (self.max_blen, self.max_blen),\n",
    "                    padding_value,\n",
    "                    dtype=bcont.dtype\n",
    "                )\n",
    "                bcont_padded[:Lb, :Lb] = bcont\n",
    "                bcont_final = bcont_padded\n",
    "            else:\n",
    "                bcont_final = bcont[: self.max_blen, : self.max_blen]\n",
    "\n",
    "            # store: binder_emb, target_emb, binder_contact, target_contact\n",
    "            self.samples.append((b_final, t_final, bcont_final, tcont_final))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.samples)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        b_arr, t_arr, bcont_arr, tcont_arr = self.samples[idx]\n",
    "\n",
    "        b_tensor = torch.tensor(b_arr, dtype=torch.float32)\n",
    "        t_tensor = torch.tensor(t_arr, dtype=torch.float32)\n",
    "        bcont_tensor = torch.tensor(bcont_arr, dtype=torch.float32)\n",
    "        tcont_tensor = torch.tensor(tcont_arr, dtype=torch.float32)\n",
    "\n",
    "        return b_tensor, t_tensor, bcont_tensor, tcont_tensor\n",
    "\n",
    "    def _get_by_name(self, name):\n",
    "        if isinstance(name, str):\n",
    "            idx = self.name_to_row[name]\n",
    "            return self.__getitem__(idx)\n",
    "\n",
    "        binder_list      = []\n",
    "        target_list      = []\n",
    "        binder_contact_l = []\n",
    "        target_contact_l = []\n",
    "\n",
    "        for n in name:\n",
    "            idx = self.name_to_row[n]\n",
    "            b_tensor, t_tensor, bcont_tensor, tcont_tensor = self.__getitem__(idx)\n",
    "            binder_list.append(b_tensor)\n",
    "            target_list.append(t_tensor)\n",
    "            binder_contact_l.append(bcont_tensor)\n",
    "            target_contact_l.append(tcont_tensor)\n",
    "\n",
    "        binder_batch      = torch.stack(binder_list, dim=0)\n",
    "        target_batch      = torch.stack(target_list, dim=0)\n",
    "        binder_contact_b  = torch.stack(binder_contact_l, dim=0)\n",
    "        target_contact_b  = torch.stack(target_contact_l, dim=0)\n",
    "\n",
    "        return binder_batch, target_batch, binder_contact_b, target_contact_b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1cd7f3a9-861a-43d1-a981-c2f3f17279ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "#Loading ESM2 embeddings and contacts: 100%|████████████████████████████████████████████████████████████| 1978/1978 [02:36<00:00, 12.66it/s]\n",
      "#Loading ESM2 embeddings and contacts: 100%|██████████████████████████████████████████████████████████████| 494/494 [00:43<00:00, 11.23it/s]\n"
     ]
    }
   ],
   "source": [
    "bemb_path = \"/zhome/c9/0/203261/DBL046_PP_osaul/DBL046_PP_osaul/tmp/data/PPint_DB/binders_embeddings_esm2\"\n",
    "temb_path = \"/zhome/c9/0/203261/DBL046_PP_osaul/DBL046_PP_osaul/tmp/data/PPint_DB/targets_embeddings_esm2\"\n",
    "\n",
    "## Contact maps paths\n",
    "bcont_path = \"/zhome/c9/0/203261/DBL046_PP_osaul/DBL046_PP_osaul/tmp/data/PPint_DB/binders_contacts\"\n",
    "tcont_path = \"/zhome/c9/0/203261/DBL046_PP_osaul/DBL046_PP_osaul/tmp/data/PPint_DB/targets_contacts\"\n",
    "\n",
    "global_max_blen = int(PPint_interaactions_df[\"seq_binder_len\"].max())\n",
    "global_max_tlen = int(PPint_interaactions_df[\"seq_target_len\"].max())\n",
    "\n",
    "training_Dataset = CLIP_PPint_analysis_dataset(\n",
    "    Df_train,\n",
    "    paths=[bemb_path, temb_path, bcont_path, tcont_path],\n",
    "    embedding_dim=1280,\n",
    "    fixed_max_blen=global_max_blen,\n",
    "    fixed_max_tlen=global_max_tlen,\n",
    ")\n",
    "\n",
    "validation_Dataset = CLIP_PPint_analysis_dataset(\n",
    "    Df_val,\n",
    "    paths=[bemb_path, temb_path, bcont_path, tcont_path],\n",
    "    embedding_dim=1280,\n",
    "    fixed_max_blen=global_max_blen,\n",
    "    fixed_max_tlen=global_max_tlen,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "924f7c11-0201-43ac-8f14-2b83c1ad20cd",
   "metadata": {},
   "source": [
    "Here the global max lengths for binder and target are taken from the whole dataset and are used for padding contact maps, cause later contact maps are parsed though `nn.Linear(input_dim=max_len, output_dim= embed_dimension_esm2=1280)`.\n",
    "\n",
    "Instead of `embed_dimension_esm2 = 1280`, different dimmension coud be used, but in essence all the contact maps eventually should have the same size `dim=1` otherwise, the same transformer cannot be used for all the maps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7079a2a1-e9d1-4202-8aa3-3beb306857ac",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([461, 1280])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accessions = [Df_val.loc[index].target_binder_id for index in indices_non_dimers_val]\n",
    "validation_Dataset._get_by_name(accessions[16])[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8c44f57b-ec35-470e-86bc-9c96e0518470",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_key_padding_mask(x, padding_value=-5000, offset=10):\n",
    "    \"\"\"\n",
    "    x can be:\n",
    "      - embeddings: [B, L, D]\n",
    "      - contact maps: [B, L, L]\n",
    "\n",
    "    Returns:\n",
    "      mask: [B, L] with True at padded positions.\n",
    "    \"\"\"\n",
    "    return (x < (padding_value + offset)).all(dim=-1)\n",
    "\n",
    "def create_mean_of_non_masked(embeddings, padding_mask, input_type=\"encodings\"):\n",
    "    \"\"\"\n",
    "    embeddings:\n",
    "      - if input_type == \"encodings\": [B, L, D]\n",
    "      - if input_type == \"contact map\": [B, L, L]\n",
    "\n",
    "    padding_mask: [B, L] boolean\n",
    "        True  = padded position\n",
    "        False = real residue\n",
    "\n",
    "    returns:\n",
    "      - if encodings: [B, D]\n",
    "      - if contact map: [B, 1]  (scalar per sequence for now)\n",
    "    \"\"\"\n",
    "    seq_embeddings = []\n",
    "\n",
    "    for i in range(embeddings.shape[0]):  # loop over batch\n",
    "        valid_pos = ~padding_mask[i]  # [L], True for real residues\n",
    "\n",
    "        if input_type == \"encodings\":\n",
    "            # embeddings[i]: [L, D]\n",
    "            non_masked_embeddings = embeddings[i][valid_pos]  # [L_real, D]\n",
    "\n",
    "            if non_masked_embeddings.shape[0] == 0:\n",
    "                print(\"You are masking all positions when creating sequence representation (encodings)\")\n",
    "                sys.exit(1)\n",
    "\n",
    "            mean_embedding = non_masked_embeddings.mean(dim=0)  # [D]\n",
    "            seq_embeddings.append(mean_embedding)\n",
    "\n",
    "        elif input_type == \"contact map\":\n",
    "            # embeddings[i]: [L, L] contact map\n",
    "            contact_map = embeddings[i]  # [L, L]\n",
    "\n",
    "            # build pairwise valid mask for this protein\n",
    "            # valid pairs are (p,q) where both residues are real\n",
    "            pair_mask = valid_pos[:, None] & valid_pos[None, :]  # [L, L]\n",
    "\n",
    "            # we only want upper triangle to avoid double-counting\n",
    "            # get indices for i < j\n",
    "            L = contact_map.shape[0]\n",
    "            rows, cols = torch.triu_indices(L, L, offset=1)\n",
    "\n",
    "            # select only pairs that are valid\n",
    "            valid_pair_mask_flat = pair_mask[rows, cols]  # [num_pairs]\n",
    "            if valid_pair_mask_flat.sum() == 0:\n",
    "                print(\"All pairs masked when summarizing contact map\")\n",
    "                sys.exit(1)\n",
    "\n",
    "            contact_vals = contact_map[rows, cols][valid_pair_mask_flat]  # [num_valid_pairs]\n",
    "\n",
    "            # average contact strength among real residues\n",
    "            mean_contact = contact_vals.mean()  # scalar\n",
    "            seq_embeddings.append(mean_contact.unsqueeze(0))  # make it [1] so we can stack later\n",
    "\n",
    "        else:\n",
    "            raise ValueError(f\"Unknown input_type {input_type}\")\n",
    "\n",
    "    return torch.stack(seq_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba656c7b-2e51-4603-853c-9990bce80143",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MiniCLIP_w_transformer_crossattn(pl.LightningModule):\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        padding_value=-5000,\n",
    "        embed_dimension=embedding_dimension,\n",
    "        num_recycles=2,\n",
    "        pep_max_len=None,\n",
    "        prot_max_len=None,\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        if pep_max_len is None or prot_max_len is None:\n",
    "            raise ValueError(\"pep_max_len and prot_max_len must be provided\")\n",
    "            \n",
    "        self.num_recycles = num_recycles # 2\n",
    "        self.padding_value = padding_value # -5000\n",
    "        self.embed_dimension = embed_dimension # 1280\n",
    "\n",
    "        self.logit_scale = nn.Parameter(torch.tensor(math.log(1/0.07)))  # ~CLIP init\n",
    "        \n",
    "        #---------------Using differtent transformer for embeddings and structure---------------#\n",
    "        \n",
    "        ##### Embedding transformer #####\n",
    "\n",
    "        # self attention\n",
    "        self.seq_encoder = nn.TransformerEncoderLayer(\n",
    "            d_model=self.embed_dimension,\n",
    "            nhead=8,\n",
    "            dropout=0.1,\n",
    "            batch_first=True,\n",
    "            dim_feedforward=self.embed_dimension\n",
    "        )\n",
    "\n",
    "        self.norm = nn.LayerNorm(self.embed_dimension)  # For residual additions\n",
    "\n",
    "        # cross attention\n",
    "        self.seq_cross_attn = nn.MultiheadAttention(\n",
    "            embed_dim=self.embed_dimension,\n",
    "            num_heads=8,\n",
    "            dropout=0.1,\n",
    "            batch_first=True\n",
    "        )\n",
    "\n",
    "        ##### Structure/ contact maps transformer #####\n",
    "\n",
    "        # projecting dim=1 to size = self.embed_dimension\n",
    "        self.pep_contact_projector = nn.Linear(in_features=pep_max_len, out_features=self.embed_dimension)\n",
    "        self.prot_contact_projector = nn.Linear(in_features=prot_max_len, out_features=self.embed_dimension)\n",
    "\n",
    "        # self attention\n",
    "        self.struct_encoder = nn.TransformerEncoderLayer(\n",
    "            d_model=self.embed_dimension,\n",
    "            nhead=8,\n",
    "            dropout=0.1,\n",
    "            batch_first=True,\n",
    "            dim_feedforward=self.embed_dimension\n",
    "            )\n",
    "\n",
    "        # cross attention\n",
    "        self.struct_cross_attn = nn.MultiheadAttention(\n",
    "            embed_dim=self.embed_dimension,\n",
    "            num_heads=8,\n",
    "            dropout=0.1,\n",
    "            batch_first=True\n",
    "        )\n",
    "\n",
    "        self.prot_projection_head = nn.Sequential(\n",
    "            nn.Linear(self.embed_dimension, 640),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(640, 320),\n",
    "        )\n",
    "\n",
    "        self.strcut_projection_head = nn.Sequential(\n",
    "            nn.Linear(self.embed_dimension, 640),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(640, 320),\n",
    "        )\n",
    "        \n",
    "    def forward(self, pep_input, prot_input, pep_cm, prot_cm, label=None, pep_int_mask=None, prot_int_mask=None, int_prob=None, mem_save=True): # , pep_tokens, prot_tokens\n",
    "\n",
    "        # masking embeddings\n",
    "        pep_mask_emb = create_key_padding_mask(embeddings=pep_input, padding_value=self.padding_value)\n",
    "        prot_mask_emb = create_key_padding_mask(embeddings=prot_input, padding_value=self.padding_value)\n",
    "\n",
    "        # masking contact maps\n",
    "        pep_mask_cm = create_key_padding_mask(embeddings=pep_cm, padding_value=self.padding_value)\n",
    "        prot_mask_cm = create_key_padding_mask(embeddings=prot_cm, padding_value=self.padding_value)\n",
    " \n",
    "        # Initialize residual states\n",
    "        pep_emb = pep_input.clone()\n",
    "        prot_emb = prot_input.clone()\n",
    "        pep_contacts = pep_cm.clone()\n",
    "        prot_contacts = prot_cm.clone()\n",
    " \n",
    "        for _ in range(self.num_recycles):\n",
    "\n",
    "            # Transformers\n",
    "            # Embedding transformer\n",
    "            pep_trans = self.seq_encoder(self.norm(pep_emb), src_key_padding_mask=pep_mask) # normalization for num stability + self attention\n",
    "            prot_trans = self.seq_encoder(self.norm(prot_emb), src_key_padding_mask=prot_mask)\n",
    "            \n",
    "            # Structure transformer\n",
    "            # binders\n",
    "            pep_contact_proj = self.pep_contact_projector(pep_contacts) # projection so that dim=1 of size 1280\n",
    "            pep_struct_encoded = self.struct_encoder(self.norm_struct(pep_contact_proj), src_key_padding_mask=pep_mask) # normalization for num stability + self attention\n",
    "            # targets\n",
    "            prot_contact_proj = self.prot_contact_projector(prot_contacts)\n",
    "            prot_struct_encoded = self.struct_encoder(self.norm_struct(prot_contact_proj), src_key_padding_mask=prot_mask)\n",
    "\n",
    "            # Cross-attention\n",
    "            pep_cross, _ = self.seq_cross_attn(query=self.norm(pep_trans), key=self.norm(prot_trans), value=self.norm(prot_trans), key_padding_mask=prot_mask)\n",
    "            prot_cross, _ = self.seq_cross_attn(query=self.norm(prot_trans), key=self.norm(pep_trans), value=self.norm(pep_trans), key_padding_mask=pep_mask)\n",
    "            pep_struct_cross, _ = self.struct_cross_attn(query=self.norm(pep_struct_encoded), key=self.norm(prot_struct_encoded), value=self.norm(prot_struct_encoded), key_padding_mask=prot_mask)\n",
    "            prot_struct_cross, _ = self.struct_cross_attn(query=self.norm(prot_struct_encoded), key=self.norm(pep_struct_encoded), value=self.norm(pep_struct_encoded), key_padding_mask=pep_mask)\n",
    "            \n",
    "            # Additive update with residual connection\n",
    "            pep_emb = pep_emb + pep_cross  \n",
    "            prot_emb = prot_emb + prot_cross\n",
    "            pep_contacts = pep_contacts + pep_struct_cross\n",
    "            prot_contacts = prot_contacts + prot_struct_cross\n",
    "\n",
    "        pep_seq_coding = create_mean_of_non_masked(pep_emb, pep_mask)\n",
    "        prot_seq_coding = create_mean_of_non_masked(prot_emb, prot_mask)\n",
    "        pep_contacts_coding = create_mean_of_non_masked(pep_contacts, pep_mask_cm)\n",
    "        prot_contacts_coding = create_mean_of_non_masked(prot_contacts, prot_mask_cm)\n",
    "        \n",
    "        # Use self-attention outputs for embeddings\n",
    "        pep_seq_coding = F.normalize(self.prot_projection_head(pep_seq_coding))\n",
    "        prot_seq_coding = F.normalize(self.prot_projection_head(prot_seq_coding))\n",
    "        pep_contacts_coding = F.normalize(self.strcut_projection_head(pep_contacts_coding))\n",
    "        prot_contacts_coding = F.normalize(self.strcut_projection_head(prot_contacts_coding))\n",
    "\n",
    "        pep_full  = torch.cat([pep_seq_coding,  pep_contacts_coding],  dim=-1)\n",
    "        prot_full = torch.cat([prot_seq_coding, prot_contacts_coding], dim=-1)\n",
    "         \n",
    "        if mem_save:\n",
    "            torch.cuda.empty_cache()\n",
    "        \n",
    "        scale = torch.exp(self.logit_scale).clamp(max=100.0)\n",
    "        logits = scale * (pep_seq_coding * prot_seq_coding).sum(dim=-1)\n",
    "        \n",
    "        return logits\n",
    "\n",
    "    def training_step(self, batch, device):\n",
    "        embedding_pep, embedding_prot = batch\n",
    "        embedding_pep, embedding_prot = embedding_pep.to(device), embedding_prot.to(device)\n",
    "        \n",
    "        positive_logits = self.forward(embedding_pep, embedding_prot)\n",
    "        \n",
    "        # Negative indexes\n",
    "        rows, cols = torch.triu_indices(embedding_prot.size(0), embedding_prot.size(0), offset=1)         \n",
    "        \n",
    "        negative_logits = self(embedding_pep[rows,:,:], \n",
    "                          embedding_prot[cols,:,:], \n",
    "                          int_prob=0.0)\n",
    "\n",
    "        # loss of predicting partner using peptide\n",
    "        positive_loss = F.binary_cross_entropy_with_logits(positive_logits, torch.ones_like(positive_logits).to(device))\n",
    " \n",
    "        # loss of predicting peptide using partner\n",
    "        negative_loss =  F.binary_cross_entropy_with_logits(negative_logits, torch.zeros_like(negative_logits).to(device))\n",
    "        \n",
    "        loss = (positive_loss + negative_loss) / 2\n",
    " \n",
    "        # del partner_prediction_loss, peptide_prediction_loss, embedding_pep, embedding_prot\n",
    "        torch.cuda.empty_cache()\n",
    "        return loss\n",
    "    \n",
    "    def validation_step(self, batch, device):\n",
    "        # Predict on random batches of training batch size\n",
    "        embedding_pep, embedding_prot = batch\n",
    "        embedding_pep, embedding_prot = embedding_pep.to(device), embedding_prot.to(device)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "\n",
    "            positive_logits = self(embedding_pep, embedding_prot)\n",
    "            \n",
    "            # loss of predicting partner using peptide\n",
    "            positive_loss = F.binary_cross_entropy_with_logits(positive_logits, torch.ones_like(positive_logits).to(device))\n",
    "            \n",
    "            # Negaive indexes\n",
    "            rows, cols = torch.triu_indices(embedding_prot.size(0), embedding_prot.size(0), offset=1)\n",
    "            \n",
    "            negative_logits = self(embedding_pep[rows,:,:], embedding_prot[cols,:,:], int_prob=0.0)\n",
    "    \n",
    "            negative_loss =  F.binary_cross_entropy_with_logits(negative_logits, torch.zeros_like(negative_logits).to(device))\n",
    "\n",
    "            loss = (positive_loss + negative_loss) / 2\n",
    "           \n",
    "            logit_matrix = torch.zeros((embedding_pep.size(0),embedding_pep.size(0)),device=self.device)\n",
    "            logit_matrix[rows, cols] = negative_logits\n",
    "            logit_matrix[cols, rows] = negative_logits\n",
    "            \n",
    "            # Fill diagonal with positive scores\n",
    "            diag_indices = torch.arange(embedding_pep.size(0), device=self.device)\n",
    "            logit_matrix[diag_indices, diag_indices] = positive_logits.squeeze()\n",
    "\n",
    "            labels = torch.arange(embedding_prot.size(0)).to(self.device)\n",
    "            peptide_predictions = logit_matrix.argmax(dim=0)\n",
    "            peptide_ranks = logit_matrix.argsort(dim=0).diag() + 1\n",
    "            peptide_mrr = (peptide_ranks).float().pow(-1).mean()\n",
    "            \n",
    "            # partner_accuracy = partner_predictions.eq(labels).float().mean()\n",
    "            peptide_accuracy = peptide_predictions.eq(labels).float().mean()\n",
    "    \n",
    "            k = 3\n",
    "            peptide_topk_accuracy = torch.any((logit_matrix.topk(k, dim=0).indices - labels.reshape(1, -1)) == 0, dim=0).sum() / logit_matrix.shape[0]\n",
    "    \n",
    "            del logit_matrix,positive_logits,negative_logits,embedding_pep,embedding_prot\n",
    "\n",
    "            return loss, peptide_accuracy, peptide_topk_accuracy\n",
    "\n",
    "    def calculate_logit_matrix(self,embedding_pep,embedding_prot):\n",
    "        rows, cols = torch.triu_indices(embedding_pep.size(0), embedding_pep.size(0), offset=1)\n",
    "        \n",
    "        positive_logits = self(embedding_pep, embedding_prot)\n",
    "        negative_logits = self(embedding_pep[rows,:,:], embedding_prot[cols,:,:], int_prob=0.0)\n",
    "        \n",
    "        logit_matrix = torch.zeros((embedding_pep.size(0),embedding_pep.size(0)),device=self.device)\n",
    "        logit_matrix[rows, cols] = negative_logits\n",
    "        logit_matrix[cols, rows] = negative_logits\n",
    "        \n",
    "        diag_indices = torch.arange(embedding_pep.size(0), device=self.device)\n",
    "        logit_matrix[diag_indices, diag_indices] = positive_logits.squeeze()\n",
    "        \n",
    "        return logit_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fadb2503-da3c-413a-b655-9d2d8e4ec911",
   "metadata": {},
   "source": [
    "### Train model from scratch with 10% of PPint dataset using old architecture (encodings only)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24222398-c484-4e4b-9256-987a529c19ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = MiniCLIP_w_transformer_crossattn(\n",
    "    embed_dimension=embedding_dimension,\n",
    "    num_recycles=number_of_recycles,\n",
    "    pep_max_len=global_max_blen,\n",
    "    prot_max_len=global_max_tlen,\n",
    ").to(\"cuda\")\n",
    "\n",
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc2460df-3d34-4548-807d-ca306e006c5d",
   "metadata": {},
   "source": [
    "### Trianing loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b817e401-cca6-44a7-a7d0-8bdf1d51cfec",
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch(iterable, n=1):\n",
    "    \"\"\"Takes any indexable iterable (e.g., a list of observation IDs) and yields contiguous slices of length n.\"\"\"\n",
    "    l = len(iterable)\n",
    "    for ndx in range(0, l, n):\n",
    "        yield iterable[ndx:min(ndx + n, l)]\n",
    "\n",
    "class TrainWrapper():\n",
    "\n",
    "    def __init__(self, \n",
    "                 model, \n",
    "                 training_loader, \n",
    "                 validation_loader,\n",
    "                 test_df,\n",
    "                 test_dataset, \n",
    "                 optimizer, \n",
    "                 EPOCHS, \n",
    "                 runID, \n",
    "                 device, \n",
    "                 test_indexes_for_auROC = None,\n",
    "                 auROC_batch_size=10, \n",
    "                 model_save_steps=False, \n",
    "                 model_save_path=False, \n",
    "                 v=False, \n",
    "                 wandb_tracker=False):\n",
    "        \n",
    "        self.model = model \n",
    "        self.training_loader = training_loader\n",
    "        self.validation_loader = validation_loader\n",
    "        self.EPOCHS = EPOCHS\n",
    "        self.wandb_tracker = wandb_tracker\n",
    "        self.model_save_steps = model_save_steps # if truthy (e.g., 1, 5), save a checkpoint every N epochs.\n",
    "        self.verbose = v\n",
    "        self.best_vloss = 1_000_000\n",
    "        self.optimizer = optimizer\n",
    "        self.runID = runID\n",
    "        self.trained_model_dir = model_save_path\n",
    "        self.print_frequency_loss = 1\n",
    "        self.device = device\n",
    "        self.test_indexes_for_auROC = test_indexes_for_auROC\n",
    "        self.auROC_batch_size = auROC_batch_size # which observation indices to use when computing auROC/auPR periodically.\n",
    "        self.test_dataset = test_dataset\n",
    "        self.test_df = test_df\n",
    "\n",
    "    def train_one_epoch(self):\n",
    "\n",
    "        self.model.train() \n",
    "        running_loss = 0 \n",
    "\n",
    "        for batch in tqdm(self.training_loader, total=len(self.training_loader), desc=\"Running through epoch\"):\n",
    "            \n",
    "            if batch[0].size(0) == 1: \n",
    "                continue\n",
    "            \n",
    "            self.optimizer.zero_grad() \n",
    "            loss = self.model.training_step(batch, self.device) \n",
    "            loss.backward() \n",
    "            self.optimizer.step() \n",
    "\n",
    "            torch.cuda.empty_cache()\n",
    "            running_loss += loss.item()\n",
    "            \n",
    "        return running_loss / len(self.training_loader)\n",
    "    \n",
    "    def calc_auroc_aupr_on_indexes(self, model, dataset, test_df, nondimer_indexes, batch_size = 10):\n",
    "\n",
    "        self.model.eval()\n",
    "        all_TP_scores, all_FP_scores = [], []\n",
    "        accessions = [test_df.loc[index].target_binder_id for index in nondimer_indexes]\n",
    "        batches_local = batch(accessions, n=batch_size)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for index_batch in tqdm(batches_local, total=int(len(accessions)/batch_size), desc=\"Calculating AUC\"):\n",
    "\n",
    "                # Loading the data into the GPU based on the index batch\n",
    "                binder_emb, target_emb = dataset._get_by_name(index_batch)\n",
    "                binder_emb, target_emb = binder_emb.to(self.device), target_emb.to(self.device)\n",
    "\n",
    "                logit_matrix = self.model.calculate_logit_matrix(binder_emb, target_emb)\n",
    "                \n",
    "                TP_scores = logit_matrix.diag().detach().cpu().tolist()\n",
    "                all_TP_scores += TP_scores\n",
    "                \n",
    "                # Get FP scores from upper triangle (excluding diagonal)\n",
    "                n = logit_matrix.size(0)\n",
    "                rows, cols = torch.triu_indices(n, n, offset=1)\n",
    "                FP_scores = logit_matrix[rows, cols].detach().cpu().tolist()\n",
    "                all_FP_scores += FP_scores\n",
    "            \n",
    "        # Calculate scores and labels\n",
    "        all_score_predictions = np.array(all_TP_scores + all_FP_scores)\n",
    "        all_labels = np.array([1]*len(all_TP_scores) + [0]*len(all_FP_scores))\n",
    "                \n",
    "        # Calculate ROC curve metrics\n",
    "        fpr, tpr, thresholds = metrics.roc_curve(all_labels, all_score_predictions)\n",
    "        auroc = metrics.roc_auc_score(all_labels, all_score_predictions)\n",
    "        aupr = metrics.average_precision_score(all_labels, all_score_predictions)\n",
    "        \n",
    "        return auroc, aupr, all_TP_scores, all_FP_scores\n",
    "\n",
    "\n",
    "    def validate (self, dataloader, indexes_for_auc=False, auROC_dataset=False, auROC_df = False):\n",
    "        self.model.eval()\n",
    "        running_loss = 0.0\n",
    "        running_accuracy = 0.0\n",
    "        running_topk_accuracy = 0.0\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for batch in tqdm(dataloader, total=len(dataloader), desc=\"First Validation run\"):\n",
    "                if batch[0].size(0) == 1: # We can't make negatives on a batch of 1\n",
    "                    continue\n",
    "                loss, partner_accuracy, peptide_topk_accuracy = self.model.validation_step(batch, self.device)\n",
    "                running_loss += loss.item()\n",
    "                running_accuracy += partner_accuracy.item()\n",
    "                running_topk_accuracy += peptide_topk_accuracy.item()\n",
    "                \n",
    "            val_loss = running_loss / len(dataloader)\n",
    "            val_accuracy = running_accuracy / len(dataloader)\n",
    "            val_topk_accuracy = running_topk_accuracy / len(dataloader)\n",
    "\n",
    "            if indexes_for_auc:\n",
    "                # Calculating auc-scores\n",
    "                non_dimer_auc, non_dimer_aupr, ___, ___  = self.calc_auroc_aupr_on_indexes(model=self.model, \n",
    "                                                                                           dataset=auROC_dataset,\n",
    "                                                                                           test_df=auROC_df,\n",
    "                                                                                           nondimer_indexes=indexes_for_auc,\n",
    "                                                                                           batch_size=self.auROC_batch_size)\n",
    "                \n",
    "                return val_loss, val_accuracy, val_topk_accuracy, non_dimer_auc, non_dimer_aupr\n",
    "            \n",
    "            else:\n",
    "                return val_loss, val_accuracy, val_topk_accuracy\n",
    "\n",
    "    def train_model(self):\n",
    "\n",
    "        if self.verbose:\n",
    "            print(f\"Training model {str(self.runID)}\")\n",
    "        \n",
    "        # --- initial validation before training\n",
    "        if self.test_indexes_for_auROC:\n",
    "            val_loss_before, val_accuracy_before, val_topk_accuracy_before, val_nondimer_auc_before, val_nondimer_aupr_before = self.validate(\n",
    "                dataloader=self.validation_loader,\n",
    "                indexes_for_auc=self.test_indexes_for_auROC,\n",
    "                auROC_dataset=self.test_dataset,\n",
    "                auROC_df=self.test_df\n",
    "            )\n",
    "        else:\n",
    "            val_loss_before, val_accuracy_before, val_topk_accuracy_before = self.validate(\n",
    "                dataloader=self.validation_loader,\n",
    "                indexes_for_auc=self.test_indexes_for_auROC,\n",
    "                auROC_dataset=self.test_dataset\n",
    "            )\n",
    "            val_nondimer_auc_before = None\n",
    "            val_nondimer_aupr_before = None\n",
    "        \n",
    "        if self.verbose: \n",
    "            if val_nondimer_auc_before is not None:\n",
    "                print(f'Before training - Val CLIP-loss {round(val_loss_before,4)}',\n",
    "                      f'Accuracy: {round(val_accuracy_before,4)}',\n",
    "                      f'Top-K accuracy : {round(val_topk_accuracy_before,4)}',\n",
    "                      f'auc: {round(val_nondimer_auc_before,3)}')\n",
    "            else:\n",
    "                print(f'Before training - Val CLIP-loss {round(val_loss_before,4)}',\n",
    "                      f'Accuracy: {round(val_accuracy_before,4)}',\n",
    "                      f'Top-K accuracy : {round(val_topk_accuracy_before,4)}')\n",
    "\n",
    "        if self.wandb_tracker:\n",
    "            metrics_to_log = {\n",
    "                \"Val-loss\": val_loss_before,\n",
    "                \"Val-acc\": val_accuracy_before,\n",
    "                \"Val-TOPK-acc\": val_topk_accuracy_before,\n",
    "            }\n",
    "            if val_nondimer_auc_before is not None:\n",
    "                metrics_to_log[\"Val non-dimer auc\"] = val_nondimer_auc_before\n",
    "                metrics_to_log[\"Val non-dimer auPR\"] = val_nondimer_aupr_before\n",
    "            self.wandb_tracker.log(metrics_to_log)\n",
    "        \n",
    "        # --- training loop\n",
    "        for epoch in tqdm(range(1, self.EPOCHS + 1), total=self.EPOCHS, desc=\"Epochs\"):\n",
    "            \n",
    "            train_loss = self.train_one_epoch()\n",
    "            \n",
    "            # validation after epoch\n",
    "            if self.test_indexes_for_auROC:\n",
    "                val_loss, val_accuracy, val_topk_accuracy, non_dimer_auc, non_dimer_aupr = self.validate(\n",
    "                    dataloader=self.validation_loader,\n",
    "                    indexes_for_auc=self.test_indexes_for_auROC,\n",
    "                    auROC_dataset=self.test_dataset,\n",
    "                    auROC_df=self.test_df\n",
    "                )\n",
    "            else:\n",
    "                val_loss, val_accuracy, val_topk_accuracy = self.validate(\n",
    "                    dataloader=self.validation_loader,\n",
    "                    indexes_for_auc=self.test_indexes_for_auROC,\n",
    "                    auROC_dataset=self.test_dataset,\n",
    "                    auROC_df=self.test_df\n",
    "                )\n",
    "                non_dimer_auc = None\n",
    "                non_dimer_aupr = None\n",
    "            \n",
    "            # checkpoint save\n",
    "            if self.model_save_steps:\n",
    "                if epoch % self.model_save_steps == 0:\n",
    "                    check_point_folder = os.path.join(\n",
    "                        self.trained_model_dir,\n",
    "                        f\"{str(self.runID)}_checkpoint_{str(epoch)}\"\n",
    "                    )\n",
    "                    if self.verbose:\n",
    "                        print(\"Saving model to:\", check_point_folder)\n",
    "                    \n",
    "                    if not os.path.exists(check_point_folder):\n",
    "                        os.makedirs(check_point_folder)\n",
    "\n",
    "                    checkpoint_path = os.path.join(\n",
    "                        check_point_folder,\n",
    "                        f\"{str(self.runID)}_checkpoint_epoch_{str(epoch)}.pth\"\n",
    "                    )\n",
    "                    torch.save(\n",
    "                        {\n",
    "                            'epoch': epoch,\n",
    "                            'model_state_dict': self.model.state_dict(),\n",
    "                            'optimizer_state_dict': self.optimizer.state_dict(),\n",
    "                            'val_loss': val_loss\n",
    "                        }, \n",
    "                        checkpoint_path\n",
    "                    )\n",
    "            \n",
    "            # console logging\n",
    "            if self.verbose:\n",
    "                if epoch % self.print_frequency_loss == 0:\n",
    "                    if non_dimer_auc is not None:\n",
    "                        print(f'EPOCH {epoch} -  Val loss {round(val_loss,4)}',\n",
    "                              f'Accuracy: {round(val_accuracy,4)}',\n",
    "                              f'Top-K accuracy: {round(val_topk_accuracy,4)}',\n",
    "                              f'Val-Auc:{round(non_dimer_auc,3)}', \n",
    "                              f'Val-auPR:{round(non_dimer_aupr,3)}')\n",
    "                    else:\n",
    "                        print(f'EPOCH {epoch} -  Val loss {round(val_loss,4)}',\n",
    "                              f'Accuracy: {round(val_accuracy,4)}',\n",
    "                              f'Top-K accuracy: {round(val_topk_accuracy,4)}')\n",
    "            \n",
    "            # wandb logging\n",
    "            if self.wandb_tracker:\n",
    "                metrics_to_log_epoch = {\n",
    "                    \"Epoch\": epoch,\n",
    "                    \"Train-loss\": train_loss,\n",
    "                    \"Val-loss\": val_loss,\n",
    "                    \"Val-acc\": val_accuracy,\n",
    "                    \"Val-TOPK-acc\": val_topk_accuracy,\n",
    "                }\n",
    "                if non_dimer_auc is not None:\n",
    "                    metrics_to_log_epoch[\"Val non-dimer auc\"] = non_dimer_auc\n",
    "                    metrics_to_log_epoch[\"Val non-dimer auPR\"] = non_dimer_aupr\n",
    "                self.wandb_tracker.log(metrics_to_log_epoch)\n",
    "\n",
    "        if self.wandb_tracker:\n",
    "            self.wandb_tracker.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c09c80cc-d9fa-4fec-baef-e6c76311a946",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 10\n",
    "learning_rate = 2e-5\n",
    "EPOCHS = 15\n",
    "g = torch.Generator().manual_seed(SEED)\n",
    "\n",
    "# login once (env var preferred)\n",
    "if use_wandb:\n",
    "    import wandb\n",
    "    wandb.login()\n",
    "\n",
    "optimizer = AdamW(model.parameters(), lr=learning_rate)\n",
    "train_dataloader = DataLoader(training_Dataset, batch_size=10)\n",
    "val_dataloader = DataLoader(validation_Dataset, batch_size=20, shuffle=False, drop_last = False)\n",
    "\n",
    "# accelerator\n",
    "accelerator = Accelerator()\n",
    "device = accelerator.device\n",
    "model, optimizer, train_dataloader, val_dataloader = accelerator.prepare(model, optimizer, train_dataloader, val_dataloader)\n",
    "\n",
    "# wandb\n",
    "if use_wandb:\n",
    "    run = wandb.init(\n",
    "        project=\"PPint_retrain_w_10percent_ofdata\",\n",
    "        name=f\"PPint_retrain\",\n",
    "        config={\"learning_rate\": learning_rate, \"batch_size\": batch_size, \"epochs\": EPOCHS,\n",
    "                \"architecture\": \"MiniCLIP_w_transformer_crossattn\", \"dataset\": \"Meta analysis\"},\n",
    "    )\n",
    "    wandb.watch(accelerator.unwrap_model(model), log=\"all\", log_freq=100)\n",
    "else:\n",
    "    run = None\n",
    "\n",
    "# train\n",
    "training_wrapper = TrainWrapper(model=model, \n",
    "                                training_loader = train_dataloader, \n",
    "                                validation_loader = val_dataloader, \n",
    "                                test_dataset = validation_Dataset,\n",
    "                                test_df = Df_val,\n",
    "                                optimizer = optimizer, \n",
    "                                EPOCHS = EPOCHS,\n",
    "                                runID = runID, \n",
    "                                device = device, \n",
    "                                test_indexes_for_auROC = indices_non_dimers_val, \n",
    "                                model_save_steps = model_save_steps,\n",
    "                                model_save_path = trained_model_dir, \n",
    "                                v = True, \n",
    "                                wandb_tracker = wandb\n",
    "                                )\n",
    "\n",
    "training_wrapper.train_model()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
