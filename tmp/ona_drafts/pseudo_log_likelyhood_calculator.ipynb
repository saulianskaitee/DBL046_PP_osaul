{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93271887-21c3-4fd2-a77e-9f4f7520aa9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import sys\n",
    "sys.path.append(\"/novo/users/cpjb/rp0689_lm_finetune/env/pLM_FT/lib/python3.10/site-packages\")\n",
    "import esm\n",
    "import warnings\n",
    "import random\n",
    "import numpy as np "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb4334f3-2809-4ba4-a153-2185723ddd83",
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch(iterable, n=1):\n",
    "    l = len(iterable)\n",
    "    for ndx in range(0, l, n):\n",
    "        yield iterable[ndx:min(ndx + n, l)]\n",
    "\n",
    "class ESMPseudoPerplexity:\n",
    "    \"\"\"\n",
    "    A class to calculate the pseudo-perplexity of protein sequences using the ESM-2 model.\n",
    "\n",
    "    This class loads a pre-trained ESM-2 model upon initialization and provides a method\n",
    "    to calculate the pseudo-log-likelihood (pLL) and pseudo-perplexity for any\n",
    "    given protein sequence. This approach is efficient for scoring multiple sequences\n",
    "    as the model is loaded only once.\n",
    "\n",
    "    Attributes:\n",
    "        model: The loaded ESM-2 model.\n",
    "        alphabet: The alphabet used by the ESM-2 model.\n",
    "        batch_converter: A utility to convert sequences to batched tensors.\n",
    "        device: The computing device (CUDA or CPU) where the model is located.\n",
    "\n",
    "    USAGE:\n",
    "\n",
    "        # In a new script or notebook:\n",
    "        # from ESM2_utils import ESMPseudoPerplexity\n",
    "\n",
    "        # # Initialize the calculator (loads the model)\n",
    "        # calculator = ESMPseudoPerplexity()\n",
    "\n",
    "        # # Calculate the pLL score for any sequence\n",
    "        # my_sequence = \"MKALIVLGLVLLSVTVQGKVQ\"\n",
    "        # score = calculator.calculate_pll_score(my_sequence)\n",
    "\n",
    "    # print(f\"The pseudo-perplexity score for '{my_sequence}' is: {score:.4f}\")\n",
    "    \"\"\"\n",
    "    def __init__(self, model_name: str = 'esm2_t33_650M_UR50D'):\n",
    "        \"\"\"\n",
    "        Initializes the model, alphabet, and batch converter.\n",
    "\n",
    "        Args:\n",
    "            model_name (str): The name of the pre-trained ESM-2 model to load.\n",
    "        \"\"\"\n",
    "        self.mask_str_token = \"<mask>\"\n",
    "        self.model, self.alphabet = esm.pretrained.load_model_and_alphabet(model_name)\n",
    "        self.batch_converter = self.alphabet.get_batch_converter()\n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        self.model = self.model.to(self.device)\n",
    "        self.model.eval()  # Disable dropout for deterministic results\n",
    "\n",
    "        print(f\"ESM-2 model '{model_name}' loaded on {self.device}.\")\n",
    "\n",
    "    def _generate_masked_sequences(self, sequence: str, mask_length: list):\n",
    "        \"\"\"\n",
    "        Generates sequences with a sliding window of masks of length N.\n",
    "\n",
    "        Args:\n",
    "            sequence (str): The input protein sequence.\n",
    "            mask_length (int): The number of adjacent tokens to mask (N).\n",
    "\n",
    "        Yields:\n",
    "            A tuple containing the masked sequence string, the start index of the\n",
    "            mask, and the end index of the mask.\n",
    "        \"\"\"\n",
    "        np.random.seed(0)\n",
    "\n",
    "        seq_indexes = np.arange(0,len(sequence))\n",
    "        np.random.shuffle(seq_indexes)\n",
    "        batched_seq_indexes = list(batch(seq_indexes,n=mask_length))\n",
    "\n",
    "        all_sequences = []\n",
    "        for masked_index in batched_seq_indexes:\n",
    "            seq_copy = list(sequence).copy()\n",
    "            for index in masked_index:\n",
    "                seq_copy[index] = self.mask_str_token\n",
    "            \n",
    "            all_sequences.append((masked_index,\"\".join(seq_copy)))\n",
    "        \n",
    "        return all_sequences\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def calculate_pll_score(self, sequence: str, mask_length: int = 1) -> float:\n",
    "        \"\"\"\n",
    "        Calculates a score based on the model's ability to predict residues\n",
    "        within a sliding window of N masked tokens.\n",
    "\n",
    "        When mask_length=1, this is equivalent to standard pseudo-log-likelihood (pLL).\n",
    "\n",
    "        Args:\n",
    "            sequence (str): The input protein sequence.\n",
    "            mask_length (int): The length of the mask stretch (N).\n",
    "            batch_size (int): The number of masked sequences to process in each batch.\n",
    "\n",
    "        Returns:\n",
    "            float: The calculated pseudo-perplexity-like score.\n",
    "        \"\"\"\n",
    "        if not sequence or not isinstance(sequence, str):\n",
    "            raise ValueError(\"Input sequence must be a non-empty string.\")\n",
    "\n",
    "        # Set seed for suffling\n",
    "        np.random.seed(0)\n",
    "                \n",
    "        # 1. Generate all sequences with sliding window masks\n",
    "        masked_data = self._generate_masked_sequences(sequence, mask_length)\n",
    "\n",
    "        # ESM-2 input\n",
    "        ESM_input = [(i, masked_seq[1]) for i, masked_seq in enumerate(masked_data)]\n",
    "     \n",
    "        # 2. Convert to batches\n",
    "        batch_labels, batch_strs, batch_tokens = self.batch_converter(ESM_input)\n",
    "        batch_lens = (batch_tokens != self.alphabet.padding_idx).sum(1)\n",
    "        batch_tokens = batch_tokens.to(self.device)\n",
    "        \n",
    "        # 3. Pass through ESM-2\n",
    "        results = self.model(batch_tokens, repr_layers=[33], return_contacts=False)\n",
    "        logits = results[\"logits\"]\n",
    "        logit_prob = torch.nn.functional.log_softmax(logits, dim=-1)\n",
    "\n",
    "        # 4. Calcualte PLL\n",
    "        #     \n",
    "        log_likelihood = 0\n",
    "        for i,(masked_index, _) in enumerate(masked_data):\n",
    "            for j in masked_index:\n",
    "                log_likelihood += logit_prob[i, j+1, self.alphabet.get_idx(sequence[j])]\n",
    "        \n",
    "        # Calculate the average log likelihood per token\n",
    "        avg_log_likelihood = log_likelihood / len(sequence)\n",
    "\n",
    "        # Compute and return the pseudo-perplexity\n",
    "        pll = torch.exp(-torch.tensor(avg_log_likelihood)).item()\n",
    "        return pll"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abbf28ce-4fec-4031-aa9d-6061117d7e6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This block demonstrates how to use the class.\n",
    "# It will only run when the script is executed directly.\n",
    "if __name__ == '__main__':\n",
    "    try:\n",
    "        # 1. Initialize the calculator once. The model is loaded here.\n",
    "        pll_calculator = ESMPseudoPerplexity()\n",
    "        # 2. Provide a sequence to calculate its score\n",
    "        # Using a truncated FGF10_HUMAN sequence as an example\n",
    "        print(\"\\nCalculating pLL score for a natural protein...\")\n",
    "        natural_protein_score = pll_calculator.calculate_pll_score(human_protein_sequence)\n",
    "\n",
    "        print(f\"-> Pseudo-Perplexity for natural protein: {natural_protein_score:.4f}\")\n",
    "        # Example with a non-natural (poly-Alanine) sequence\n",
    "        non_natural_sequence = \"A\" * 80\n",
    "        print(\"\\nCalculating pLL score for a non-natural sequence...\")\n",
    "        non_natural_score = pll_calculator.calculate_pll_score(non_natural_sequence)\n",
    "        print(f\"-> Pseudo-Perplexity for non-natural protein: {non_natural_score:.4f}\")\n",
    "\n",
    "        # Example of scoring multiple sequences efficiently\n",
    "        print(\"\\nCalculating pLL for a list of short peptides...\")\n",
    "        sequences_to_score = [\"MKYKL\", \"VLLLE\", \"AGVTV\"]\n",
    "        for seq in sequences_to_score:\n",
    "            score = pll_calculator.calculate_pll_score(seq)\n",
    "            print(f\"- pLL for '{seq}': {score:.4f}\")\n",
    "\n",
    "    except ImportError:\n",
    "        warnings.warn(\"Please install the 'fair-esm' library to run this script. Use: pip install fair-esm\")\n",
    "    except Exception as e:\n",
    "        print(f\"An unexpected error occurred: {e}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
