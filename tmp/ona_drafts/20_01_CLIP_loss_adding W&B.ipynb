{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7f838074-e829-40d5-9b15-949a9bfe9708",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/work3/s232958/envs/esm_cuda/lib/python3.10/site-packages/transformers/utils/generic.py:441: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.\n",
      "  _torch_pytree._register_pytree_node(\n",
      "/work3/s232958/envs/esm_cuda/lib/python3.10/site-packages/transformers/utils/generic.py:309: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.\n",
      "  _torch_pytree._register_pytree_node(\n",
      "/work3/s232958/envs/esm_cuda/lib/python3.10/site-packages/transformers/utils/generic.py:309: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.\n",
      "  _torch_pytree._register_pytree_node(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tesla V100-SXM2-32GB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/work3/s232958/envs/esm_cuda/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: 'Could not load this library: /work3/s232958/envs/esm_cuda/lib/python3.10/site-packages/torchvision/image.so'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?\n",
      "  warn(\n"
     ]
    }
   ],
   "source": [
    "import uuid, sys, os\n",
    "import requests\n",
    "import random\n",
    "import pandas as pd\n",
    "import math\n",
    "import numpy as np\n",
    "from numpy.linalg import norm\n",
    "from tqdm import tqdm, trange\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import metrics\n",
    "from scipy import stats\n",
    "\n",
    "from transformers import EsmModel, AutoTokenizer # huggingface\n",
    "import esm\n",
    "\n",
    "import torch\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
    "os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"expandable_segments:True\"\n",
    "torch.cuda.set_device(0)  # 0 == \"first visible\" -> actually GPU 2 on the node\n",
    "torch.manual_seed(0)\n",
    "print(torch.cuda.get_device_name(0))\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader, TensorDataset, WeightedRandomSampler\n",
    "import pytorch_lightning as pl\n",
    "from torch.optim import AdamW\n",
    "from accelerate import Accelerator\n",
    "import training_utils.partitioning_utils as pat_utils\n",
    "from torch.utils.checkpoint import checkpoint\n",
    "\n",
    "# LoRA\n",
    "from peft import LoraConfig, get_peft_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c5d0770f-c901-45f7-b5e7-801a2f8f261b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch: 2.9.1+cu128\n",
      "Using device: cuda\n",
      "Current location: /zhome/c9/0/203261/DBL046_PP_osaul/DBL046_PP_osaul/tmp/ona_drafts\n"
     ]
    }
   ],
   "source": [
    "print(\"PyTorch:\", torch.__version__)\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print('Using device:', device)\n",
    "print(\"Current location:\", os.getcwd())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1da12c27-b0c9-4166-bf60-a7e182e83805",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /zhome/c9/0/203261/.netrc\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33ms232958\u001b[0m (\u001b[33ms232958-danmarks-tekniske-universitet-dtu\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "requests.get(\"https://api.wandb.ai/status\").status_code\n",
    "\n",
    "import wandb\n",
    "wandb.login(key=\"f8a6d759fe657b095d56bddbdb4d586dfaebd468\", relogin=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a771b897-1647-4594-b648-41d2639d5282",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Setting a seed to have the same initiation of weights\n",
    "\n",
    "def set_seed(seed: int = 0):\n",
    "    # Python & NumPy\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    \n",
    "    # PyTorch\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)  # for multi-GPU\n",
    "\n",
    "    # CuDNN settings (for convolution etc.)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "    # (Optional) for some Python hashing randomness\n",
    "    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n",
    "\n",
    "SEED = 0\n",
    "set_seed(SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fcfc6c69-e492-4bf1-a23e-e8ee41a27d4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model parameters\n",
    "memory_verbose = False\n",
    "use_wandb = True # Used to track loss in real-time without printing\n",
    "\n",
    "seq_embed_dimension = 1280 #| 960 | 1152\n",
    "# struct_embed_dimension = 256\n",
    "number_of_recycles = 2\n",
    "padding_value = -5000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b152a01f-a72c-4d75-9e1c-6b6f937515eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total memory:  34.072559616\n",
      "Reserved memory:  0.0\n",
      "Allocated memory:  0.0\n",
      "Free memory:  0.0\n"
     ]
    }
   ],
   "source": [
    "# ## Training variables\n",
    "runID = uuid.uuid4()\n",
    "\n",
    "def print_mem_consumption():\n",
    "    # 1. Total memory available on the GPU (device 0)\n",
    "    t = torch.cuda.get_device_properties(0).total_memory\n",
    "    # 2. How much memory PyTorch has *reserved* from CUDA\n",
    "    r = torch.cuda.memory_reserved(0)\n",
    "    # 3. How much of that reserved memory is actually *used* by tensors\n",
    "    a = torch.cuda.memory_allocated(0)\n",
    "    # 4. Reserved but not currently allocated (so “free inside PyTorch’s pool”)\n",
    "    f = r - a\n",
    "\n",
    "    print(\"Total memory: \", t/1e9)      # total VRAM in GB\n",
    "    print(\"Reserved memory: \", r/1e9)   # PyTorch’s reserved pool in GB\n",
    "    print(\"Allocated memory: \", a//1e9) # actually in use (integer division)\n",
    "    print(\"Free memory: \", f/1e9)       # slack in the reserved pool in GB\n",
    "print_mem_consumption()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b001f70e-d769-4c9e-ad66-6e6c919a0a9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_key_padding_mask(embeddings, padding_value=-5000, offset=10):\n",
    "    return (embeddings < (padding_value + offset)).all(dim=-1)\n",
    "\n",
    "def create_mean_of_non_masked(embeddings, padding_mask):\n",
    "    # Use masked select and mean to compute the mean of non-masked elements\n",
    "    # embeddings should be of shape (batch_size, seq_len, features)\n",
    "    seq_embeddings = []\n",
    "    for i in range(embeddings.shape[0]): # looping over all batch elements\n",
    "        non_masked_embeddings = embeddings[i][~padding_mask[i]] # shape [num_real_tokens, features]\n",
    "        if len(non_masked_embeddings) == 0:\n",
    "            print(\"You are masking all positions when creating sequence representation\")\n",
    "            sys.exit(1)\n",
    "        mean_embedding = non_masked_embeddings.mean(dim=0) # sequence is represented by the single vecotr [1152] [features]\n",
    "        seq_embeddings.append(mean_embedding)\n",
    "    return torch.stack(seq_embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e349b030-2126-46c1-bb65-8d25e7fb6c0e",
   "metadata": {},
   "source": [
    "### Loading Data for Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1277c85e-2ea7-44f6-8523-8b1362106edb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>interface_id</th>\n",
       "      <th>PDB</th>\n",
       "      <th>ID1</th>\n",
       "      <th>ID2</th>\n",
       "      <th>seq_target</th>\n",
       "      <th>seq_target_len</th>\n",
       "      <th>seq_pdb_target</th>\n",
       "      <th>pdb_target_len</th>\n",
       "      <th>target_chain</th>\n",
       "      <th>seq_binder</th>\n",
       "      <th>seq_binder_len</th>\n",
       "      <th>seq_pdb_binder</th>\n",
       "      <th>pdb_binder_len</th>\n",
       "      <th>binder_chain</th>\n",
       "      <th>pdb_path</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>6IDB_0</td>\n",
       "      <td>6IDB</td>\n",
       "      <td>6IDB_0_A</td>\n",
       "      <td>6IDB_0_B</td>\n",
       "      <td>DKICLGHHAVSNGTKVNTLTERGVEVVNATETVERTNIPRICSKGK...</td>\n",
       "      <td>317</td>\n",
       "      <td>DKICLGHHAVSNGTKVNTLTERGVEVVNATETVERTNIPRICSKGK...</td>\n",
       "      <td>317</td>\n",
       "      <td>6IDB_A</td>\n",
       "      <td>GLFGAIAGFIENGWEGLIDGWYGFRHQNAQGEGTAADYKSTQSAID...</td>\n",
       "      <td>172</td>\n",
       "      <td>GLFGAIAGFIENGWEGLIDGWYGFRHQNAQGEGTAADYKSTQSAID...</td>\n",
       "      <td>172</td>\n",
       "      <td>6IDB_B</td>\n",
       "      <td>6idb.pdb.gz</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2WZP_3</td>\n",
       "      <td>2WZP</td>\n",
       "      <td>2WZP_3_D</td>\n",
       "      <td>2WZP_3_G</td>\n",
       "      <td>VQLQESGGGLVQAGGSLRLSCTASRRTGSNWCMGWFRQLAGKEPEL...</td>\n",
       "      <td>122</td>\n",
       "      <td>VQLQESGGGLVQAGGSLRLSCTASRRTGSNWCMGWFRQLAGKEPEL...</td>\n",
       "      <td>122</td>\n",
       "      <td>2WZP_D</td>\n",
       "      <td>TIKNFTFFSPNSTEFPVGSNNDGKLYMMLTGMDYRTIRRKDWSSPL...</td>\n",
       "      <td>266</td>\n",
       "      <td>TIKNFTFFSPNSTEFPVGSNNDGKLYMMLTGMDYRTIRRKDWSSPL...</td>\n",
       "      <td>266</td>\n",
       "      <td>2WZP_G</td>\n",
       "      <td>2wzp.pdb.gz</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1ZKP_0</td>\n",
       "      <td>1ZKP</td>\n",
       "      <td>1ZKP_0_A</td>\n",
       "      <td>1ZKP_0_C</td>\n",
       "      <td>LYFQSNAKTVVGFWGGFPEAGEATSGYLFEHDGFRLLVDCGSGVLA...</td>\n",
       "      <td>246</td>\n",
       "      <td>LYFQSNAMKMTVVGFWGGFPEAGEATSGYLFEHDGFRLLVDCGSGV...</td>\n",
       "      <td>251</td>\n",
       "      <td>1ZKP_A</td>\n",
       "      <td>AKTVVGFWGGFPEAGEATSGYLFEHDGFRLLVDCGSGVLAQLQKYI...</td>\n",
       "      <td>240</td>\n",
       "      <td>AMKMTVVGFWGGFPEAGEATSGYLFEHDGFRLLVDCGSGVLAQLQK...</td>\n",
       "      <td>245</td>\n",
       "      <td>1ZKP_C</td>\n",
       "      <td>1zkp.pdb.gz</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>6GRH_3</td>\n",
       "      <td>6GRH</td>\n",
       "      <td>6GRH_3_C</td>\n",
       "      <td>6GRH_3_D</td>\n",
       "      <td>SKHELSLVEVTHYTDPEVLAIVKDFHVRGNFASLPEFAERTFVSAV...</td>\n",
       "      <td>266</td>\n",
       "      <td>SKHELSLVEVTHYTDPEVLAIVKDFHVRGNFASLPEFAERTFVSAV...</td>\n",
       "      <td>266</td>\n",
       "      <td>6GRH_C</td>\n",
       "      <td>MINVYSNLMSAWPATMAMSPKLNRNMPTFSQIWDYERITPASAAGE...</td>\n",
       "      <td>396</td>\n",
       "      <td>MINVYSNLMSAWPATMAMSPKLNRNMPTFSQIWDYERITPASAAGE...</td>\n",
       "      <td>396</td>\n",
       "      <td>6GRH_D</td>\n",
       "      <td>6grh.pdb.gz</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>8R57_1</td>\n",
       "      <td>8R57</td>\n",
       "      <td>8R57_1_M</td>\n",
       "      <td>8R57_1_f</td>\n",
       "      <td>DLMTALQLVMKKSSAHDGLVKGLREAAKAIEKHAAQICVLAEDCDQ...</td>\n",
       "      <td>118</td>\n",
       "      <td>DLMTALQLVMKKSSAHDGLVKGLREAAKAIEKHAAQICVLAEDCDQ...</td>\n",
       "      <td>118</td>\n",
       "      <td>8R57_M</td>\n",
       "      <td>PKKQKHKHKKVKLAVLQFYKVDDATGKVTRLRKECPNADCGAGTFM...</td>\n",
       "      <td>64</td>\n",
       "      <td>PKKQKHKHKKVKLAVLQFYKVDDATGKVTRLRKECPNADCGAGTFM...</td>\n",
       "      <td>64</td>\n",
       "      <td>8R57_f</td>\n",
       "      <td>8r57.pdb.gz</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1972</th>\n",
       "      <td>4YO8_0</td>\n",
       "      <td>4YO8</td>\n",
       "      <td>4YO8_0_A</td>\n",
       "      <td>4YO8_0_B</td>\n",
       "      <td>HENLYFQGVQKIGILGAMREEITPILELFGVDFEEIPLGGNVFHKG...</td>\n",
       "      <td>238</td>\n",
       "      <td>HENLYFQGVQKIGILGAMREEITPILELFGVDFEEIPLGGNVFHKG...</td>\n",
       "      <td>238</td>\n",
       "      <td>4YO8_A</td>\n",
       "      <td>HHHHHENLYFQGVQKIGILGAMREEITPILELFGVDFEEIPLGGNV...</td>\n",
       "      <td>242</td>\n",
       "      <td>HHHHHENLYFQGVQKIGILGAMREEITPILELFGVDFEEIPLGGNV...</td>\n",
       "      <td>242</td>\n",
       "      <td>4YO8_B</td>\n",
       "      <td>4yo8.pdb.gz</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1973</th>\n",
       "      <td>3CKI_0</td>\n",
       "      <td>3CKI</td>\n",
       "      <td>3CKI_0_A</td>\n",
       "      <td>3CKI_0_B</td>\n",
       "      <td>DPMKNTCKLLVVADHRFYRYMGRGEESTTTNYLIELIDRVDDIYRN...</td>\n",
       "      <td>256</td>\n",
       "      <td>DPMKNTCKLLVVADHRFYRYMGRGEESTTTNYLIELIDRVDDIYRN...</td>\n",
       "      <td>256</td>\n",
       "      <td>3CKI_A</td>\n",
       "      <td>CTCSPSHPQDAFCNSDIVIRAKVVGKKLVKEGPFGTLVYTIKQMKM...</td>\n",
       "      <td>121</td>\n",
       "      <td>CTCSPSHPQDAFCNSDIVIRAKVVGKKLVKEGPFGTLVYTIKQMKM...</td>\n",
       "      <td>121</td>\n",
       "      <td>3CKI_B</td>\n",
       "      <td>3cki.pdb.gz</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1974</th>\n",
       "      <td>7MHY_1</td>\n",
       "      <td>7MHY</td>\n",
       "      <td>7MHY_1_M</td>\n",
       "      <td>7MHY_1_N</td>\n",
       "      <td>QVQLRQSGAELAKPGASVKMSCKASGYTFTNYWLHWIKQRPGQGLE...</td>\n",
       "      <td>118</td>\n",
       "      <td>QVQLRQSGAELAKPGASVKMSCKASGYTFTNYWLHWIKQRPGQGLE...</td>\n",
       "      <td>118</td>\n",
       "      <td>7MHY_M</td>\n",
       "      <td>DVLMTQTPLSLPVSLGDQVSISCRSSQSIVHNTYLEWYLQKPGQSP...</td>\n",
       "      <td>109</td>\n",
       "      <td>DVLMTQTPLSLPVSLGDQVSISCRSSQSIVHNTYLEWYLQKPGQSP...</td>\n",
       "      <td>109</td>\n",
       "      <td>7MHY_N</td>\n",
       "      <td>7mhy.pdb.gz</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1975</th>\n",
       "      <td>7MHY_2</td>\n",
       "      <td>7MHY</td>\n",
       "      <td>7MHY_2_O</td>\n",
       "      <td>7MHY_2_P</td>\n",
       "      <td>IQLVQSGPELVKISCKASGYTFTNYGMNWVRQAPGKGLKWMGWINT...</td>\n",
       "      <td>100</td>\n",
       "      <td>IQLVQSGPELVKISCKASGYTFTNYGMNWVRQAPGKGLKWMGWINT...</td>\n",
       "      <td>100</td>\n",
       "      <td>7MHY_O</td>\n",
       "      <td>VLMTQTPLSLPVSISCRSSQSIVHSNGNTYLEWYLQKPGQSPKLLI...</td>\n",
       "      <td>94</td>\n",
       "      <td>VLMTQTPLSLPVSISCRSSQSIVHSNGNTYLEWYLQKPGQSPKLLI...</td>\n",
       "      <td>94</td>\n",
       "      <td>7MHY_P</td>\n",
       "      <td>7mhy.pdb.gz</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1976</th>\n",
       "      <td>6O42_0</td>\n",
       "      <td>6O42</td>\n",
       "      <td>6O42_0_L</td>\n",
       "      <td>6O42_0_H</td>\n",
       "      <td>EIVLTQSPGTLSLSPGERATLSCRASQSVSSSYLAWYQQKPGQAPR...</td>\n",
       "      <td>214</td>\n",
       "      <td>EIVLTQSPGTLSLSPGERATLSCRASQSVSSSYLAWYQQKPGQAPR...</td>\n",
       "      <td>214</td>\n",
       "      <td>6O42_L</td>\n",
       "      <td>QVQLVQSGAEVKKPGSSVKVSCKASGGTFSSYAISWVRQAPGQGLE...</td>\n",
       "      <td>220</td>\n",
       "      <td>QVQLVQSGAEVKKPGSSVKVSCKASGGTFSSYAISWVRQAPGQGLE...</td>\n",
       "      <td>220</td>\n",
       "      <td>6O42_H</td>\n",
       "      <td>6o42.pdb.gz</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1977 rows × 15 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     interface_id   PDB       ID1       ID2  \\\n",
       "0          6IDB_0  6IDB  6IDB_0_A  6IDB_0_B   \n",
       "1          2WZP_3  2WZP  2WZP_3_D  2WZP_3_G   \n",
       "2          1ZKP_0  1ZKP  1ZKP_0_A  1ZKP_0_C   \n",
       "3          6GRH_3  6GRH  6GRH_3_C  6GRH_3_D   \n",
       "4          8R57_1  8R57  8R57_1_M  8R57_1_f   \n",
       "...           ...   ...       ...       ...   \n",
       "1972       4YO8_0  4YO8  4YO8_0_A  4YO8_0_B   \n",
       "1973       3CKI_0  3CKI  3CKI_0_A  3CKI_0_B   \n",
       "1974       7MHY_1  7MHY  7MHY_1_M  7MHY_1_N   \n",
       "1975       7MHY_2  7MHY  7MHY_2_O  7MHY_2_P   \n",
       "1976       6O42_0  6O42  6O42_0_L  6O42_0_H   \n",
       "\n",
       "                                             seq_target  seq_target_len  \\\n",
       "0     DKICLGHHAVSNGTKVNTLTERGVEVVNATETVERTNIPRICSKGK...             317   \n",
       "1     VQLQESGGGLVQAGGSLRLSCTASRRTGSNWCMGWFRQLAGKEPEL...             122   \n",
       "2     LYFQSNAKTVVGFWGGFPEAGEATSGYLFEHDGFRLLVDCGSGVLA...             246   \n",
       "3     SKHELSLVEVTHYTDPEVLAIVKDFHVRGNFASLPEFAERTFVSAV...             266   \n",
       "4     DLMTALQLVMKKSSAHDGLVKGLREAAKAIEKHAAQICVLAEDCDQ...             118   \n",
       "...                                                 ...             ...   \n",
       "1972  HENLYFQGVQKIGILGAMREEITPILELFGVDFEEIPLGGNVFHKG...             238   \n",
       "1973  DPMKNTCKLLVVADHRFYRYMGRGEESTTTNYLIELIDRVDDIYRN...             256   \n",
       "1974  QVQLRQSGAELAKPGASVKMSCKASGYTFTNYWLHWIKQRPGQGLE...             118   \n",
       "1975  IQLVQSGPELVKISCKASGYTFTNYGMNWVRQAPGKGLKWMGWINT...             100   \n",
       "1976  EIVLTQSPGTLSLSPGERATLSCRASQSVSSSYLAWYQQKPGQAPR...             214   \n",
       "\n",
       "                                         seq_pdb_target  pdb_target_len  \\\n",
       "0     DKICLGHHAVSNGTKVNTLTERGVEVVNATETVERTNIPRICSKGK...             317   \n",
       "1     VQLQESGGGLVQAGGSLRLSCTASRRTGSNWCMGWFRQLAGKEPEL...             122   \n",
       "2     LYFQSNAMKMTVVGFWGGFPEAGEATSGYLFEHDGFRLLVDCGSGV...             251   \n",
       "3     SKHELSLVEVTHYTDPEVLAIVKDFHVRGNFASLPEFAERTFVSAV...             266   \n",
       "4     DLMTALQLVMKKSSAHDGLVKGLREAAKAIEKHAAQICVLAEDCDQ...             118   \n",
       "...                                                 ...             ...   \n",
       "1972  HENLYFQGVQKIGILGAMREEITPILELFGVDFEEIPLGGNVFHKG...             238   \n",
       "1973  DPMKNTCKLLVVADHRFYRYMGRGEESTTTNYLIELIDRVDDIYRN...             256   \n",
       "1974  QVQLRQSGAELAKPGASVKMSCKASGYTFTNYWLHWIKQRPGQGLE...             118   \n",
       "1975  IQLVQSGPELVKISCKASGYTFTNYGMNWVRQAPGKGLKWMGWINT...             100   \n",
       "1976  EIVLTQSPGTLSLSPGERATLSCRASQSVSSSYLAWYQQKPGQAPR...             214   \n",
       "\n",
       "     target_chain                                         seq_binder  \\\n",
       "0          6IDB_A  GLFGAIAGFIENGWEGLIDGWYGFRHQNAQGEGTAADYKSTQSAID...   \n",
       "1          2WZP_D  TIKNFTFFSPNSTEFPVGSNNDGKLYMMLTGMDYRTIRRKDWSSPL...   \n",
       "2          1ZKP_A  AKTVVGFWGGFPEAGEATSGYLFEHDGFRLLVDCGSGVLAQLQKYI...   \n",
       "3          6GRH_C  MINVYSNLMSAWPATMAMSPKLNRNMPTFSQIWDYERITPASAAGE...   \n",
       "4          8R57_M  PKKQKHKHKKVKLAVLQFYKVDDATGKVTRLRKECPNADCGAGTFM...   \n",
       "...           ...                                                ...   \n",
       "1972       4YO8_A  HHHHHENLYFQGVQKIGILGAMREEITPILELFGVDFEEIPLGGNV...   \n",
       "1973       3CKI_A  CTCSPSHPQDAFCNSDIVIRAKVVGKKLVKEGPFGTLVYTIKQMKM...   \n",
       "1974       7MHY_M  DVLMTQTPLSLPVSLGDQVSISCRSSQSIVHNTYLEWYLQKPGQSP...   \n",
       "1975       7MHY_O  VLMTQTPLSLPVSISCRSSQSIVHSNGNTYLEWYLQKPGQSPKLLI...   \n",
       "1976       6O42_L  QVQLVQSGAEVKKPGSSVKVSCKASGGTFSSYAISWVRQAPGQGLE...   \n",
       "\n",
       "      seq_binder_len                                     seq_pdb_binder  \\\n",
       "0                172  GLFGAIAGFIENGWEGLIDGWYGFRHQNAQGEGTAADYKSTQSAID...   \n",
       "1                266  TIKNFTFFSPNSTEFPVGSNNDGKLYMMLTGMDYRTIRRKDWSSPL...   \n",
       "2                240  AMKMTVVGFWGGFPEAGEATSGYLFEHDGFRLLVDCGSGVLAQLQK...   \n",
       "3                396  MINVYSNLMSAWPATMAMSPKLNRNMPTFSQIWDYERITPASAAGE...   \n",
       "4                 64  PKKQKHKHKKVKLAVLQFYKVDDATGKVTRLRKECPNADCGAGTFM...   \n",
       "...              ...                                                ...   \n",
       "1972             242  HHHHHENLYFQGVQKIGILGAMREEITPILELFGVDFEEIPLGGNV...   \n",
       "1973             121  CTCSPSHPQDAFCNSDIVIRAKVVGKKLVKEGPFGTLVYTIKQMKM...   \n",
       "1974             109  DVLMTQTPLSLPVSLGDQVSISCRSSQSIVHNTYLEWYLQKPGQSP...   \n",
       "1975              94  VLMTQTPLSLPVSISCRSSQSIVHSNGNTYLEWYLQKPGQSPKLLI...   \n",
       "1976             220  QVQLVQSGAEVKKPGSSVKVSCKASGGTFSSYAISWVRQAPGQGLE...   \n",
       "\n",
       "      pdb_binder_len binder_chain     pdb_path  \n",
       "0                172       6IDB_B  6idb.pdb.gz  \n",
       "1                266       2WZP_G  2wzp.pdb.gz  \n",
       "2                245       1ZKP_C  1zkp.pdb.gz  \n",
       "3                396       6GRH_D  6grh.pdb.gz  \n",
       "4                 64       8R57_f  8r57.pdb.gz  \n",
       "...              ...          ...          ...  \n",
       "1972             242       4YO8_B  4yo8.pdb.gz  \n",
       "1973             121       3CKI_B  3cki.pdb.gz  \n",
       "1974             109       7MHY_N  7mhy.pdb.gz  \n",
       "1975              94       7MHY_P  7mhy.pdb.gz  \n",
       "1976             220       6O42_H  6o42.pdb.gz  \n",
       "\n",
       "[1977 rows x 15 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Df_train = pd.read_csv(\"/work3/s232958/data/PPint_DB/PPint_train_w_pbd_lens.csv\",index_col=0).reset_index(drop=True)\n",
    "Df_test = pd.read_csv(\"/work3/s232958/data/PPint_DB/PPint_test_w_pbd_lens.csv\",index_col=0).reset_index(drop=True)\n",
    "\n",
    "Df_train[\"target_chain\"] = [str(row.ID1[:5]+row.ID1[-1]) for __, row in Df_train.iterrows()]\n",
    "Df_train[\"binder_chain\"] = [str(row.ID2[:5]+row.ID2[-1]) for __, row in Df_train.iterrows()]\n",
    "\n",
    "Df_test[\"target_chain\"] = [str(row.ID1[:5]+row.ID1[-1]) for __, row in Df_test.iterrows()]\n",
    "Df_test[\"binder_chain\"] = [str(row.ID2[:5]+row.ID2[-1]) for __, row in Df_test.iterrows()]\n",
    "\n",
    "Df_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e2dc3459-94e9-4184-88a3-b13120177122",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'SKVVKFSYMWTINNFSFCREEMGEVIKSSTFSSKLKWCLRVNPKGLDSKDYLSLYLLLVSCPKEVRAKFKFSILNAKGEETKAMESQRAYRFVQGKDWGFKKFIRRGFLLDEANGLLPDDKLTLFCEVSVVQDSQTMNMVKVPECRLADELGGLWENSRFTDCCLCVAGQEFQAHKAILAARSPVFSAMFEHKNRVEINDVEPEVFKEMMCFIYTGKAPNLDKMADDLLAAADKYALERLKVMCEDALCSNLSVENAAEILILADLHSADQLKTQAVDFINYHA'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Df_train[Df_train.ID1.str.startswith(\"3HU6\")].seq_target.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "aab57963-a7cc-4b28-b250-bf5f5529f82a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>sequence</th>\n",
       "      <th>seq_len</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>6IDB_A</td>\n",
       "      <td>DKICLGHHAVSNGTKVNTLTERGVEVVNATETVERTNIPRICSKGK...</td>\n",
       "      <td>317</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2WZP_D</td>\n",
       "      <td>VQLQESGGGLVQAGGSLRLSCTASRRTGSNWCMGWFRQLAGKEPEL...</td>\n",
       "      <td>122</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1ZKP_A</td>\n",
       "      <td>LYFQSNAMKMTVVGFWGGFPEAGEATSGYLFEHDGFRLLVDCGSGV...</td>\n",
       "      <td>251</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>6GRH_C</td>\n",
       "      <td>SKHELSLVEVTHYTDPEVLAIVKDFHVRGNFASLPEFAERTFVSAV...</td>\n",
       "      <td>266</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>8R57_M</td>\n",
       "      <td>DLMTALQLVMKKSSAHDGLVKGLREAAKAIEKHAAQICVLAEDCDQ...</td>\n",
       "      <td>118</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3949</th>\n",
       "      <td>4YO8_B</td>\n",
       "      <td>HHHHHENLYFQGVQKIGILGAMREEITPILELFGVDFEEIPLGGNV...</td>\n",
       "      <td>242</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3950</th>\n",
       "      <td>3CKI_B</td>\n",
       "      <td>CTCSPSHPQDAFCNSDIVIRAKVVGKKLVKEGPFGTLVYTIKQMKM...</td>\n",
       "      <td>121</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3951</th>\n",
       "      <td>7MHY_N</td>\n",
       "      <td>DVLMTQTPLSLPVSLGDQVSISCRSSQSIVHNTYLEWYLQKPGQSP...</td>\n",
       "      <td>109</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3952</th>\n",
       "      <td>7MHY_P</td>\n",
       "      <td>VLMTQTPLSLPVSISCRSSQSIVHSNGNTYLEWYLQKPGQSPKLLI...</td>\n",
       "      <td>94</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3953</th>\n",
       "      <td>6O42_H</td>\n",
       "      <td>QVQLVQSGAEVKKPGSSVKVSCKASGGTFSSYAISWVRQAPGQGLE...</td>\n",
       "      <td>220</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3949 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          ID                                           sequence  seq_len\n",
       "0     6IDB_A  DKICLGHHAVSNGTKVNTLTERGVEVVNATETVERTNIPRICSKGK...      317\n",
       "1     2WZP_D  VQLQESGGGLVQAGGSLRLSCTASRRTGSNWCMGWFRQLAGKEPEL...      122\n",
       "2     1ZKP_A  LYFQSNAMKMTVVGFWGGFPEAGEATSGYLFEHDGFRLLVDCGSGV...      251\n",
       "3     6GRH_C  SKHELSLVEVTHYTDPEVLAIVKDFHVRGNFASLPEFAERTFVSAV...      266\n",
       "4     8R57_M  DLMTALQLVMKKSSAHDGLVKGLREAAKAIEKHAAQICVLAEDCDQ...      118\n",
       "...      ...                                                ...      ...\n",
       "3949  4YO8_B  HHHHHENLYFQGVQKIGILGAMREEITPILELFGVDFEEIPLGGNV...      242\n",
       "3950  3CKI_B  CTCSPSHPQDAFCNSDIVIRAKVVGKKLVKEGPFGTLVYTIKQMKM...      121\n",
       "3951  7MHY_N  DVLMTQTPLSLPVSLGDQVSISCRSSQSIVHNTYLEWYLQKPGQSP...      109\n",
       "3952  7MHY_P  VLMTQTPLSLPVSISCRSSQSIVHSNGNTYLEWYLQKPGQSPKLLI...       94\n",
       "3953  6O42_H  QVQLVQSGAEVKKPGSSVKVSCKASGGTFSSYAISWVRQAPGQGLE...      220\n",
       "\n",
       "[3949 rows x 3 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_Df1 = Df_train[[\"target_chain\", \"seq_pdb_target\", \"pdb_target_len\"]].rename(columns = {\n",
    "    \"seq_pdb_target\" : \"sequence\",\n",
    "    \"target_chain\" : \"ID\",\n",
    "    \"pdb_target_len\": \"seq_len\",\n",
    "})\n",
    "\n",
    "train_Df2 = Df_train[[\"binder_chain\", \"seq_pdb_binder\", \"pdb_binder_len\"]].rename(columns = {\n",
    "    \"seq_pdb_binder\" : \"sequence\",\n",
    "    \"binder_chain\" : \"ID\",\n",
    "    \"pdb_binder_len\": \"seq_len\",\n",
    "})\n",
    "\n",
    "Df_train_LONG = pd.concat([train_Df1, train_Df2], axis=0, ignore_index=True).drop_duplicates(subset=\"ID\", keep=\"first\")\n",
    "Df_train_LONG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e9355a32-db3d-4962-b5b3-6c7515eb52c4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>sequence</th>\n",
       "      <th>seq_len</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1NNW_A</td>\n",
       "      <td>VYVAVLANIAGNLPALTAALSRIEEMREEGYEIEKYYILGNIVGLF...</td>\n",
       "      <td>251</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3UCN_A</td>\n",
       "      <td>TADLSPLLEANRKWADECAAKDSTYFSKVAGSQAPEYLYIGCADSR...</td>\n",
       "      <td>222</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1POV_1</td>\n",
       "      <td>QHRSRSESSIESFFARGACVTIMTVDNPASTTNKDKLFAVWKITYK...</td>\n",
       "      <td>235</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3R6Y_C</td>\n",
       "      <td>VRIEKDFLGEKEIPKDAYYGVQTIRATENFPITGYRIHPELIKSLG...</td>\n",
       "      <td>383</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5YHI_A</td>\n",
       "      <td>PMRYPVDVYTGKIQVDGELMLTELGLEGDGPDRALCHYPREHYLYW...</td>\n",
       "      <td>202</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>983</th>\n",
       "      <td>3GXE_F</td>\n",
       "      <td>GLPGMKGHRGF</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>984</th>\n",
       "      <td>6LY5_l</td>\n",
       "      <td>ANFIKPYNDDPFVGHLATPITSSAVTRSLLKNLPAYRFGLTPLLRG...</td>\n",
       "      <td>144</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>985</th>\n",
       "      <td>5MLK_B</td>\n",
       "      <td>ARISKVLVANRGEIAVRVIRAARDAGLPSVAVYAEPDAESPHVRLA...</td>\n",
       "      <td>384</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>986</th>\n",
       "      <td>8BS4_B</td>\n",
       "      <td>GHPVLEKLKAAHSYNPKEFEWNLKSGRVFIIKSYSEDDIHRSIKYS...</td>\n",
       "      <td>193</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>987</th>\n",
       "      <td>6WDS_H</td>\n",
       "      <td>VQLVESGGGLVKPGGLRLSCAASGFTFSTYIMTWVRQAPGRGLEWV...</td>\n",
       "      <td>115</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>985 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         ID                                           sequence  seq_len\n",
       "0    1NNW_A  VYVAVLANIAGNLPALTAALSRIEEMREEGYEIEKYYILGNIVGLF...      251\n",
       "1    3UCN_A  TADLSPLLEANRKWADECAAKDSTYFSKVAGSQAPEYLYIGCADSR...      222\n",
       "2    1POV_1  QHRSRSESSIESFFARGACVTIMTVDNPASTTNKDKLFAVWKITYK...      235\n",
       "3    3R6Y_C  VRIEKDFLGEKEIPKDAYYGVQTIRATENFPITGYRIHPELIKSLG...      383\n",
       "4    5YHI_A  PMRYPVDVYTGKIQVDGELMLTELGLEGDGPDRALCHYPREHYLYW...      202\n",
       "..      ...                                                ...      ...\n",
       "983  3GXE_F                                        GLPGMKGHRGF       11\n",
       "984  6LY5_l  ANFIKPYNDDPFVGHLATPITSSAVTRSLLKNLPAYRFGLTPLLRG...      144\n",
       "985  5MLK_B  ARISKVLVANRGEIAVRVIRAARDAGLPSVAVYAEPDAESPHVRLA...      384\n",
       "986  8BS4_B  GHPVLEKLKAAHSYNPKEFEWNLKSGRVFIIKSYSEDDIHRSIKYS...      193\n",
       "987  6WDS_H  VQLVESGGGLVKPGGLRLSCAASGFTFSTYIMTWVRQAPGRGLEWV...      115\n",
       "\n",
       "[985 rows x 3 columns]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_Df1 = Df_test[[\"target_chain\", \"seq_pdb_target\", \"pdb_target_len\"]].rename(columns = {\n",
    "    \"seq_pdb_target\" : \"sequence\",\n",
    "    \"target_chain\" : \"ID\",\n",
    "    \"pdb_target_len\": \"seq_len\",\n",
    "})\n",
    "\n",
    "test_Df2 = Df_test[[\"binder_chain\", \"seq_pdb_binder\", \"pdb_binder_len\"]].rename(columns = {\n",
    "    \"seq_pdb_binder\" : \"sequence\",\n",
    "    \"binder_chain\" : \"ID\",\n",
    "    \"pdb_binder_len\": \"seq_len\",\n",
    "})\n",
    "\n",
    "Df_test_LONG = pd.concat([test_Df1, test_Df2], axis=0, ignore_index=True).drop_duplicates(subset=\"ID\", keep=\"first\")\n",
    "Df_test_LONG"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b40ed436-2f6a-42d8-912c-a0e9ff15b06a",
   "metadata": {},
   "source": [
    "#### Loading seqeunce, structural_embeddings & using pooled embeddings for CLIP (PPint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7d15e1e2-6358-47d8-9fc5-e9e3f977566d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "#Loading ESM2 embeddings and contacts: 100%|███████████████████████████████████████| 3949/3949 [00:10<00:00, 387.62it/s]\n",
      "#Loading ESM2 embeddings and contacts: 100%|█████████████████████████████████████████| 985/985 [00:02<00:00, 387.40it/s]\n"
     ]
    }
   ],
   "source": [
    "class CLIP_PPint_w_esmIF(Dataset):\n",
    "    def __init__(\n",
    "        self,\n",
    "        dframe,\n",
    "        paths,\n",
    "        embedding_dim_struct=512,\n",
    "        embedding_dim_seq=1280,\n",
    "        embedding_pad_value=-5000.0,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.dframe = dframe.copy()\n",
    "        self.embedding_dim_seq = embedding_dim_seq\n",
    "        self.embedding_dim_struct = embedding_dim_struct\n",
    "        self.emb_pad = embedding_pad_value\n",
    "\n",
    "        # lengths\n",
    "        self.max_len = self.dframe[\"seq_len\"].max()+2\n",
    "\n",
    "        # paths\n",
    "        self.seq_encodings_path, self.struct_encodings_path = paths\n",
    "\n",
    "        # index & storage\n",
    "        self.dframe.set_index(\"ID\", inplace=True)\n",
    "        self.accessions = self.dframe.index.astype(str).tolist()\n",
    "        self.name_to_row = {name: i for i, name in enumerate(self.accessions)}\n",
    "        self.samples = []\n",
    "\n",
    "        for accession in tqdm(self.accessions, total=len(self.accessions), desc=\"#Loading ESM2 embeddings and contacts\"):\n",
    "\n",
    "            # laod embeddings\n",
    "            emb_struct = np.load(os.path.join(self.struct_encodings_path, f\"{accession}.npy\"))[1:-1, :] # remove EOS BOS, to use only real aa token for cos-sim\n",
    "            sequence = self.dframe.loc[accession].sequence\n",
    "\n",
    "            if len(sequence) != emb_struct.shape[0]:\n",
    "                print(sequence, emb_struct.shape[0])\n",
    "\n",
    "            if emb_struct.shape[1] != self.embedding_dim_struct:\n",
    "                raise ValueError(\"Embedding dim mismatch with 'embedding_dim'.\")\n",
    "\n",
    "            if emb_struct.shape[0] < self.max_len:\n",
    "                emb_struct = np.concatenate([emb_struct, np.full((self.max_len - emb_struct.shape[0], emb_struct.shape[1]), self.emb_pad, dtype=emb_struct.dtype)], axis=0)\n",
    "            else:\n",
    "                emb_struct = emb_struct[: self.max_len] # no padding was used\n",
    "\n",
    "            self.samples.append((sequence, emb_struct))\n",
    "\n",
    "    # ---- Dataset API ----\n",
    "    def __len__(self):\n",
    "        return len(self.samples)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        sequence, emb_struct = self.samples[idx]\n",
    "        emb_struct = torch.from_numpy(emb_struct).float()\n",
    "        label = torch.tensor(1, dtype=torch.float32)  # single scalar labe        \n",
    "        return sequence, emb_struct, label\n",
    "\n",
    "    def _get_by_name(self, name):\n",
    "        # Single item -> return exactly what __getitem__ returns\n",
    "        if isinstance(name, str):\n",
    "            return self.__getitem__(self.name_to_row[name])\n",
    "        \n",
    "        # Multiple items -> fetch all\n",
    "        out = [self.__getitem__(self.name_to_row[n]) for n in list(name)]\n",
    "        # emb_seq_list, emb_struct_list, label_list = zip(*out)\n",
    "        sequence_list, emb_struct_list, lbl_list = zip(*out)\n",
    "    \n",
    "        # Stack embeddings    \n",
    "        emb_struct_stacked  = torch.stack([torch.as_tensor(x) for x in emb_struct_list],  dim=0)  # [B, ...]\n",
    "    \n",
    "        # Stack labels\n",
    "        lbl_stacked = torch.stack(lbl_list)  # [B]\n",
    "    \n",
    "        return list(sequence_list), emb_struct_stacked, lbl_stacked\n",
    "\n",
    "emb_seq_path = \"/work3/s232958/data/PPint_DB/embeddings_esm2\"\n",
    "emb_struct_path = \"/work3/s232958/data/PPint_DB/esmif_embeddings_noncanonical\"\n",
    "\n",
    "train_Dataset = CLIP_PPint_w_esmIF(\n",
    "    Df_train_LONG,\n",
    "    paths=[emb_seq_path, emb_struct_path],\n",
    "    embedding_dim_seq=1280,\n",
    "    embedding_dim_struct=512\n",
    ")\n",
    "\n",
    "test_Dataset = CLIP_PPint_w_esmIF(\n",
    "    Df_test_LONG,\n",
    "    paths=[emb_seq_path, emb_struct_path],\n",
    "    embedding_dim_seq=1280,\n",
    "    embedding_dim_struct=512\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1d730ce-e870-489d-8df2-acdd2cf6126d",
   "metadata": {},
   "source": [
    "#### Loading seqeunce, structural_embeddings & using pooled embeddings for CLIP (meta-anlaysis dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1fb9c871-9e89-4228-9cfc-68a4ee4bf523",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>sequence</th>\n",
       "      <th>seq_len</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>FGFR2_124</td>\n",
       "      <td>DIVEEAHKLLSRAMSEAMENDDPDKLRRANELYFKLEEALKNNDPK...</td>\n",
       "      <td>62</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>EGFR_2_149</td>\n",
       "      <td>SEELVEKVVEEILNSDLSNDQKILETHDRLMELHDQGKISKEEYYK...</td>\n",
       "      <td>58</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>FGFR2_339</td>\n",
       "      <td>TINRVFHLHIQGDTEEARKAHEELVEEVRRWAEELAKRLNLTVRVT...</td>\n",
       "      <td>65</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>FGFR2_1234</td>\n",
       "      <td>DDLRKVERIASELAFFAAEQNDTKVAFTALELIHQLIRAIFHNDEE...</td>\n",
       "      <td>64</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>IL2Ra_48</td>\n",
       "      <td>DEEVEELEELLEKAEDPRERAKLLRELAKLIRRDPRLRELATEVVA...</td>\n",
       "      <td>65</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3543</th>\n",
       "      <td>t_SARS_CoV2_RBD</td>\n",
       "      <td>TNLCPFGEVFNATRFASVYAWNRKRISNCVADYSVLYNSASFSTFK...</td>\n",
       "      <td>195</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3544</th>\n",
       "      <td>t_VirB8</td>\n",
       "      <td>ANPYISVANIMLQNYVKQREKYNYDTLKEQFTFIKNASTSIVYMQF...</td>\n",
       "      <td>138</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3545</th>\n",
       "      <td>t_sntx_2</td>\n",
       "      <td>MICHNQQSSQPPTTKTCSEGQCYKKTWRDHRGTIIERGCGCPTVKP...</td>\n",
       "      <td>60</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3546</th>\n",
       "      <td>t_sntx</td>\n",
       "      <td>MICYNQQSSQPPTTKTCSETSCYKKTWRDHRGTIIERGCGCPKVKP...</td>\n",
       "      <td>60</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3547</th>\n",
       "      <td>t_EGFR_3</td>\n",
       "      <td>VCQGTSNKLTQLGTFEDHFLSLQRMFNNCEVVLGNLEITYVQRNYD...</td>\n",
       "      <td>157</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3548 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                   ID                                           sequence  \\\n",
       "0           FGFR2_124  DIVEEAHKLLSRAMSEAMENDDPDKLRRANELYFKLEEALKNNDPK...   \n",
       "1          EGFR_2_149  SEELVEKVVEEILNSDLSNDQKILETHDRLMELHDQGKISKEEYYK...   \n",
       "2           FGFR2_339  TINRVFHLHIQGDTEEARKAHEELVEEVRRWAEELAKRLNLTVRVT...   \n",
       "3          FGFR2_1234  DDLRKVERIASELAFFAAEQNDTKVAFTALELIHQLIRAIFHNDEE...   \n",
       "4            IL2Ra_48  DEEVEELEELLEKAEDPRERAKLLRELAKLIRRDPRLRELATEVVA...   \n",
       "...               ...                                                ...   \n",
       "3543  t_SARS_CoV2_RBD  TNLCPFGEVFNATRFASVYAWNRKRISNCVADYSVLYNSASFSTFK...   \n",
       "3544          t_VirB8  ANPYISVANIMLQNYVKQREKYNYDTLKEQFTFIKNASTSIVYMQF...   \n",
       "3545         t_sntx_2  MICHNQQSSQPPTTKTCSEGQCYKKTWRDHRGTIIERGCGCPTVKP...   \n",
       "3546           t_sntx  MICYNQQSSQPPTTKTCSETSCYKKTWRDHRGTIIERGCGCPKVKP...   \n",
       "3547         t_EGFR_3  VCQGTSNKLTQLGTFEDHFLSLQRMFNNCEVVLGNLEITYVQRNYD...   \n",
       "\n",
       "      seq_len  \n",
       "0          62  \n",
       "1          58  \n",
       "2          65  \n",
       "3          64  \n",
       "4          65  \n",
       "...       ...  \n",
       "3543      195  \n",
       "3544      138  \n",
       "3545       60  \n",
       "3546       60  \n",
       "3547      157  \n",
       "\n",
       "[3548 rows x 3 columns]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "meta_df = pd.read_csv(\"/work3/s232958/data/meta_analysis/interaction_df_metaanal_w_pbd_lens.csv\").drop(columns = [\"binder_id\", \"target_id\"]).rename(columns = {\n",
    "    \"target_id_mod\" : \"target_id\",\n",
    "    \"target_binder_ID\" : \"binder_id\",\n",
    "})\n",
    "\n",
    "meta_df[\"target_id_mod\"] = [str(\"t_\"+row.target_id) for __, row in meta_df.iterrows()]\n",
    "\n",
    "# Interaction Dict\n",
    "meta_df_shuffled = meta_df.sample(frac=1, random_state=0).reset_index(drop=True)\n",
    "\n",
    "meta_df_shuffled_LONG_binder = meta_df_shuffled[[\"binder_id\", \"binder_seq\", \"seq_len_binder\"]].rename(columns = {\n",
    "    \"binder_id\" : \"ID\",\n",
    "    \"binder_seq\" : \"sequence\",\n",
    "    \"seq_len_binder\": \"seq_len\",\n",
    "})\n",
    "\n",
    "meta_df_shuffled_LONG_taget = meta_df_shuffled[[\"target_id_mod\", \"target_seq\", \"seq_len_target\"]].rename(columns = {\n",
    "    \"target_id_mod\" : \"ID\",\n",
    "    \"target_seq\" : \"sequence\",\n",
    "    \"seq_len_target\": \"seq_len\",\n",
    "}).drop_duplicates(subset=\"ID\", keep=\"first\")\n",
    "\n",
    "meta_df_shuffled_LONG = pd.concat([meta_df_shuffled_LONG_binder, meta_df_shuffled_LONG_taget], axis=0, ignore_index=True)\n",
    "meta_sample_Df = meta_df_shuffled_LONG.sample(n=len(Df_test_LONG), random_state=0).reset_index(drop=True)\n",
    "meta_df_shuffled_LONG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ce63d95b-d45e-4b0e-b754-42b945373802",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "#Loading ESM2 embeddings and contacts: 100%|█████████████████████████████████████████| 985/985 [00:01<00:00, 574.47it/s]\n"
     ]
    }
   ],
   "source": [
    "class CLIP_Meta_w_esmIF(Dataset):\n",
    "    def __init__(\n",
    "        self,\n",
    "        dframe,\n",
    "        embedding_dim_struct=512,\n",
    "        embedding_dim_seq=1280,\n",
    "        embedding_pad_value=-5000.0,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.dframe = dframe.copy()\n",
    "        self.embedding_dim_seq = embedding_dim_seq\n",
    "        self.embedding_dim_struct = embedding_dim_struct\n",
    "        self.emb_pad = embedding_pad_value\n",
    "\n",
    "        # lengths\n",
    "        self.max_len = self.dframe[\"seq_len\"].max()\n",
    "\n",
    "        # index & storage\n",
    "        \n",
    "        self.dframe.set_index(\"ID\", inplace=True)\n",
    "        self.accessions = self.dframe.index.astype(str).tolist()\n",
    "        self.name_to_row = {name: i for i, name in enumerate(self.accessions)}\n",
    "        self.samples = []\n",
    "\n",
    "        for accession in tqdm(self.accessions, total=len(self.accessions), desc=\"#Loading ESM2 embeddings and contacts\"):\n",
    "            \n",
    "            if accession.startswith(\"t_\"):\n",
    "                esmIF_path = \"/work3/s232958/data/meta_analysis/esmif_embeddings_targets\"\n",
    "                \n",
    "                # emb_struct = np.load(os.path.join(esmIF_path, f\"{accession[2:]}.npy\"))\n",
    "                emb_struct = np.load(os.path.join(esmIF_path, f\"{accession[2:]}.npy\"))[1:-1, :] # remove EOS BOS, to use only real aa token for cos-sim\n",
    "                sequence = str(self.dframe.loc[accession].sequence)\n",
    "\n",
    "            else:\n",
    "                esmIF_path = \"/work3/s232958/data/meta_analysis/esmif_embeddings_binders\"\n",
    "\n",
    "                # emb_struct = np.load(os.path.join(esmIF_path, f\"{accession}.npy\"))     # [Lb, D]\n",
    "                emb_struct = np.load(os.path.join(esmIF_path, f\"{accession}.npy\"))[1:-1, :] # remove EOS BOS, to use only real aa token for cos-sim\n",
    "                sequence = str(self.dframe.loc[accession].sequence)            \n",
    "\n",
    "            if len(sequence) != emb_struct.shape[0]:\n",
    "                print(str(sequence), len(sequence), emb_struct.shape[0])\n",
    "\n",
    "            if emb_struct.shape[1] != self.embedding_dim_struct:\n",
    "                raise ValueError(\"Embedding dim mismatch with 'embedding_dim'.\")\n",
    "\n",
    "            if emb_struct.shape[0] < self.max_len:\n",
    "                emb_struct = np.concatenate([emb_struct, np.full((self.max_len - emb_struct.shape[0], emb_struct.shape[1]), self.emb_pad, dtype=emb_struct.dtype)], axis=0)\n",
    "            else:\n",
    "                emb_struct = emb_struct[: self.max_len] # no padding was used\n",
    "\n",
    "            self.samples.append((sequence, emb_struct))\n",
    "\n",
    "    # ---- Dataset API ----\n",
    "    def __len__(self):\n",
    "        return len(self.samples)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        sequence, emb_struct = self.samples[idx]\n",
    "        emb_struct = torch.from_numpy(emb_struct).float()\n",
    "        label = torch.tensor(1, dtype=torch.float32)  # single scalar labe        \n",
    "        return sequence, emb_struct, label\n",
    "\n",
    "    def _get_by_name(self, name):\n",
    "        # Single item -> return exactly what __getitem__ returns\n",
    "        if isinstance(name, str):\n",
    "            return self.__getitem__(self.name_to_row[name])\n",
    "        \n",
    "        # Multiple items -> fetch all\n",
    "        out = [self.__getitem__(self.name_to_row[n]) for n in list(name)]\n",
    "        # emb_seq_list, emb_struct_list, label_list = zip(*out)\n",
    "        sequence_list, emb_struct_list, lbl_list = zip(*out)\n",
    "    \n",
    "        # Stack embeddings    \n",
    "        emb_struct_stacked  = torch.stack([torch.as_tensor(x) for x in emb_struct_list],  dim=0)  # [B, ...]\n",
    "    \n",
    "        # Stack labels\n",
    "        lbl_stacked = torch.stack(lbl_list)  # [B]\n",
    "    \n",
    "        return list(sequence_list), emb_struct_stacked, lbl_stacked\n",
    "\n",
    "esm2_path_binders = \"/work3/s232958/data/meta_analysis/embeddings_esm2_binders\"\n",
    "esm2_path_targets = \"/work3/s232958/data/meta_analysis/embeddings_esm2_targets\"\n",
    "\n",
    "## Contact maps paths\n",
    "esmIF_path_binders = \"/work3/s232958/data/meta_analysis/esmif_embeddings_binders\"\n",
    "esmIF_path_targets = \"/work3/s232958/data/meta_analysis/esmif_embeddings_targets\"\n",
    "\n",
    "# meta_Dataset_train = CLIP_Meta_w_esmIF(\n",
    "#     meta_df_shuffled_LONG_train,\n",
    "#     embedding_dim_seq=1280,\n",
    "#     embedding_dim_struct=512\n",
    "# )\n",
    "\n",
    "meta_Dataset = CLIP_Meta_w_esmIF(\n",
    "    meta_sample_Df,\n",
    "    embedding_dim_seq=1280,\n",
    "    embedding_dim_struct=512\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24d71f96-2474-4724-8a83-129fbe9ddf5d",
   "metadata": {},
   "source": [
    "### Contrastive Sequence-Structure Pre-training (CSSP)\n",
    "\n",
    "- train for 25 epochs, save every 5 epochs and plot in W&B\n",
    "- stop training of `seq_down` after `2 epochs`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4cfd101c-a7ed-4708-afb0-ed6703649e0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ESM2EncoderLoRA(nn.Module):\n",
    "    def __init__(self, padding_value=-5000.0):\n",
    "        super().__init__()\n",
    "\n",
    "        self.padding_value = padding_value\n",
    "\n",
    "        self.model = EsmModel.from_pretrained(\n",
    "            \"facebook/esm2_t33_650M_UR50D\",\n",
    "            output_hidden_states=True\n",
    "        )\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(\"facebook/esm2_t33_650M_UR50D\")\n",
    "\n",
    "        # Freeze original weights\n",
    "        for p in self.model.parameters():\n",
    "            p.requires_grad = False\n",
    "\n",
    "        # LoRA on top layers\n",
    "        lora_cfg = LoraConfig(\n",
    "            task_type=\"FEATURE_EXTRACTION\",\n",
    "            inference_mode=False,\n",
    "            r=4,\n",
    "            lora_alpha=1,\n",
    "            lora_dropout=0.1,\n",
    "            bias=\"none\",\n",
    "            # target_modules=[\"query\", \"key\", \"value\", \"dense\"],\n",
    "            target_modules=[\"query\", \"key\", \"value\", \"dense\"],\n",
    "            layers_to_transform=list(range(25, 33)),\n",
    "        )\n",
    "\n",
    "        self.model = get_peft_model(self.model, lora_cfg)\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def get_attentions(self, sequences):\n",
    "        inputs = self.tokenizer(\n",
    "            sequences, return_tensors=\"pt\", padding=True\n",
    "        ).to(self.model.device)\n",
    "\n",
    "        out = self.model(**inputs, output_attentions=True)\n",
    "        return out.attentions   # list[num_layers] → [B, num_heads, L, L]\n",
    "\n",
    "    def forward(self, sequences):\n",
    "        inputs = self.tokenizer(\n",
    "            sequences, return_tensors=\"pt\", padding=True\n",
    "        ).to(self.model.device)\n",
    "\n",
    "        out = self.model(**inputs)\n",
    "        reps = out.hidden_states[-1]                  # [B, Ltok, 1280]\n",
    "        reps = reps[:, 1:-1, :]                       # remove CLS/EOS\n",
    "\n",
    "        seq_lengths = [len(s) for s in sequences]\n",
    "        Lmax = max(seq_lengths)\n",
    "\n",
    "        B, D = reps.size(0), reps.size(-1)\n",
    "        padded = torch.full((B, Lmax, D), self.padding_value, device=reps.device)\n",
    "\n",
    "        for i, (r, real_len) in enumerate(zip(reps, seq_lengths)):\n",
    "            padded[i, :real_len] = r[:real_len]\n",
    "\n",
    "        return padded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3f0a79a-929b-47b7-a933-0718faa436eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def non_padding_mask(embeddings, padding_value=-5000, offset=10):\n",
    "    return (embeddings > (padding_value + offset)).all(dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "cee6ce23-8969-4fee-aae7-231abe416d74",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CSSPBoostingESM(nn.Module):\n",
    "    def __init__(self, seq_embed_dim=1280, struct_embed_dim=512, padding_value=-5000):\n",
    "        super().__init__()\n",
    "        self.padding_value = padding_value\n",
    "        self.struct_embed_dim = 512\n",
    "        self.seq_encoder = ESM2EncoderLoRA()\n",
    "        self.seq_down = nn.Linear(seq_embed_dim, struct_embed_dim)  \n",
    "        self.logit_scale = nn.Parameter(torch.tensor(math.log(1/0.07)))\n",
    "\n",
    "    def forward(self, sequences, struct_embed):\n",
    "    \n",
    "        # ---- encode sequence ----\n",
    "        seq_embed = self.seq_encoder(sequences)          # [B, Lseq, 1280]\n",
    "        B, Lseq, _ = seq_embed.shape\n",
    "        _, Lstr, D = struct_embed.shape                  # D = 512\n",
    "    \n",
    "        # ---- masks ----\n",
    "        seq_mask = non_padding_mask(seq_embed, self.padding_value)      # True = real\n",
    "        struct_mask = non_padding_mask(struct_embed, self.padding_value)  # True = real\n",
    "    \n",
    "        # enforce residue alignment\n",
    "        assert (\n",
    "            seq_mask.sum(dim=1).cpu().tolist()\n",
    "            == struct_mask.sum(dim=1).cpu().tolist()\n",
    "        ), \"Sequence and structure residue counts do not match\"\n",
    "    \n",
    "        # ---- project seq + pad to structure length ----\n",
    "        seq_embed_proj = torch.full(\n",
    "            (B, Lstr, D),\n",
    "            self.padding_value,\n",
    "            device=seq_embed.device,\n",
    "            dtype=seq_embed.dtype,\n",
    "        )\n",
    "    \n",
    "        for i in range(B):\n",
    "            real_seq = seq_embed[i][seq_mask[i]]     # [Li, 1280]\n",
    "            proj = self.seq_down(real_seq)            # [Li, 512]\n",
    "            seq_embed_proj[i, :proj.size(0)] = proj   # align positions\n",
    "    \n",
    "        seq_pooled = create_mean_of_non_masked(seq_embed_proj, create_key_padding_mask(seq_embed_proj))\n",
    "        struct_pooled = create_mean_of_non_masked(struct_embed, create_key_padding_mask(struct_embed))\n",
    "    \n",
    "        seq_full = F.normalize(seq_pooled, dim=-1)\n",
    "        struct_full = F.normalize(struct_pooled, dim=-1)\n",
    "    \n",
    "        scale = torch.exp(self.logit_scale).clamp(max=100)\n",
    "        logits_seq = scale * (seq_full @ struct_full.T)\n",
    "        logits_struct = scale * (struct_full @ seq_full.T)\n",
    "    \n",
    "        return logits_seq, logits_struct, seq_embed_proj, struct_embed, struct_mask\n",
    "\n",
    "    def training_step(self, batch, device):\n",
    "    \n",
    "        sequences, struct_embed, _ = batch\n",
    "        struct_embed = struct_embed.to(device)\n",
    "    \n",
    "        logits_seq, logits_struct, seq_embed_proj, struct_embed, struct_mask = self.forward(sequences, struct_embed)\n",
    "    \n",
    "        # ---- CLIP loss ----\n",
    "        B = logits_seq.size(0)\n",
    "        labels = torch.arange(B, device=device)\n",
    "    \n",
    "        loss_seq = F.cross_entropy(logits_seq, labels)\n",
    "        loss_struct = F.cross_entropy(logits_struct, labels)\n",
    "        clip_loss = 0.5 * (loss_seq + loss_struct)\n",
    "    \n",
    "        # ---- token-level cosine loss (aligned positions) ----\n",
    "        cos = F.cosine_similarity(seq_embed_proj, struct_embed, dim=-1)  # [B, Lstr]\n",
    "        cos = cos * struct_mask.float()                                   # mask padding\n",
    "    \n",
    "        per_token_loss = 1.0 - (cos.sum(dim=1) / struct_mask.sum(dim=1)).mean()\n",
    "    \n",
    "        return clip_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "63c9aec4-8272-45ec-9906-08a0631d504a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/work3/s232958/envs/esm_cuda/lib/python3.10/site-packages/huggingface_hub/file_download.py:942: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "Some weights of EsmModel were not initialized from the model checkpoint at facebook/esm2_t33_650M_UR50D and are newly initialized: ['esm.pooler.dense.bias', 'esm.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of EsmModel were not initialized from the model checkpoint at facebook/esm2_t33_650M_UR50D and are newly initialized: ['esm.pooler.dense.bias', 'esm.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "model = CSSPBoostingESM(\n",
    "    seq_embed_dim=1280,\n",
    "    struct_embed_dim=512,\n",
    "    padding_value=-5000,\n",
    ").to(device)\n",
    "\n",
    "# model\n",
    "\n",
    "runID = uuid.uuid4()\n",
    "learning_rate = 2e-5\n",
    "EPOCHS = 100\n",
    "batch_size = 10\n",
    "model = CSSPBoostingESM(seq_embed_dim=1280, struct_embed_dim=512).to(\"cuda\")\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# optimizer = AdamW(model.parameters(), lr=learning_rate)\n",
    "seq_down_params = set(model.seq_down.parameters())\n",
    "other_params = [p for p in model.parameters() if p not in seq_down_params]\n",
    "optimizer = torch.optim.AdamW([\n",
    "    {\"params\": other_params, \"lr\": learning_rate},\n",
    "    {\"params\": model.seq_down.parameters(),\"lr\": 1e-5},\n",
    "])\n",
    "\n",
    "accelerator = Accelerator()\n",
    "device = accelerator.device\n",
    "\n",
    "train_dataloader = DataLoader(train_Dataset, batch_size=10, shuffle=True)\n",
    "test_dataloader = DataLoader(test_Dataset, batch_size=10, shuffle=False)\n",
    "val_dataloader = DataLoader(meta_Dataset, batch_size=10, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "e9072c55-5d54-4a1e-9e63-d11d04e0c291",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "- token-lvl cos-siminilarity\n",
    "- train seq_down for 4 epochs and then freeze all parameters\n",
    "- train in total for 50 epochs, save model every 5 epochs and plot progress in W&B\n",
    "\"\"\"\n",
    "\n",
    "class TrainWrapper():\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        model=model,\n",
    "        train_loader=train_dataloader,\n",
    "        test_loader=test_dataloader,\n",
    "        val_loader=val_dataloader,\n",
    "        optimizer=optimizer,\n",
    "        epochs=EPOCHS,\n",
    "        device=device,\n",
    "        wandb_tracker=False,\n",
    "        save_at = []\n",
    "    ):\n",
    "        self.model = model\n",
    "        self.training_loader = train_loader\n",
    "        self.testing_loader = test_loader\n",
    "        self.validation_loader = val_loader\n",
    "        self.EPOCHS = epochs\n",
    "        self.optimizer = optimizer\n",
    "        self.device = device\n",
    "        self.wandb_tracker = wandb_tracker\n",
    "        self.save_at = set(save_at)\n",
    "\n",
    "    def train_one_epoch(self):\n",
    "\n",
    "        self.model.train()\n",
    "        self.model.seq_encoder.train()\n",
    "\n",
    "        running_loss = 0\n",
    "\n",
    "        for batch in tqdm(self.training_loader, total=len(self.training_loader), desc=\"Running through epoch\"):\n",
    "\n",
    "            sequences, struct_embed, labels = batch\n",
    "            struct_embed = struct_embed.to(self.device)\n",
    "            labels = labels.to(self.device)\n",
    "\n",
    "            self.optimizer.zero_grad()\n",
    "            loss = self.model.training_step((sequences, struct_embed, labels), self.device)\n",
    "\n",
    "            loss.backward()\n",
    "            self.optimizer.step()\n",
    "\n",
    "            running_loss += loss.item()\n",
    "\n",
    "        return running_loss / len(self.training_loader)\n",
    "    \n",
    "\n",
    "    @torch.no_grad()\n",
    "    def compute_embeddings_cos_similariy(self, loader=None, loader_name=\"test\"):\n",
    "    \n",
    "        if loader is None:\n",
    "            loader = self.testing_loader\n",
    "    \n",
    "        self.model.eval()\n",
    "        self.model.seq_encoder.eval()\n",
    "    \n",
    "        all_embeds = []\n",
    "        cosine_similarities = []\n",
    "    \n",
    "        for batch in tqdm(loader, desc=f\"Computing cosine similarity & embeddings ({loader_name})\"):\n",
    "    \n",
    "            seqs, struct_embed, _ = batch\n",
    "            struct_embed = struct_embed.to(self.device)      # [B, Ls, 512]\n",
    "    \n",
    "            # ---- sequence embeddings ----\n",
    "            seq_embed = self.model.seq_encoder(seqs)         # [B, Lq, 1280]\n",
    "            B, Lq, _ = seq_embed.shape\n",
    "            _, Ls, Ds = struct_embed.shape\n",
    "    \n",
    "            seq_mask = non_padding_mask(seq_embed, self.model.padding_value)   # [B, Lq]\n",
    "            str_mask = non_padding_mask(struct_embed, self.model.padding_value)  # [B, Ls]\n",
    "    \n",
    "            # enforce residue-wise alignment\n",
    "            assert (seq_mask.sum(dim=1).cpu().tolist()== str_mask.sum(dim=1).cpu().tolist())\n",
    "    \n",
    "            # ---- project seq tokens + pad to structure length ----\n",
    "            seq_embed_proj = torch.full((B, Ls, Ds), self.model.padding_value, device=seq_embed.device, dtype=seq_embed.dtype)\n",
    "    \n",
    "            for i in range(B):\n",
    "                real_seq = seq_embed[i][seq_mask[i]]      # [Li, 1280]\n",
    "                proj = self.model.seq_down(real_seq)      # [Li, 512]\n",
    "                seq_embed_proj[i, :proj.size(0)] = proj   # pad up to Ls\n",
    "    \n",
    "            # ---- token-level cosine similarity (aligned positions) ----\n",
    "            cos = F.cosine_similarity(seq_embed_proj, struct_embed, dim=-1)  # [B, Ls]\n",
    "            cos = cos * str_mask.float()   # mask padding\n",
    "    \n",
    "            per_seq_cos = cos.sum(dim=1) / str_mask.sum(dim=1)\n",
    "            cosine_similarities.extend(per_seq_cos.cpu().tolist())\n",
    "    \n",
    "            # ---- pooled sequence embeddings (projected space) ----\n",
    "            seq_pooled = create_mean_of_non_masked(seq_embed_proj, create_key_padding_mask(seq_embed_proj))\n",
    "            seq_full = F.normalize(seq_pooled, dim=-1)\n",
    "            all_embeds.append(seq_full.cpu())\n",
    "    \n",
    "        all_embeds = torch.cat(all_embeds, dim=0)\n",
    "    \n",
    "        avg_cos = float(np.mean(cosine_similarities))\n",
    "        std_cos = float(np.std(cosine_similarities))\n",
    "    \n",
    "        return all_embeds, cosine_similarities, avg_cos, std_cos\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def eval_train_loss(self):\n",
    "        self.model.eval()\n",
    "        running = 0.0\n",
    "        for sequences, struct_embed, labels in tqdm(self.training_loader, total=len(self.training_loader), desc=f\"Computing Training Loss before training\"):\n",
    "            struct_embed = struct_embed.to(self.device)\n",
    "            labels = labels.to(self.device)\n",
    "            loss = self.model.training_step((sequences, struct_embed, labels), self.device)\n",
    "            running += loss.item()\n",
    "        return running / len(self.training_loader)\n",
    "\n",
    "    def plot_embeddings_drift_cos_similarity_change(self, start_embeddings, end_embeddings, cosine_similarities):\n",
    "\n",
    "        drift = (end_embeddings - start_embeddings).norm(dim=1).cpu().numpy()\n",
    "        cosine_similarities = np.array(cosine_similarities)\n",
    "\n",
    "        fig, ax = plt.subplots(1, 2, figsize=(8, 4))\n",
    "\n",
    "        ax[0].hist(drift, bins=30, color=\"steelblue\", alpha=0.8)\n",
    "        ax[0].set_title(\"Embedding Drift per Sequence\", fontsize=8)\n",
    "        ax[0].set_xlabel(\"L2 Norm Drift\", fontsize=8)\n",
    "        ax[0].set_ylabel(\"Density\", fontsize=8)\n",
    "\n",
    "        ax[1].hist(cosine_similarities, bins=40, color=\"darkorange\", alpha=0.7, density=True)\n",
    "        ax[1].set_title(\"Cosine Similarities (ESM-2 vs ESM-IF)\", fontsize=8)\n",
    "        ax[1].set_xlabel(\"Cosine Similarity\", fontsize=8)\n",
    "        ax[1].set_ylabel(\"Density\", fontsize=8)\n",
    "\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "\n",
    "    def train_model(self, save_every: int = 5):\n",
    "    \n",
    "        # run_dir = f\"/work3/s232958/data/trained/boostingESM2wESMIF/train_on_PPint_combinedLoss02/{runID}/\"\n",
    "        run_dir = f\"/work3/s232958/data/trained/boostingESM2wESMIF/train_on_PPint_combinedLoss02/2d2b1f23-4d4e-49b0-a768-7625349e0800/\"\n",
    "        os.makedirs(run_dir, exist_ok=True)\n",
    "    \n",
    "        print(\"\\nTrainable parameters inside seq_encoder (LoRA layers):\")\n",
    "        for name, p in self.model.seq_encoder.named_parameters():\n",
    "            if p.requires_grad:\n",
    "                print(\"  \", name)\n",
    "    \n",
    "        # ---- Save checkpoint BEFORE training (epoch 0) ----\n",
    "        # save_path_encoder = os.path.join(run_dir, \"seq_encoder_before_training.pt\")\n",
    "        # save_path_projhead = os.path.join(run_dir, \"seq_down_before_training.pt\")\n",
    "        # torch.save(self.model.seq_encoder.state_dict(), save_path_encoder)\n",
    "        # torch.save(self.model.seq_down.state_dict(), save_path_projhead)\n",
    "        # print(f\"Saved seq_encoder checkpoint before training -> {save_path_encoder}\")\n",
    "        # print(f\"Saved proj head checkpoint before training -> {save_path_projhead}\")\n",
    "    \n",
    "        # ---- START embeddings/cos-sim for both loaders ----\n",
    "        print(\"Computing Loss before training:\")\n",
    "        train_loss = self.eval_train_loss()\n",
    "        print(f\"[TRAIN] Loss {train_loss}\")\n",
    "        \n",
    "        print(\"\\nExtracting START embeddings & cosine similarities (val + test)...\")\n",
    "        start_val_emb, start_val_cos, start_val_avg, start_val_std = self.compute_embeddings_cos_similariy(loader=self.validation_loader, loader_name=\"val\")\n",
    "        print(f\"[VAL]  avg cos: {start_val_avg:.4f}, std: {start_val_std:.4f}\")\n",
    "        self.plot_embeddings_drift_cos_similarity_change(start_val_emb, start_val_emb, start_val_cos)\n",
    "    \n",
    "        start_test_emb, start_test_cos, start_test_avg, start_test_std = self.compute_embeddings_cos_similariy(loader=self.testing_loader, loader_name=\"test\")\n",
    "        print(f\"[TEST] avg cos: {start_test_avg:.4f}, std: {start_test_std:.4f}\")\n",
    "        # self.plot_embeddings_drift_cos_similarity_change(start_test_emb, start_test_emb, start_test_cos)\n",
    "\n",
    "        if self.wandb_tracker:\n",
    "            self.wandb_tracker.log({\n",
    "                \"train/loss\": train_loss,\n",
    "                \"val/cos_avg\": start_val_avg,\n",
    "                \"val/cos_std\": start_val_std,\n",
    "                \"test/cos_avg\": start_test_avg,\n",
    "                \"test/cos_std\": start_test_std,\n",
    "            }, step=0)\n",
    "    \n",
    "        for epoch in range(1, self.EPOCHS + 1):\n",
    "        \n",
    "            # if epoch == 5:\n",
    "            if epoch == 1:\n",
    "                print(f\"\\nFreezing projection head (seq_down) after {epoch-1} epoch(s), continuing training of the rest...\")\n",
    "                for p in self.model.seq_down.parameters():\n",
    "                    p.requires_grad = False\n",
    "                self.optimizer.zero_grad(set_to_none=True)\n",
    "    \n",
    "            train_loss = self.train_one_epoch()\n",
    "            print(f\"Epoch {epoch}: loss={train_loss:.4f}\")\n",
    "    \n",
    "            # Optional: monitor cos-sim each epoch (val + test)\n",
    "            val_emb, val_cos, val_avg, val_std = self.compute_embeddings_cos_similariy(loader=self.validation_loader, loader_name=\"val\")\n",
    "            print(f\"[VAL]  avg cos: {val_avg:.4f}, std: {val_std:.4f}\")\n",
    "    \n",
    "            test_emb, test_cos, test_avg, test_std = self.compute_embeddings_cos_similariy(loader=self.testing_loader, loader_name=\"test\")\n",
    "            print(f\"[TEST] avg cos: {test_avg:.4f}, std: {test_std:.4f}\")\n",
    "    \n",
    "            # ---- Save checkpoints when ----            \n",
    "            value = round(val_avg, 2)\n",
    "            if value in self.save_at:\n",
    "                save_path_encoder = os.path.join(run_dir, f\"seq_encoder_cos-sim{value}.pt\")\n",
    "                save_path_projhead = os.path.join(run_dir, f\"seq_down_cos-sim{value}.pt\")\n",
    "                save_path_model = os.path.join(run_dir, f\"model_cos-sim{value}.pt\")\n",
    "                torch.save(self.model.state_dict(), save_path_model)\n",
    "                torch.save(self.model.seq_encoder.state_dict(), save_path_encoder)\n",
    "                torch.save(self.model.seq_down.state_dict(), save_path_projhead)\n",
    "                print(f\"Saved whole model checkpoint -> {save_path_model}\")\n",
    "                print(f\"Saved seq_encoder checkpoint -> {save_path_encoder}\")\n",
    "                print(f\"Saved proj head checkpoint -> {save_path_projhead}\")\n",
    "                self.save_at.remove(value)\n",
    "\n",
    "            if self.wandb_tracker:\n",
    "                self.wandb_tracker.log({\n",
    "                    \"train/loss\": train_loss,\n",
    "                    \"val/cos_avg\": val_avg,\n",
    "                    \"val/cos_std\": val_std,\n",
    "                    \"test/cos_avg\": test_avg,\n",
    "                    \"test/cos_std\": test_std,\n",
    "                }, step=epoch)\n",
    "\n",
    "            if len(self.save_at)==0:\n",
    "                # ---- END embeddings/cos-sim for both loaders ----\n",
    "                print(\"\\nExtracting END embeddings & cosine similarities (val + test)...\")\n",
    "                end_val_emb, end_val_cos, val_avg, val_std = self.compute_embeddings_cos_similariy(loader=self.validation_loader, loader_name=\"val\")\n",
    "                print(f\"[VAL]  avg cos: {val_avg:.4f}, std: {val_std:.4f}\")\n",
    "                self.plot_embeddings_drift_cos_similarity_change(start_val_emb, end_val_emb, end_val_cos)\n",
    "        \n",
    "                end_test_emb, end_test_cos, test_avg, test_std = self.compute_embeddings_cos_similariy(loader=self.testing_loader, loader_name=\"test\")\n",
    "                print(f\"[TEST] avg cos: {test_avg:.4f}, std: {test_std:.4f}\")\n",
    "            \n",
    "                # Return test triplet like your original expectation:\n",
    "                return (start_val_emb, end_val_emb, end_val_cos), (start_test_emb, end_test_emb, end_test_cos)\n",
    "    \n",
    "        # ---- END embeddings/cos-sim for both loaders ----\n",
    "        print(\"\\nExtracting END embeddings & cosine similarities (val + test)...\")\n",
    "        end_val_emb, end_val_cos, val_avg, val_std = self.compute_embeddings_cos_similariy(loader=self.validation_loader, loader_name=\"val\")\n",
    "        print(f\"[VAL]  avg cos: {val_avg:.4f}, std: {val_std:.4f}\")\n",
    "        self.plot_embeddings_drift_cos_similarity_change(start_val_emb, end_val_emb, end_val_cos)\n",
    "\n",
    "        end_test_emb, end_test_cos, test_avg, test_std = self.compute_embeddings_cos_similariy(loader=self.testing_loader, loader_name=\"test\")\n",
    "        print(f\"[TEST] avg cos: {test_avg:.4f}, std: {test_std:.4f}\")\n",
    "    \n",
    "        # Return test triplet like your original expectation:\n",
    "        return (start_val_emb, end_val_emb, end_val_cos), (start_test_emb, end_test_emb, end_test_cos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "0378f042-bfee-47bc-bfab-e4336077e1cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fe4d47d7-cc18-449b-b994-ec9ff1c417c1\n"
     ]
    }
   ],
   "source": [
    "print(runID)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a34b248-273c-40b5-9bf8-6dff944745b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "### wandb\n",
    "if use_wandb:\n",
    "    run = wandb.init(\n",
    "        project=\"Boosting_ESM_2_w_ESM_IF_02\",\n",
    "        name=f\"freeze_projhead_after_epoch4_{runID}\",\n",
    "        config={\"learning_rate\": learning_rate, \n",
    "                \"batch_size\": batch_size, \n",
    "                \"epochs\": EPOCHS,\n",
    "                \"architecture\": \"MiniCLIP_w_transformer_crossattn\", \n",
    "                \"dataset\": \n",
    "                \"PPint\"},\n",
    "    )\n",
    "    wandb.watch(accelerator.unwrap_model(model), log=\"all\", log_freq=100)\n",
    "else:\n",
    "    run = None\n",
    "\n",
    "# accelerator\n",
    "model, optimizer, train_dataloader, test_dataloader, val_dataloader = accelerator.prepare(model, optimizer, train_dataloader, test_dataloader, val_dataloader)\n",
    "\n",
    "training_wrapper = TrainWrapper(\n",
    "    model=model,\n",
    "    train_loader=train_dataloader,\n",
    "    test_loader=test_dataloader,\n",
    "    val_loader=val_dataloader,\n",
    "    optimizer=optimizer,\n",
    "    epochs=EPOCHS,\n",
    "    device=device,\n",
    "    wandb_tracker=wandb,\n",
    "    save_at = [0.05, 0.10, 0.15, 0.20, 0.30, 0.40, 0.5, 0.6, 0.7, 0.8]\n",
    ")\n",
    "\n",
    "val_params, test_params = training_wrapper.train_model(save_every=[5, 10, 25, 50, 60, 70, 80, 90, 100])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3841e630-d6e4-4be5-bfd9-15b7cbe0307d",
   "metadata": {},
   "source": [
    "### Quick check-up of updates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "500eb040-da96-416e-9dba-6bf91914e1e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CLIP_PPint_w_esmIF(Dataset):\n",
    "    def __init__(\n",
    "        self,\n",
    "        dframe,\n",
    "        paths,\n",
    "        embedding_dim_struct=512,\n",
    "        embedding_dim_seq=1280,\n",
    "        embedding_pad_value=-5000.0,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.dframe = dframe.copy()\n",
    "        self.embedding_dim_seq = embedding_dim_seq\n",
    "        self.embedding_dim_struct = embedding_dim_struct\n",
    "        self.emb_pad = embedding_pad_value\n",
    "\n",
    "        # lengths\n",
    "        self.seq_len = self.dframe[\"seq_len\"].max()\n",
    "\n",
    "        # index & storage\n",
    "        self.dframe.set_index(\"ID\", inplace=True)\n",
    "        self.accessions = self.dframe.index.astype(str).tolist()\n",
    "        self.name_to_row = {name: i for i, name in enumerate(self.accessions)}\n",
    "        self.samples = []\n",
    "\n",
    "        for accession in tqdm(self.accessions, total=len(self.accessions), desc=\"#Loading ESM2 embeddings and contacts\"):\n",
    "\n",
    "            # laod embeddings\n",
    "            if accession.startswith(\"t_\"):\n",
    "                esmIF_path = \"/work3/s232958/data/meta_analysis/esmif_embeddings_targets\"\n",
    "                esm2_path = \"/work3/s232958/data/meta_analysis/embeddings_esm2_targets\"\n",
    "               \n",
    "                emb_struct = np.load(os.path.join(esmIF_path, f\"{accession[2:]}.npy\"))\n",
    "                emb_seq = np.load(os.path.join(esm2_path, f\"{accession[2:]}.npy\"))\n",
    "                sequence = str(self.dframe.loc[accession].sequence)\n",
    "\n",
    "            else:\n",
    "                esmIF_path = \"/work3/s232958/data/meta_analysis/esmif_embeddings_binders\"\n",
    "                esm2_path = \"/work3/s232958/data/meta_analysis/embeddings_esm2_binders\"\n",
    "\n",
    "                emb_struct = np.load(os.path.join(esmIF_path, f\"{accession}.npy\"))\n",
    "                emb_seq = np.load(os.path.join(esm2_path, f\"{accession}.npy\"))\n",
    "                sequence = str(self.dframe.loc[accession].sequence)\n",
    "\n",
    "            # quich check whether embedding dimmension is as it suppose to be\n",
    "            if emb_seq.shape[1] != self.embedding_dim_seq:\n",
    "                raise ValueError(\"Embedding dim mismatch with 'embedding_dim_seq'.\")\n",
    "            if emb_struct.shape[1] != self.embedding_dim_struct:\n",
    "                raise ValueError(\"Embedding dim mismatch with 'embedding_dim'.\")\n",
    "                \n",
    "            # add -5000 to all the padded target rows\n",
    "            if emb_seq.shape[0] < self.seq_len:\n",
    "                emb_seq = np.concatenate([emb_seq, np.full((self.seq_len - emb_seq.shape[0], emb_seq.shape[1]), self.emb_pad, dtype=emb_seq.dtype)], axis=0)\n",
    "            else:\n",
    "                emb_seq = emb_seq[: self.seq_len] # no padding was usedd\n",
    "\n",
    "            if emb_struct.shape[0] < self.seq_len:\n",
    "                emb_struct = np.concatenate([emb_struct, np.full((self.seq_len - emb_struct.shape[0], emb_struct.shape[1]), self.emb_pad, dtype=emb_struct.dtype)], axis=0)\n",
    "            else:\n",
    "                emb_struct = emb_struct[: self.seq_len] # no padding was used\n",
    "\n",
    "            self.samples.append((emb_seq, sequence, emb_struct))\n",
    "\n",
    "    # ---- Dataset API ----\n",
    "    def __len__(self):\n",
    "        return len(self.samples)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        emb_seq, seq, emb_struct = self.samples[idx]\n",
    "        emb_seq, emb_struct = torch.from_numpy(emb_seq).float(), torch.from_numpy(emb_struct).float()\n",
    "        # label = torch.tensor(1, dtype=torch.float32)  # single scalar labe\n",
    "        return emb_seq, seq, emb_struct\n",
    "\n",
    "    def _get_by_name(self, name):\n",
    "        # Single item -> return exactly what __getitem__ returns\n",
    "        if isinstance(name, str):\n",
    "            return self.__getitem__(self.name_to_row[name])\n",
    "        \n",
    "        # Multiple items -> fetch all\n",
    "        out = [self.__getitem__(self.name_to_row[n]) for n in list(name)]\n",
    "        # emb_seq_list, emb_struct_list, label_list = zip(*out)\n",
    "        emb_seq_list, seqs_list, emb_struct_list = zip(*out)\n",
    "    \n",
    "        # Stack embeddings\n",
    "        emb_seq_stacked  = torch.stack([torch.as_tensor(x) for x in emb_seq_list],  dim=0)  # [B, ...]        \n",
    "        emb_struct_stacked  = torch.stack([torch.as_tensor(x) for x in emb_struct_list],  dim=0)  # [B, ...]\n",
    "    \n",
    "        # Stack labels\n",
    "        # labels = torch.stack(lbl_list)  # [B]\n",
    "    \n",
    "        return emb_seq_stacked, seqs_list, emb_struct_stacked\n",
    "\n",
    "emb_seq_path = \"/work3/s232958/data/meta_analysis/embeddings_esm2_binders\"\n",
    "emb_struct_path = \"/work3/s232958/data/meta_analysis/esmif_embeddings_binders\"\n",
    "\n",
    "meta_Dataset = CLIP_PPint_w_esmIF(\n",
    "    meta_df_shuffled_LONG,\n",
    "    paths=[emb_seq_path, emb_struct_path],\n",
    "    embedding_dim_seq=1280,\n",
    "    embedding_dim_struct=512\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21333b2d-5dd8-4455-bf85-7329f1310f3e",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "#### Stop after epoch 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb44f545-4476-4782-b9e9-747a8ba862da",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Training seq_down for 1 epoch \n",
    "seq_encoder_checkpoint_path = \"/work3/s232958/data/trained/boostingESM2wESMIF/train_on_PPint/5d521d76-3082-4258-99ba-0255033e8f16/ESM2EncoderLoRA_epoch_10.pt\"\n",
    "seq_encoder_state_dict = torch.load(seq_encoder_checkpoint_path, map_location=device)\n",
    "seq_encoder = ESM2EncoderLoRA()\n",
    "seq_encoder.load_state_dict(seq_encoder_state_dict)\n",
    "seq_encoder.to(device)\n",
    "seq_encoder.eval()\n",
    "\n",
    "seq_down_checkpoint_path = \"/work3/s232958/data/trained/boostingESM2wESMIF/train_on_PPint/5d521d76-3082-4258-99ba-0255033e8f16/ProjHead_epoch_10.pt\"\n",
    "seq_down_state_dict = torch.load(seq_down_checkpoint_path, map_location=device)\n",
    "seq_down = nn.Linear(1280, 512)\n",
    "seq_down.load_state_dict(seq_down_state_dict)\n",
    "seq_down.to(device)\n",
    "# seq_down.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eca76ebd-44d4-4e39-bd14-a703cd47e3d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Similarity using original embeddings and trained projection head\n",
    "_loader = DataLoader(meta_Dataset, batch_size=5)\n",
    "cosine_similarities_BEFORE = []\n",
    "seq_proj = nn.Linear(1280, 512)\n",
    "\n",
    "for batch in _loader:\n",
    "    seq_embed, seqs_list, struct_embed = batch\n",
    "    seq_embed, struct_embed = seq_embed.to(device), struct_embed.to(device)\n",
    "    \n",
    "    # seq_embed = self.model.seq_encoder(seqs)  # [B, L, 1280]\n",
    "    seq_mask = create_key_padding_mask(seq_embed)\n",
    "    seq_pooled = create_mean_of_non_masked(seq_embed, seq_mask)  # [B, 1280]\n",
    "    seq_pooled_proj = seq_down(seq_pooled)\n",
    "    # seq_pooled_proj = seq_proj(seq_pooled)\n",
    "\n",
    "    struct_mask = create_key_padding_mask(struct_embed)\n",
    "    struct_pooled = create_mean_of_non_masked(struct_embed, struct_mask)  # [B, 512]\n",
    "\n",
    "    seq_full = F.normalize(seq_pooled_proj, dim=-1)\n",
    "    struct_full = F.normalize(struct_pooled, dim=-1)\n",
    "\n",
    "    cos = F.cosine_similarity(seq_full, struct_full, dim=-1)  # [B]\n",
    "    cosine_similarities_BEFORE.append(cos.mean().item())\n",
    "\n",
    "plt.figure(figsize=(6, 4))\n",
    "plt.hist(cosine_similarities_BEFORE, bins=50, alpha=0.6, label=\"Cosine similarities before\", density=True)\n",
    "# plt.hist(neg_logits, bins=50, alpha=0.6, label=\"Negative pairs\", density=True)\n",
    "\n",
    "plt.xlabel(\"cos-sim\")\n",
    "plt.ylabel(\"Density\")\n",
    "plt.title(\"Cosine similarities ESM-2 and ESM-IF before training!\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "407c415d-b9f6-4563-9a13-787ea1de5a83",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Similarity using original embeddings and random nn.Linear(1280, 512) head\n",
    "\n",
    "_loader = DataLoader(meta_Dataset, batch_size=5)\n",
    "cosine_similarities_BEFORE = []\n",
    "seq_proj = nn.Linear(1280, 512).to(device)\n",
    "\n",
    "for batch in _loader:\n",
    "    seq_embed, seqs_list, struct_embed = batch\n",
    "    seq_embed, struct_embed = seq_embed.to(device), struct_embed.to(device)\n",
    "    \n",
    "    # seq_embed = self.model.seq_encoder(seqs)  # [B, L, 1280]\n",
    "    seq_mask = create_key_padding_mask(seq_embed)\n",
    "    seq_pooled = create_mean_of_non_masked(seq_embed, seq_mask)  # [B, 1280]\n",
    "    # seq_pooled_proj = seq_down(seq_pooled)\n",
    "    seq_pooled_proj = seq_proj(seq_pooled)\n",
    "\n",
    "    struct_mask = create_key_padding_mask(struct_embed)\n",
    "    struct_pooled = create_mean_of_non_masked(struct_embed, struct_mask)  # [B, 512]\n",
    "\n",
    "    seq_full = F.normalize(seq_pooled_proj, dim=-1)\n",
    "    struct_full = F.normalize(struct_pooled, dim=-1)\n",
    "\n",
    "    cos = F.cosine_similarity(seq_full, struct_full, dim=-1)  # [B]\n",
    "    cosine_similarities_BEFORE.append(cos.mean().item())\n",
    "\n",
    "plt.figure(figsize=(6, 4))\n",
    "plt.hist(cosine_similarities_BEFORE, bins=50, alpha=0.6, label=\"Cosine similarities before\", density=True)\n",
    "# plt.hist(neg_logits, bins=50, alpha=0.6, label=\"Negative pairs\", density=True)\n",
    "\n",
    "plt.xlabel(\"cos-sim\")\n",
    "plt.ylabel(\"Density\")\n",
    "plt.title(\"Cosine similarities ESM-2 and ESM-IF before training!\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce3e3647-8445-483e-af10-7c43815e33fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Similarity using both trained seq_encoder + trained proj_head\n",
    "_loader = DataLoader(meta_Dataset, batch_size=5)\n",
    "cosine_similarities_AFTER = []\n",
    "\n",
    "for batch in _loader:\n",
    "    __, seqs_list, struct_embed = batch\n",
    "    __, struct_embed = seq_embed.to(device), struct_embed.to(device)\n",
    "    \n",
    "    seq_embed = seq_encoder(seqs_list)  # [B, L, 1280]\n",
    "    \n",
    "    # seq_embed = self.model.seq_encoder(seqs)  # [B, L, 1280]\n",
    "    seq_mask = create_key_padding_mask(seq_embed)\n",
    "    seq_pooled = create_mean_of_non_masked(seq_embed, seq_mask)  # [B, 1280]\n",
    "    seq_pooled_proj = seq_down(seq_pooled)\n",
    "\n",
    "    struct_mask = create_key_padding_mask(struct_embed)\n",
    "    struct_pooled = create_mean_of_non_masked(struct_embed, struct_mask)  # [B, 512]\n",
    "\n",
    "    seq_full = F.normalize(seq_pooled_proj, dim=-1)\n",
    "    struct_full = F.normalize(struct_pooled, dim=-1)\n",
    "\n",
    "    cos = F.cosine_similarity(seq_full, struct_full, dim=-1)  # [B]\n",
    "    cosine_similarities_AFTER.append(cos.mean().item())\n",
    "\n",
    "plt.figure(figsize=(6, 4))\n",
    "plt.hist(cosine_similarities_AFTER, bins=50, alpha=0.6, label=\"Cosine similarities before\", density=True)\n",
    "# plt.hist(neg_logits, bins=50, alpha=0.6, label=\"Negative pairs\", density=True)\n",
    "\n",
    "plt.xlabel(\"cos-sim\")\n",
    "plt.ylabel(\"Density\")\n",
    "plt.title(\"Cosine similarities ESM-2 and ESM-IF AFTER training!\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f20ff9c-c760-4cd5-8e1e-e8041124a531",
   "metadata": {},
   "source": [
    "#### Stop after epohs 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a136344-9b4a-4494-a293-693f1cb368aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Training seq_down for 2 epochs \n",
    "seq_encoder_checkpoint_path = \"/work3/s232958/data/trained/boostingESM2wESMIF/train_on_PPint/80599b33-d6bd-464a-819f-41a1758742dc/seq_encoder_10.pt\"\n",
    "seq_encoder_state_dict = torch.load(seq_encoder_checkpoint_path, map_location=device)\n",
    "seq_encoder = ESM2EncoderLoRA()\n",
    "seq_encoder.load_state_dict(seq_encoder_state_dict)\n",
    "seq_encoder.to(device)\n",
    "seq_encoder.eval()\n",
    "\n",
    "seq_down_checkpoint_path = \"/work3/s232958/data/trained/boostingESM2wESMIF/train_on_PPint/80599b33-d6bd-464a-819f-41a1758742dc/seq_down_10.pt\"\n",
    "seq_down_state_dict = torch.load(seq_down_checkpoint_path, map_location=device)\n",
    "seq_down = nn.Linear(1280, 512)\n",
    "seq_down.load_state_dict(seq_down_state_dict)\n",
    "seq_down.to(device)\n",
    "# seq_down.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "142b2055-c46b-48f1-aec9-38275ff7bcd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Similarity using original embeddings and trained projection head\n",
    "_loader = DataLoader(meta_Dataset, batch_size=5)\n",
    "cosine_similarities_BEFORE = []\n",
    "seq_proj = nn.Linear(1280, 512)\n",
    "\n",
    "for batch in _loader:\n",
    "    seq_embed, seqs_list, struct_embed = batch\n",
    "    seq_embed, struct_embed = seq_embed.to(device), struct_embed.to(device)\n",
    "    \n",
    "    # seq_embed = self.model.seq_encoder(seqs)  # [B, L, 1280]\n",
    "    seq_mask = create_key_padding_mask(seq_embed)\n",
    "    seq_pooled = create_mean_of_non_masked(seq_embed, seq_mask)  # [B, 1280]\n",
    "    seq_pooled_proj = seq_down(seq_pooled)\n",
    "    # seq_pooled_proj = seq_proj(seq_pooled)\n",
    "\n",
    "    struct_mask = create_key_padding_mask(struct_embed)\n",
    "    struct_pooled = create_mean_of_non_masked(struct_embed, struct_mask)  # [B, 512]\n",
    "\n",
    "    seq_full = F.normalize(seq_pooled_proj, dim=-1)\n",
    "    struct_full = F.normalize(struct_pooled, dim=-1)\n",
    "\n",
    "    cos = F.cosine_similarity(seq_full, struct_full, dim=-1)  # [B]\n",
    "    cosine_similarities_BEFORE.append(cos.mean().item())\n",
    "\n",
    "plt.figure(figsize=(6, 4))\n",
    "plt.hist(cosine_similarities_BEFORE, bins=50, alpha=0.6, label=\"Cosine similarities before\", density=True)\n",
    "# plt.hist(neg_logits, bins=50, alpha=0.6, label=\"Negative pairs\", density=True)\n",
    "\n",
    "plt.xlabel(\"cos-sim\")\n",
    "plt.ylabel(\"Density\")\n",
    "plt.title(\"Cosine similarities ESM-2 and ESM-IF before training!\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24f37992-e9f1-46b7-ab6c-ee104b86c2a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Similarity using original embeddings and random nn.Linear(1280, 512) head\n",
    "\n",
    "_loader = DataLoader(meta_Dataset, batch_size=5)\n",
    "cosine_similarities_BEFORE = []\n",
    "seq_proj = nn.Linear(1280, 512).to(device)\n",
    "\n",
    "for batch in _loader:\n",
    "    seq_embed, seqs_list, struct_embed = batch\n",
    "    seq_embed, struct_embed = seq_embed.to(device), struct_embed.to(device)\n",
    "    \n",
    "    # seq_embed = self.model.seq_encoder(seqs)  # [B, L, 1280]\n",
    "    seq_mask = create_key_padding_mask(seq_embed)\n",
    "    seq_pooled = create_mean_of_non_masked(seq_embed, seq_mask)  # [B, 1280]\n",
    "    # seq_pooled_proj = seq_down(seq_pooled)\n",
    "    seq_pooled_proj = seq_proj(seq_pooled)\n",
    "\n",
    "    struct_mask = create_key_padding_mask(struct_embed)\n",
    "    struct_pooled = create_mean_of_non_masked(struct_embed, struct_mask)  # [B, 512]\n",
    "\n",
    "    seq_full = F.normalize(seq_pooled_proj, dim=-1)\n",
    "    struct_full = F.normalize(struct_pooled, dim=-1)\n",
    "\n",
    "    cos = F.cosine_similarity(seq_full, struct_full, dim=-1)  # [B]\n",
    "    cosine_similarities_BEFORE.append(cos.mean().item())\n",
    "\n",
    "plt.figure(figsize=(6, 4))\n",
    "plt.hist(cosine_similarities_BEFORE, bins=50, alpha=0.6, label=\"Cosine similarities before\", density=True)\n",
    "# plt.hist(neg_logits, bins=50, alpha=0.6, label=\"Negative pairs\", density=True)\n",
    "\n",
    "plt.xlabel(\"cos-sim\")\n",
    "plt.ylabel(\"Density\")\n",
    "plt.title(\"Cosine similarities ESM-2 and ESM-IF before training!\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57698fd1-c3b7-47e8-8a34-5e620be5aca4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "### Similarity using both trained seq_encoder + trained proj_head\n",
    "_loader = DataLoader(meta_Dataset, batch_size=5)\n",
    "cosine_similarities_AFTER = []\n",
    "\n",
    "for batch in _loader:\n",
    "    __, seqs_list, struct_embed = batch\n",
    "    __, struct_embed = seq_embed.to(device), struct_embed.to(device)\n",
    "    \n",
    "    seq_embed = seq_encoder(seqs_list)  # [B, L, 1280]\n",
    "    \n",
    "    # seq_embed = self.model.seq_encoder(seqs)  # [B, L, 1280]\n",
    "    seq_mask = create_key_padding_mask(seq_embed)\n",
    "    seq_pooled = create_mean_of_non_masked(seq_embed, seq_mask)  # [B, 1280]\n",
    "    seq_pooled_proj = seq_down(seq_pooled)\n",
    "\n",
    "    struct_mask = create_key_padding_mask(struct_embed)\n",
    "    struct_pooled = create_mean_of_non_masked(struct_embed, struct_mask)  # [B, 512]\n",
    "\n",
    "    seq_full = F.normalize(seq_pooled_proj, dim=-1)\n",
    "    struct_full = F.normalize(struct_pooled, dim=-1)\n",
    "\n",
    "    cos = F.cosine_similarity(seq_full, struct_full, dim=-1)  # [B]\n",
    "    cosine_similarities_AFTER.append(cos.mean().item())\n",
    "\n",
    "plt.figure(figsize=(6, 4))\n",
    "plt.hist(cosine_similarities_AFTER, bins=50, alpha=0.6, label=\"Cosine similarities before\", density=True)\n",
    "# plt.hist(neg_logits, bins=50, alpha=0.6, label=\"Negative pairs\", density=True)\n",
    "\n",
    "plt.xlabel(\"cos-sim\")\n",
    "plt.ylabel(\"Density\")\n",
    "plt.title(\"Cosine similarities ESM-2 and ESM-IF AFTER training!\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75a3d682-bb77-4ab2-9104-15847d8fd8fc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "570e2df1-92aa-44a4-a4dc-4530f01c2934",
   "metadata": {},
   "source": [
    "### Contrastive Sequence-Structure Pre-training (CSSP) - 2\n",
    "\n",
    "Different learning rates for `seq_encoder` and `seq_down`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "933747ea-e3cb-474e-8581-4acc003316b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_key_padding_mask(embeddings, padding_value=-5000, offset=10):\n",
    "    \"\"\"\n",
    "    Purpose: return vector indicating which rows are not padded (don't have values = -5000)\n",
    "    \"\"\"\n",
    "    return (embeddings < (padding_value + offset)).all(dim=-1)\n",
    "\n",
    "def create_mean_of_non_masked(embeddings, padding_mask):\n",
    "    # Use masked select and mean to compute the mean of non-masked elements\n",
    "    # embeddings should be of shape (batch_size, seq_len, features)\n",
    "    seq_embeddings = []\n",
    "    for i in range(embeddings.shape[0]): # looping over all batch elements\n",
    "        non_masked_embeddings = embeddings[i][~padding_mask[i]] # shape [num_real_tokens, features]\n",
    "        if len(non_masked_embeddings) == 0:\n",
    "            print(\"You are masking all positions when creating sequence representation\")\n",
    "            sys.exit(1)\n",
    "        mean_embedding = non_masked_embeddings.mean(dim=0) # sequence is represented by the single vecotr [1280] [features]\n",
    "        seq_embeddings.append(mean_embedding)\n",
    "    return torch.stack(seq_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f1ef6dd-5a18-4986-b156-b754dd5cf333",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ESM2EncoderLoRA(nn.Module):\n",
    "    def __init__(self, padding_value=-5000.0):\n",
    "        super().__init__()\n",
    "\n",
    "        self.padding_value = padding_value\n",
    "\n",
    "        self.model = EsmModel.from_pretrained(\n",
    "            \"facebook/esm2_t33_650M_UR50D\",\n",
    "            output_hidden_states=True\n",
    "        )\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(\"facebook/esm2_t33_650M_UR50D\")\n",
    "\n",
    "        # Freeze original weights\n",
    "        for p in self.model.parameters():\n",
    "            p.requires_grad = False\n",
    "\n",
    "        # LoRA on top layers\n",
    "        lora_cfg = LoraConfig(\n",
    "            task_type=\"FEATURE_EXTRACTION\",\n",
    "            inference_mode=False,\n",
    "            r=4,\n",
    "            lora_alpha=1,\n",
    "            lora_dropout=0.1,\n",
    "            bias=\"none\",\n",
    "            # target_modules=[\"query\", \"key\", \"value\", \"dense\"],\n",
    "            target_modules=[\"query\", \"key\", \"value\", \"dense\"],\n",
    "            layers_to_transform=list(range(25, 33)),\n",
    "        )\n",
    "\n",
    "        self.model = get_peft_model(self.model, lora_cfg)\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def get_attentions(self, sequences):\n",
    "        inputs = self.tokenizer(\n",
    "            sequences, return_tensors=\"pt\", padding=True\n",
    "        ).to(self.model.device)\n",
    "\n",
    "        out = self.model(**inputs, output_attentions=True)\n",
    "        return out.attentions   # list[num_layers] → [B, num_heads, L, L]\n",
    "\n",
    "    def forward(self, sequences):\n",
    "        inputs = self.tokenizer(\n",
    "            sequences, return_tensors=\"pt\", padding=True\n",
    "        ).to(self.model.device)\n",
    "\n",
    "        out = self.model(**inputs)\n",
    "        reps = out.hidden_states[-1]                  # [B, Ltok, 1280]\n",
    "        reps = reps[:, 1:-1, :]                       # remove CLS/EOS\n",
    "\n",
    "        seq_lengths = [len(s) for s in sequences]\n",
    "        Lmax = max(seq_lengths)\n",
    "\n",
    "        B, D = reps.size(0), reps.size(-1)\n",
    "        padded = torch.full((B, Lmax, D), self.padding_value, device=reps.device)\n",
    "\n",
    "        for i, (r, real_len) in enumerate(zip(reps, seq_lengths)):\n",
    "            padded[i, :real_len] = r[:real_len]\n",
    "\n",
    "        return padded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24aaf3bb-962d-433b-9006-976584bdd0b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CSSPBoostingESM(nn.Module):\n",
    "    def __init__(self, seq_embed_dim=1280, struct_embed_dim=512, padding_value=-5000):\n",
    "        super().__init__()\n",
    "        self.padding_value = padding_value\n",
    "        self.seq_encoder = ESM2EncoderLoRA()\n",
    "        self.seq_down = nn.Linear(seq_embed_dim, struct_embed_dim)\n",
    "\n",
    "        # for p in self.struct_up.parameters():\n",
    "        #     p.requires_grad = False\n",
    "            \n",
    "        self.logit_scale = nn.Parameter(torch.tensor(math.log(1/0.07)))\n",
    "\n",
    "    def forward(self, sequences, struct_embed):\n",
    "        \n",
    "        seq_embed = self.seq_encoder(sequences)  # [B, L, 1280]\n",
    "        seq_mask = create_key_padding_mask(seq_embed, self.padding_value)\n",
    "        seq_pooled = create_mean_of_non_masked(seq_embed, seq_mask)          # [B, 1280]\n",
    "        seq_pooled_proj = self.seq_down(seq_pooled)                          # [B, 512]\n",
    "    \n",
    "        struct_mask = create_key_padding_mask(struct_embed, self.padding_value)\n",
    "        struct_pooled = create_mean_of_non_masked(struct_embed, struct_mask) # [B, 512]\n",
    "    \n",
    "        seq_full = F.normalize(seq_pooled_proj, dim=-1)\n",
    "        struct_full = F.normalize(struct_pooled, dim=-1)\n",
    "    \n",
    "        scale = torch.exp(self.logit_scale).clamp(max=100)\n",
    "        logits_seq    = scale * (seq_full @ struct_full.T)   # [B, B]\n",
    "        logits_struct = scale * (struct_full @ seq_full.T)   # [B, B]\n",
    "    \n",
    "        return logits_seq, logits_struct, seq_pooled_proj, struct_pooled, struct_mask\n",
    "\n",
    "    def training_step(self, batch, device):\n",
    "        sequences, struct_embed, _ = batch\n",
    "        struct_embed = struct_embed.to(device)\n",
    "        logits_seq, logits_struct, seq_pooled_proj, struct_pooled, struct_mask = self.forward(sequences, struct_embed)\n",
    "\n",
    "        B = logits_seq.shape[0]\n",
    "        labels = torch.arange(B, device=device)\n",
    "\n",
    "        loss_seq    = F.cross_entropy(logits_seq, labels)\n",
    "        loss_struct = F.cross_entropy(logits_struct, labels)\n",
    "        clip_loss = (loss_seq + loss_struct) / 2\n",
    "\n",
    "        return clip_loss "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00cdff1b-ea3e-4703-a2d7-67a3e5fc7f31",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = CSSPBoostingESM(\n",
    "    seq_embed_dim=1280,\n",
    "    struct_embed_dim=512,\n",
    "    padding_value=-5000,\n",
    ").to(device)\n",
    "\n",
    "# model\n",
    "\n",
    "runID = uuid.uuid4()\n",
    "learning_rate = 2e-5\n",
    "EPOCHS = 10\n",
    "# batch_size = 3\n",
    "model = CSSPBoostingESM(seq_embed_dim=1280, struct_embed_dim=512).to(\"cuda\")\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "optimizer = AdamW(model.parameters(), lr=learning_rate)\n",
    "accelerator = Accelerator()\n",
    "device = accelerator.device\n",
    "\n",
    "train_dataloader = DataLoader(train_Dataset, batch_size=10, shuffle=True)\n",
    "test_dataloader = DataLoader(test_Dataset, batch_size=10, shuffle=False)\n",
    "val_dataloader = DataLoader(meta_Dataset, batch_size=10, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ea2ab6e-3ce7-437b-8812-1fd551151a55",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "- cos-similarity calculated per sequence\n",
    "- train seq_down for 2 epochs and then freeze all parameters\n",
    "\"\"\"\n",
    "\n",
    "class TrainWrapper():\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        model=model,\n",
    "        train_loader=train_dataloader,\n",
    "        test_loader=test_dataloader,\n",
    "        val_loader=val_dataloader,\n",
    "        optimizer=optimizer,\n",
    "        epochs=EPOCHS,\n",
    "        device=device\n",
    "    ):\n",
    "        self.model = model\n",
    "        self.training_loader = train_loader\n",
    "        self.testing_loader = test_loader\n",
    "        self.validation_loader = val_loader\n",
    "        self.EPOCHS = epochs\n",
    "        self.optimizer = optimizer\n",
    "        self.device = device\n",
    "\n",
    "\n",
    "    def train_one_epoch(self):\n",
    "\n",
    "        self.model.train()\n",
    "        self.model.seq_encoder.train()\n",
    "\n",
    "        running_loss = 0\n",
    "\n",
    "        for batch in tqdm(self.training_loader, total=len(self.training_loader), desc=\"Running through epoch\"):\n",
    "\n",
    "            sequences, struct_embed, labels = batch\n",
    "            struct_embed = struct_embed.to(self.device)\n",
    "            labels = labels.to(self.device)\n",
    "\n",
    "            self.optimizer.zero_grad()\n",
    "            loss = self.model.training_step((sequences, struct_embed, labels), self.device)\n",
    "\n",
    "            loss.backward()\n",
    "            self.optimizer.step()\n",
    "\n",
    "            running_loss += loss.item()\n",
    "\n",
    "        return running_loss / len(self.training_loader)\n",
    "    \n",
    "\n",
    "    @torch.no_grad()\n",
    "    def compute_embeddings_cos_similariy(self, loader=None, loader_name=\"test\"):\n",
    "\n",
    "        if loader is None:\n",
    "            loader = self.testing_loader\n",
    "\n",
    "        self.model.eval()\n",
    "        self.model.seq_encoder.eval()\n",
    "\n",
    "        all_embeds = []\n",
    "        cosine_similarities = []\n",
    "\n",
    "        for batch in tqdm(loader, desc=f\"Computing cosine similarity & embeddings ({loader_name})\"):\n",
    "\n",
    "            seqs, struct_embed, _ = batch\n",
    "            struct_embed = struct_embed.to(self.device)\n",
    "\n",
    "            # ---- sequence embeddings ----\n",
    "            seq_embed = self.model.seq_encoder(seqs)  # [B, L, 1280]\n",
    "            seq_mask = create_key_padding_mask(seq_embed)  # NOTE: only correct if seq_embed padding uses -5000\n",
    "            seq_pooled = create_mean_of_non_masked(seq_embed, seq_mask)  # [B, 1280]\n",
    "            seq_pooled_proj = self.model.seq_down(seq_pooled)\n",
    "\n",
    "            # ---- structure embeddings ----\n",
    "            struct_mask = create_key_padding_mask(struct_embed)\n",
    "            struct_pooled = create_mean_of_non_masked(struct_embed, struct_mask)  # [B, 512]\n",
    "\n",
    "            # ---- cosine similarity in CLIP space (per sequence) ----\n",
    "            seq_full = F.normalize(seq_pooled_proj, dim=-1)\n",
    "            struct_full = F.normalize(struct_pooled, dim=-1)\n",
    "\n",
    "            cos = F.cosine_similarity(seq_full, struct_full, dim=-1)  # [B]\n",
    "\n",
    "            cosine_similarities.extend(cos.cpu().tolist())\n",
    "            all_embeds.extend(seq_full.detach().cpu())\n",
    "\n",
    "        all_embeds = torch.stack(all_embeds)\n",
    "        avg_cos = float(np.mean(cosine_similarities))\n",
    "        std_cos = float(np.std(cosine_similarities))\n",
    "\n",
    "        return all_embeds, cosine_similarities, avg_cos, std_cos\n",
    "\n",
    "\n",
    "    def plot_embeddings_drift_cos_similarity_change(self, start_embeddings, end_embeddings, cosine_similarities):\n",
    "\n",
    "        drift = (end_embeddings - start_embeddings).norm(dim=1).cpu().numpy()\n",
    "        cosine_similarities = np.array(cosine_similarities)\n",
    "\n",
    "        fig, ax = plt.subplots(1, 2, figsize=(8, 4))\n",
    "\n",
    "        ax[0].hist(drift, bins=30, color=\"steelblue\", alpha=0.8)\n",
    "        ax[0].set_title(\"Embedding Drift per Sequence\", fontsize=8)\n",
    "        ax[0].set_xlabel(\"L2 Norm Drift\", fontsize=8)\n",
    "        ax[0].set_ylabel(\"Density\", fontsize=8)\n",
    "\n",
    "        ax[1].hist(cosine_similarities, bins=40, color=\"darkorange\", alpha=0.7, density=True)\n",
    "        ax[1].set_title(\"Cosine Similarities (ESM-2 vs ESM-IF)\", fontsize=8)\n",
    "        ax[1].set_xlabel(\"Cosine Similarity\", fontsize=8)\n",
    "        ax[1].set_ylabel(\"Density\", fontsize=8)\n",
    "\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "\n",
    "    def train_model(self, save_every: int = 5):\n",
    "\n",
    "        run_dir = f\"/work3/s232958/data/trained/boostingESM2wESMIF/train_on_PPint/{runID}/\"\n",
    "        os.makedirs(run_dir, exist_ok=True)\n",
    "\n",
    "        print(\"\\nTrainable parameters inside seq_encoder (LoRA layers):\")\n",
    "        for name, p in self.model.seq_encoder.named_parameters():\n",
    "            if p.requires_grad:\n",
    "                print(\"  \", name)\n",
    "\n",
    "        # ---- Save checkpoint BEFORE training (epoch 0) ----\n",
    "        save_path_encoder = os.path.join(run_dir, \"ESM2EncoderLoRA_epoch_0.pt\")\n",
    "        save_path_projhead = os.path.join(run_dir, \"ProjHead_epoch_0.pt\")\n",
    "        torch.save(self.model.seq_encoder.state_dict(), save_path_encoder)\n",
    "        torch.save(self.model.seq_down.state_dict(), save_path_projhead)\n",
    "        print(f\"Saved seq_encoder checkpoint before training -> {save_path_encoder}\")\n",
    "        print(f\"Saved proj head checkpoint before training -> {save_path_projhead}\")\n",
    "\n",
    "        # ---- START embeddings/cos-sim for both loaders ----\n",
    "        print(\"\\nExtracting START embeddings & cosine similarities (val + test)...\")\n",
    "        start_val_emb, start_val_cos, start_val_avg, start_val_std = self.compute_embeddings_cos_similariy(\n",
    "            loader=self.validation_loader, loader_name=\"val\"\n",
    "        )\n",
    "        print(f\"[VAL]  avg cos: {start_val_avg:.4f}, std: {start_val_std:.4f}\")\n",
    "\n",
    "        start_test_emb, start_test_cos, start_test_avg, start_test_std = self.compute_embeddings_cos_similariy(\n",
    "            loader=self.testing_loader, loader_name=\"test\"\n",
    "        )\n",
    "        print(f\"[TEST] avg cos: {start_test_avg:.4f}, std: {start_test_std:.4f}\")\n",
    "\n",
    "        # ---- Train ONLY 2 epochs (seq_down trainable), then freeze everything and STOP ----\n",
    "        max_train_epochs = min(self.EPOCHS, 2)\n",
    "\n",
    "        for epoch in range(1, max_train_epochs + 1):\n",
    "\n",
    "            train_loss = self.train_one_epoch()\n",
    "            print(f\"Epoch {epoch}: loss={train_loss:.4f}\")\n",
    "\n",
    "            # ---- optional checkpoints ----\n",
    "            if epoch % save_every == 0:\n",
    "                save_path_encoder = os.path.join(run_dir, f\"ESM2EncoderLoRA_epoch_{epoch}.pt\")\n",
    "                save_path_projhead = os.path.join(run_dir, f\"ProjHead_epoch_{epoch}.pt\")\n",
    "                torch.save(self.model.seq_encoder.state_dict(), save_path_encoder)\n",
    "                torch.save(self.model.seq_down.state_dict(), save_path_projhead)\n",
    "                print(f\"Saved seq_encoder checkpoint -> {save_path_encoder}\")\n",
    "                print(f\"Saved proj head checkpoint -> {save_path_projhead}\")\n",
    "\n",
    "        # ---- Freeze ALL parameters after 2 epochs (full stop) ----\n",
    "        print(\"\\nFreezing ALL model parameters after 2 epochs (full stop)...\")\n",
    "        for p in self.model.parameters():\n",
    "            p.requires_grad = False\n",
    "\n",
    "        # ---- END embeddings/cos-sim for both loaders ----\n",
    "        print(\"\\nExtracting END embeddings & cosine similarities (val + test)...\")\n",
    "        end_val_emb, end_val_cos, end_val_avg, end_val_std = self.compute_embeddings_cos_similariy(\n",
    "            loader=self.validation_loader, loader_name=\"val\"\n",
    "        )\n",
    "        print(f\"[VAL]  avg cos: {end_val_avg:.4f}, std: {end_val_std:.4f}\")\n",
    "\n",
    "        end_test_emb, end_test_cos, end_test_avg, end_test_std = self.compute_embeddings_cos_similariy(\n",
    "            loader=self.testing_loader, loader_name=\"test\"\n",
    "        )\n",
    "        print(f\"[TEST] avg cos: {end_test_avg:.4f}, std: {end_test_std:.4f}\")\n",
    "\n",
    "        # Optional plots (val + test)\n",
    "        # self.plot_embeddings_drift_cos_similarity_change(start_val_emb, end_val_emb, end_val_cos)\n",
    "        # self.plot_embeddings_drift_cos_similarity_change(start_test_emb, end_test_emb, end_test_cos)\n",
    "\n",
    "        return {\n",
    "            \"val\":  (start_val_emb,  end_val_emb,  end_val_cos),\n",
    "            \"test\": (start_test_emb, end_test_emb, end_test_cos),\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dacfce9-d166-4e1e-beda-b1cd1a51c67f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# accelerator\n",
    "model, optimizer, train_dataloader = accelerator.prepare(model, optimizer, train_dataloader)\n",
    "\n",
    "training_wrapper = TrainWrapper(\n",
    "    model=model,\n",
    "    train_loader=train_dataloader,\n",
    "    test_loader=test_dataloader,\n",
    "    optimizer=optimizer,\n",
    "    epochs=EPOCHS,\n",
    "    device=device,\n",
    ")\n",
    "\n",
    "val_params, test_params = training_wrapper.train_model(save_every=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5f9ce55-97ec-40cb-946f-1ca150a1e22d",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "#### Some testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70058f1b-9a3a-45ab-a602-c2cfafb2f9a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ESM2EncoderLoRA(nn.Module):\n",
    "    def __init__(self, padding_value=-5000.0):\n",
    "        super().__init__()\n",
    "\n",
    "        self.padding_value = padding_value\n",
    "\n",
    "        self.model = EsmModel.from_pretrained(\n",
    "            \"facebook/esm2_t33_650M_UR50D\",\n",
    "            output_hidden_states=True\n",
    "        )\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(\"facebook/esm2_t33_650M_UR50D\")\n",
    "\n",
    "        # Freeze original weights\n",
    "        for p in self.model.parameters():\n",
    "            p.requires_grad = False\n",
    "\n",
    "        # LoRA on top layers\n",
    "        lora_cfg = LoraConfig(\n",
    "            task_type=\"FEATURE_EXTRACTION\",\n",
    "            inference_mode=False,\n",
    "            r=16,\n",
    "            lora_alpha=128,\n",
    "            lora_dropout=0.1,\n",
    "            bias=\"none\",\n",
    "            target_modules=[\"query\", \"key\", \"value\", \"dense\"],\n",
    "            layers_to_transform=list(range(25, 33)),\n",
    "        )\n",
    "\n",
    "        self.model = get_peft_model(self.model, lora_cfg)\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def get_attentions(self, sequences):\n",
    "        inputs = self.tokenizer(\n",
    "            sequences, return_tensors=\"pt\", padding=True\n",
    "        ).to(self.model.device)\n",
    "\n",
    "        out = self.model(**inputs, output_attentions=True)\n",
    "        return out.attentions   # list[num_layers] → [B, num_heads, L, L]\n",
    "\n",
    "    def forward(self, sequences):\n",
    "        inputs = self.tokenizer(\n",
    "            sequences, return_tensors=\"pt\", padding=True\n",
    "        ).to(self.model.device)\n",
    "\n",
    "        out = self.model(**inputs)\n",
    "        reps = out.hidden_states[-1]                  # [B, Ltok, 1280]\n",
    "        reps = reps[:, 1:-1, :]                       # remove CLS/EOS\n",
    "\n",
    "        seq_lengths = [len(s) for s in sequences]\n",
    "        Lmax = max(seq_lengths)\n",
    "\n",
    "        B, D = reps.size(0), reps.size(-1)\n",
    "        padded = torch.full((B, Lmax, D), self.padding_value, device=reps.device)\n",
    "\n",
    "        for i, (r, real_len) in enumerate(zip(reps, seq_lengths)):\n",
    "            padded[i, :real_len] = r[:real_len]\n",
    "\n",
    "        return padded"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ac8287e-5b29-43c6-a7ed-8c7e85dd6471",
   "metadata": {},
   "source": [
    "$\\cos(\\theta) = \\frac{a \\cdot b}{\\|a\\| \\, \\|b\\|}$\n",
    "\n",
    "`numpy.linalg.norm` computes the vector norm, usually the Euclidean norm also called the $L2$ norm.\n",
    "\n",
    "For vector $x = (x_1, x_2, ..., x_n)$: \n",
    "\n",
    "${\\|x\\|} = \\sqrt{{x_1}^2 + {x_2}^2 + ... + {x_n}^2}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "901372af-f9d7-4993-bd76-0587f5a51c21",
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0c40fe9-bdb0-4c6e-a8a8-38b13c5b76a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# _loader = DataLoader(test_Dataset, batch_size=5)\n",
    "# proj = nn.Linear(1280, 512).to(device)\n",
    "# encoder_ = ESM2EncoderLoRA().eval().to(device)\n",
    "# # encoder_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34d9ce5a-c93d-49ba-9b38-62a736cd3cd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "cosine_similarities = []\n",
    "\n",
    "for batch in tqdm(_loader, total=len(_loader)):\n",
    "    seqs, structs_emeb, lbls = batch\n",
    "    seqs_emeb = encoder_(seqs)\n",
    "    seqs_emeb_proj = proj(seqs_emeb.to(device))          # shape: [B, L, 512]\n",
    "\n",
    "    # Create masks (must match projected embeddings)\n",
    "    seq_mask = create_key_padding_mask(seqs_emeb_proj)\n",
    "    struct_mask = create_key_padding_mask(structs_emeb)\n",
    "\n",
    "    # Mean-pool (returns shape: [B, 512])\n",
    "    vectors_seq = create_mean_of_non_masked(seqs_emeb_proj, seq_mask).cpu().detach().numpy()\n",
    "    vectors_struct = create_mean_of_non_masked(structs_emeb, struct_mask).cpu().detach().numpy()\n",
    "\n",
    "    # Loop through batch items\n",
    "    for i in range(vectors_seq.shape[0]):\n",
    "        cosine = np.dot(vectors_seq[i], vectors_struct[i]) / (norm(vectors_seq[i]) * norm(vectors_struct[i]))\n",
    "        cosine_similarities.append(cosine)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f8ee13f-9642-4250-ad33-ebb97b53cd14",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(6, 4))\n",
    "plt.hist(cosine_similarities, bins=50, alpha=0.6, label=\"Cosine similarities before\", density=True)\n",
    "# plt.hist(neg_logits, bins=50, alpha=0.6, label=\"Negative pairs\", density=True)\n",
    "\n",
    "plt.xlabel(\"cos-sim\")\n",
    "plt.ylabel(\"Density\")\n",
    "plt.title(\"Cosine similarities ESM-2 and ESM-IF before training!\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74d0695c-91ce-47bf-a2a2-bf6abe9426c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_key_padding_mask(embeddings, padding_value=-5000, offset=10):\n",
    "    \"\"\"\n",
    "    Purpose: return vector indicating which rows are not padded (don't have values = -5000)\n",
    "    \"\"\"\n",
    "    return (embeddings < (padding_value + offset)).all(dim=-1)\n",
    "\n",
    "def create_mean_of_non_masked(embeddings, padding_mask):\n",
    "    # Use masked select and mean to compute the mean of non-masked elements\n",
    "    # embeddings should be of shape (batch_size, seq_len, features)\n",
    "    seq_embeddings = []\n",
    "    for i in range(embeddings.shape[0]): # looping over all batch elements\n",
    "        non_masked_embeddings = embeddings[i][~padding_mask[i]] # shape [num_real_tokens, features]\n",
    "        if len(non_masked_embeddings) == 0:\n",
    "            print(\"You are masking all positions when creating sequence representation\")\n",
    "            sys.exit(1)\n",
    "        mean_embedding = non_masked_embeddings.mean(dim=0) # sequence is represented by the single vecotr [1280] [features]\n",
    "        seq_embeddings.append(mean_embedding)\n",
    "    return torch.stack(seq_embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03bd98fa-16fb-41f4-ac75-58f19e9753ff",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "#### ESM2 encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc25b295-413b-4744-9848-4b4ca98f9b8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# class ESM2EncoderOriginal(nn.Module):\n",
    "#     def __init__(self, padding_value=-5000.0):\n",
    "#         super().__init__()\n",
    "\n",
    "#         self.padding_value = padding_value\n",
    "\n",
    "#         self.model = EsmModel.from_pretrained(\n",
    "#             \"facebook/esm2_t33_650M_UR50D\",\n",
    "#             output_hidden_states=True\n",
    "#         )\n",
    "#         self.tokenizer = AutoTokenizer.from_pretrained(\"facebook/esm2_t33_650M_UR50D\")\n",
    "\n",
    "#     @torch.no_grad()\n",
    "#     def get_attentions(self, sequences):\n",
    "#         inputs = self.tokenizer(\n",
    "#             sequences, return_tensors=\"pt\", padding=True\n",
    "#         ).to(self.model.device)\n",
    "\n",
    "#         out = self.model(**inputs, output_attentions=True)\n",
    "#         return out.attentions   # list[num_layers] → [B, num_heads, L, L]\n",
    "\n",
    "#     def forward(self, sequences):\n",
    "#         inputs = self.tokenizer(\n",
    "#             sequences, return_tensors=\"pt\", padding=True\n",
    "#         ).to(self.model.device)\n",
    "\n",
    "#         out = self.model(**inputs)\n",
    "#         reps = out.hidden_states[-1]                  # [B, Ltok, 1280]\n",
    "#         reps = reps[:, 1:-1, :]                       # remove CLS/EOS\n",
    "\n",
    "#         seq_lengths = [len(s) for s in sequences]\n",
    "#         Lmax = max(seq_lengths)\n",
    "\n",
    "#         B, D = reps.size(0), reps.size(-1)\n",
    "#         padded = torch.full((B, Lmax, D), self.padding_value, device=reps.device)\n",
    "\n",
    "#         for i, (r, real_len) in enumerate(zip(reps, seq_lengths)):\n",
    "#             padded[i, :real_len] = r[:real_len]\n",
    "\n",
    "#         return padded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d78c8e79-d35f-48b0-8e27-b5d1262bc504",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sequences = []\n",
    "# for i, row in Df_test_LONG.iterrows():\n",
    "#     seq_name = f\"prot_{i}\"\n",
    "#     sequence = str(row.sequence)\n",
    "#     entry = (seq_name, sequence)\n",
    "#     sequences.append(entry)\n",
    "# sequences[:5]\n",
    "\n",
    "# # Embedings original seqeunces\n",
    "# encoder = ESM2EncoderOriginal().to(\"cuda\")   # create model\n",
    "# embeds = encoder(seq)\n",
    "\n",
    "# print(embeds.shape)\n",
    "\n",
    "# # Mean pooling\n",
    "# seq_mask_emb = create_key_padding_mask(embeddings = embeds, padding_value = -5000.0).to(device)\n",
    "# seq_embed_pooled_START = create_mean_of_non_masked(embeds, seq_mask_emb)\n",
    "# seq_embed_pooled_START"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f3217ae-c608-40b0-a558-c408647a9c9e",
   "metadata": {},
   "source": [
    "### ESM encoder with LoRA added"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee0b3435-47cc-4459-aecb-bafb8171f227",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ESM2EncoderLoRA(nn.Module):\n",
    "    def __init__(self, padding_value=-5000.0):\n",
    "        super().__init__()\n",
    "\n",
    "        self.padding_value = padding_value\n",
    "\n",
    "        self.model = EsmModel.from_pretrained(\n",
    "            \"facebook/esm2_t33_650M_UR50D\",\n",
    "            output_hidden_states=True\n",
    "        )\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(\"facebook/esm2_t33_650M_UR50D\")\n",
    "\n",
    "        # Freeze original weights\n",
    "        for p in self.model.parameters():\n",
    "            p.requires_grad = False\n",
    "\n",
    "        # LoRA on top layers\n",
    "        lora_cfg = LoraConfig(\n",
    "            task_type=\"FEATURE_EXTRACTION\",\n",
    "            inference_mode=False,\n",
    "            r=4,\n",
    "            lora_alpha=1,\n",
    "            lora_dropout=0.1,\n",
    "            bias=\"none\",\n",
    "            # target_modules=[\"query\", \"key\", \"value\", \"dense\"],\n",
    "            target_modules=[\"query\", \"key\", \"value\", \"dense\"],\n",
    "            layers_to_transform=list(range(25, 33)),\n",
    "        )\n",
    "\n",
    "        self.model = get_peft_model(self.model, lora_cfg)\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def get_attentions(self, sequences):\n",
    "        inputs = self.tokenizer(\n",
    "            sequences, return_tensors=\"pt\", padding=True\n",
    "        ).to(self.model.device)\n",
    "\n",
    "        out = self.model(**inputs, output_attentions=True)\n",
    "        return out.attentions   # list[num_layers] → [B, num_heads, L, L]\n",
    "\n",
    "    def forward(self, sequences):\n",
    "        inputs = self.tokenizer(\n",
    "            sequences, return_tensors=\"pt\", padding=True\n",
    "        ).to(self.model.device)\n",
    "\n",
    "        out = self.model(**inputs)\n",
    "        reps = out.hidden_states[-1]                  # [B, Ltok, 1280]\n",
    "        reps = reps[:, 1:-1, :]                       # remove CLS/EOS\n",
    "\n",
    "        seq_lengths = [len(s) for s in sequences]\n",
    "        Lmax = max(seq_lengths)\n",
    "\n",
    "        B, D = reps.size(0), reps.size(-1)\n",
    "        padded = torch.full((B, Lmax, D), self.padding_value, device=reps.device)\n",
    "\n",
    "        for i, (r, real_len) in enumerate(zip(reps, seq_lengths)):\n",
    "            padded[i, :real_len] = r[:real_len]\n",
    "\n",
    "        return padded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ebee1fd-ed97-4488-8f07-bdfc25b3779b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# encoder = ESM2EncoderLoRA()\n",
    "# encoder = encoder.eval()\n",
    "# sequences = [\"MKTAGALA\",\"GAY\"]\n",
    "# padded, mask = encoder(sequences)\n",
    "# print(padded.shape)\n",
    "# seq_mask_emb = create_key_padding_mask(embeddings = padded, padding_value = -5000.0).to(device)\n",
    "# seq_embed_pooled_START = create_mean_of_non_masked(padded, seq_mask_emb)\n",
    "# print(seq_embed_pooled_START)\n",
    "# print(padded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7660987-acc6-4d72-8c52-a5afafab55be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Embedings original seqeunces\n",
    "# sequences = [\n",
    "#     (\"seq1\", \"MKTAGALA\"),\n",
    "#     (\"seq2\", \"GAY\")\n",
    "# ]\n",
    "# encoder = ESM2EncoderOriginal()   # create model\n",
    "# encoder = encoder.eval()\n",
    "# embeds = encoder(sequences)\n",
    "\n",
    "# # Mean pooling\n",
    "# seq_mask_emb = create_key_padding_mask(embeddings = embeds, padding_value = -5000.0)\n",
    "# seq_embed_pooled_START = create_mean_of_non_masked(embeds, seq_mask_emb)\n",
    "# seq_embed_pooled_START\n",
    "# print(embeds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5f126da-f8e7-42bb-8259-f5a64fe19930",
   "metadata": {},
   "source": [
    "### CSSPBoostingESM model - 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ff662cb-dd28-41d2-86ee-c23275fd8588",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ESM2EncoderLoRA(nn.Module):\n",
    "    def __init__(self, padding_value=-5000.0):\n",
    "        super().__init__()\n",
    "\n",
    "        self.padding_value = padding_value\n",
    "\n",
    "        self.model = EsmModel.from_pretrained(\n",
    "            \"facebook/esm2_t33_650M_UR50D\",\n",
    "            output_hidden_states=True\n",
    "        )\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(\"facebook/esm2_t33_650M_UR50D\")\n",
    "\n",
    "        # Freeze original weights\n",
    "        for p in self.model.parameters():\n",
    "            p.requires_grad = False\n",
    "\n",
    "        # LoRA on top layers\n",
    "        lora_cfg = LoraConfig(\n",
    "            task_type=\"FEATURE_EXTRACTION\",\n",
    "            inference_mode=False,\n",
    "            r=4,\n",
    "            lora_alpha=1,\n",
    "            lora_dropout=0.1,\n",
    "            bias=\"none\",\n",
    "            # target_modules=[\"query\", \"key\", \"value\", \"dense\"],\n",
    "            target_modules=[\"query\", \"key\", \"value\", \"dense\"],\n",
    "            layers_to_transform=list(range(25, 33)),\n",
    "        )\n",
    "\n",
    "        self.model = get_peft_model(self.model, lora_cfg)\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def get_attentions(self, sequences):\n",
    "        inputs = self.tokenizer(\n",
    "            sequences, return_tensors=\"pt\", padding=True\n",
    "        ).to(self.model.device)\n",
    "\n",
    "        out = self.model(**inputs, output_attentions=True)\n",
    "        return out.attentions   # list[num_layers] → [B, num_heads, L, L]\n",
    "\n",
    "    def forward(self, sequences):\n",
    "        inputs = self.tokenizer(\n",
    "            sequences, return_tensors=\"pt\", padding=True\n",
    "        ).to(self.model.device)\n",
    "\n",
    "        out = self.model(**inputs)\n",
    "        reps = out.hidden_states[-1]                  # [B, Ltok, 1280]\n",
    "        reps = reps[:, 1:-1, :]                       # remove CLS/EOS\n",
    "\n",
    "        seq_lengths = [len(s) for s in sequences]\n",
    "        Lmax = max(seq_lengths)\n",
    "\n",
    "        B, D = reps.size(0), reps.size(-1)\n",
    "        padded = torch.full((B, Lmax, D), self.padding_value, device=reps.device)\n",
    "\n",
    "        for i, (r, real_len) in enumerate(zip(reps, seq_lengths)):\n",
    "            padded[i, :real_len] = r[:real_len]\n",
    "\n",
    "        return padded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a85de7c-0e55-45e3-881f-67ed31ed55b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CSSPBoostingESM(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        seq_embed_dim=1280,\n",
    "        struct_embed_dim=512,\n",
    "        padding_value=-5000,\n",
    "        num_heads=8,\n",
    "        num_recycles=2\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        self.seq_embed_dim = seq_embed_dim\n",
    "        self.struct_embed_dim = struct_embed_dim\n",
    "        self.padding_value = padding_value\n",
    "        self.logit_scale = nn.Parameter(torch.tensor(math.log(1/0.07)))\n",
    "\n",
    "        # ESM2 encoder\n",
    "        self.seq_encoder = ESM2EncoderLoRA()\n",
    "        self.seq_down = nn.Linear(self.seq_embed_dim, self.struct_embed_dim)\n",
    "\n",
    "    def forward(self, sequences, struct_embed):\n",
    "        \n",
    "        seq_embed = self.seq_encoder(sequences)        # [B, Ls, 1280]\n",
    "        seq_embed_proj = self.seq_down(seq_embed)          # [B, Ls, 512]\n",
    "\n",
    "        ### pooling\n",
    "        seq_mask = create_key_padding_mask(seq_embed_proj, self.padding_value)\n",
    "        struct_mask = create_key_padding_mask(struct_embed, self.padding_value)\n",
    "        \n",
    "        seq_pooled = create_mean_of_non_masked(seq_embed_proj, seq_mask)      \n",
    "        struct_pooled = create_mean_of_non_masked(struct_embed, struct_mask)\n",
    "        \n",
    "        seq_full = F.normalize(seq_pooled, dim=-1)\n",
    "        struct_full = F.normalize(struct_pooled, dim=-1)\n",
    "        \n",
    "        scale = torch.exp(self.logit_scale).clamp(max=100)\n",
    "    \n",
    "        logits_seq = scale * seq_full @ struct_full.T     # seq → struct\n",
    "        logits_struct = scale * struct_full @ seq_full.T  # struct → seq\n",
    "\n",
    "        # cos_loss = 1 - F.cosine_similarity(seq_full, struct_full).mean()\n",
    "    \n",
    "        return logits_seq, logits_struct\n",
    "\n",
    "    def training_step(self, batch, device):\n",
    "        sequences, struct_embed, labels = batch\n",
    "        \n",
    "        logits_seq, logits_struct = self.forward(sequences, struct_embed.to(device))\n",
    "        \n",
    "        B = len(sequences)\n",
    "\n",
    "        labels = torch.arange(B, device=logits_seq.device)\n",
    "        \n",
    "        loss_seq = F.cross_entropy(logits_seq, labels)\n",
    "        loss_struct = F.cross_entropy(logits_struct, labels)\n",
    "        \n",
    "        loss = (loss_seq + loss_struct) / 2\n",
    "\n",
    "        return loss\n",
    "\n",
    "    def validation_step_PPint(self, batch, device):\n",
    "\n",
    "        sequences, struct_embed, labels = batch\n",
    "        struct_embed = struct_embed.to(device)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "\n",
    "            B = len(sequences)\n",
    "    \n",
    "            positive_logits = self.forward(sequences, struct_embed)\n",
    "            positive_loss = F.binary_cross_entropy_with_logits(positive_logits, torch.ones_like(positive_logits).to(device))\n",
    "\n",
    "            rows, cols = torch.triu_indices(B, B, offset=1)\n",
    "            sequences_list  = [sequences[i] for i in rows.tolist()]  # list of [Li, 256]\n",
    "\n",
    "            negative_logits = self.forward(sequences_list, struct_embed[cols, :, :])\n",
    "            negative_loss =  F.binary_cross_entropy_with_logits(negative_logits, torch.zeros_like(negative_logits).to(device))\n",
    "                        \n",
    "            loss = (positive_loss + negative_loss) / 2\n",
    "            \n",
    "            logit_matrix = torch.zeros((B, B), device=device)\n",
    "            logit_matrix[rows, cols] = negative_logits\n",
    "            logit_matrix[cols, rows] = negative_logits\n",
    "            diag_indices = torch.arange(B, device=device)\n",
    "            logit_matrix[diag_indices, diag_indices] = positive_logits.squeeze()\n",
    "\n",
    "            labels = torch.arange(B).to(device)\n",
    "            accuracy = peptide_predictions.eq(labels).float().mean()\n",
    "    \n",
    "            return loss, accuracy\n",
    "\n",
    "    def validation_step_MetaDataset(self, batch, device):\n",
    "        sequences, struct_embed, labels = batch\n",
    "        struct_embed, labels = struct_embed.to(device), labels.float().to(device)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            \n",
    "            logits = self.forward(sequences, struct_embed).float()\n",
    "            loss = F.binary_cross_entropy_with_logits(logits, labels)\n",
    "            \n",
    "            return logits, loss\n",
    "\n",
    "    def calculate_logit_matrix(self, sequences, struct_embed):\n",
    "        \n",
    "        B = len(sequences)\n",
    "        rows, cols = torch.triu_indices(B, B, offset=1)\n",
    "        \n",
    "        positive_logits = self.forward(sequences, struct_embed)\n",
    "        \n",
    "        sequences_list  = [sequences[i] for i in rows.tolist()] \n",
    "        negative_logits = self.forward(sequences[rows,:,:], struct_embed[cols,:,:])\n",
    "        \n",
    "        logit_matrix = torch.zeros((B, B), device=sequences.device)\n",
    "        logit_matrix[rows, cols] = negative_logits\n",
    "        logit_matrix[cols, rows] = negative_logits\n",
    "        \n",
    "        diag_indices = torch.arange(B, device=sequences.device)\n",
    "        logit_matrix[diag_indices, diag_indices] = positive_logits.squeeze()\n",
    "        \n",
    "        return logit_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c677930-6f7e-4559-b7c4-1d6b2915f861",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = CSSPBoostingESM(\n",
    "    seq_embed_dim=1280,\n",
    "    struct_embed_dim=512,\n",
    "    num_recycles=2\n",
    ").to(\"cuda\")\n",
    "\n",
    "# model\n",
    "\n",
    "runID = uuid.uuid4()\n",
    "learning_rate = 2e-5\n",
    "EPOCHS = 15\n",
    "# batch_size = 3\n",
    "model = CSSPBoostingESM(seq_embed_dim=1280, struct_embed_dim=512, num_recycles=2).to(\"cuda\")\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "optimizer = AdamW(model.parameters(), lr=learning_rate)\n",
    "accelerator = Accelerator()\n",
    "device = accelerator.device\n",
    "\n",
    "train_dataloader = DataLoader(train_Dataset, batch_size=5, shuffle=True)\n",
    "test_dataloader = DataLoader(test_Dataset, batch_size=5, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "537b05cd-c510-44b8-a3f4-829c816af718",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TrainWrapper():\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        model=model,\n",
    "        train_loader=train_dataloader,\n",
    "        test_loader=test_dataloader,\n",
    "        optimizer=optimizer,\n",
    "        epochs=EPOCHS,\n",
    "        device=device\n",
    "    ):\n",
    "        self.model = model\n",
    "        self.training_loader = train_loader\n",
    "        self.testing_loader = test_loader\n",
    "        self.EPOCHS = epochs\n",
    "        self.optimizer = optimizer\n",
    "        self.device = device\n",
    "\n",
    "        # projection used for diagnostics\n",
    "        self.proj = nn.Linear(1280, 512).to(self.device)\n",
    "\n",
    "    def train_one_epoch(self):\n",
    "\n",
    "        self.model.train()\n",
    "        self.model.seq_encoder.train()\n",
    "\n",
    "        running_loss = 0\n",
    "\n",
    "        for batch in tqdm(self.training_loader, total=len(self.training_loader), desc=\"Running through epoch\"):\n",
    "\n",
    "            sequences, struct_embed, labels = batch\n",
    "            struct_embed = struct_embed.to(self.device)\n",
    "            labels = labels.to(self.device)\n",
    "\n",
    "            self.optimizer.zero_grad()\n",
    "            loss = self.model.training_step((sequences, struct_embed, labels), self.device)\n",
    "\n",
    "            loss.backward()\n",
    "            self.optimizer.step()\n",
    "\n",
    "            running_loss += loss.item()\n",
    "\n",
    "        return running_loss / len(self.training_loader)\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def compute_embeddings_cos_similariy(self):\n",
    "\n",
    "        self.model.eval()\n",
    "        self.model.seq_encoder.eval()\n",
    "\n",
    "        all_embeds = []\n",
    "        cosine_similarities = []\n",
    "\n",
    "        for batch in tqdm(self.testing_loader, desc=\"Computing cosine similarity & embeddings\"):\n",
    "\n",
    "            seqs, struct_embed , __ = batch\n",
    "            seq_embed = self.model.seq_encoder(seqs)\n",
    "            seq_embed_proj = self.model.seq_down(seq_embed)\n",
    "            \n",
    "            struct_embed = struct_embed.to(self.device)\n",
    "            struct_mask = (struct_embed != -5000).any(dim=-1)\n",
    "            \n",
    "            for i in range(struct_mask.shape[0]):\n",
    "                true_len = struct_mask[i].sum()\n",
    "                seq_i = seq_embed_proj[i, :true_len, :]            # [L_true, D]\n",
    "                struct_i = struct_embed[i, :true_len, :]        # [L_true, D]\n",
    "                \n",
    "                cos = F.cosine_similarity(seq_i, struct_i, dim=-1)   # [L_true]\n",
    "                cosine_similarities.append(cos.mean().item())\n",
    "    \n",
    "                # pool seq embedding\n",
    "                pooled_embed = seq_i.mean(dim=0).cpu()\n",
    "                all_embeds.append(pooled_embed)\n",
    "    \n",
    "        all_embeds = torch.stack(all_embeds)\n",
    "        avg_cos = np.mean(cosine_similarities)\n",
    "        std_cos = np.std(cosine_similarities)\n",
    "    \n",
    "        return all_embeds, cosine_similarities, float(avg_cos), float(std_cos)\n",
    "\n",
    "    def plot_embeddings_drift_cos_similarity_change(self, start_embeddings, end_embeddings, cosine_similarities):\n",
    "\n",
    "        drift = (end_embeddings - start_embeddings).norm(dim=1).cpu().numpy()\n",
    "        cosine_similarities = np.array(cosine_similarities)\n",
    "\n",
    "        fig, ax = plt.subplots(1, 2, figsize=(8, 4))\n",
    "\n",
    "        # Drift subplot\n",
    "        ax[0].hist(drift, bins=30, color=\"steelblue\", alpha=0.8)\n",
    "        ax[0].set_title(\"Embedding Drift per Sequence\", fontsize=8)\n",
    "        ax[0].set_xlabel(\"L2 Norm Drift\", fontsize=8)\n",
    "        ax[0].set_ylabel(\"Density\", fontsize=8)\n",
    "\n",
    "        # Cosine similarity subplot\n",
    "        ax[1].hist(cosine_similarities, bins=40, color=\"darkorange\", alpha=0.7, density=True)\n",
    "        ax[1].set_title(\"Cosine Similarities (ESM-2 vs ESM-IF)\", fontsize=8)\n",
    "        ax[1].set_xlabel(\"Cosine Similarity\", fontsize=8)\n",
    "        ax[1].set_ylabel(\"Density\", fontsize=8)\n",
    "\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "    def train_model(self):\n",
    "\n",
    "        print(\"\\nTrainable parameters inside seq_encoder (LoRA layers):\")\n",
    "        for name, p in self.model.seq_encoder.named_parameters():\n",
    "            if p.requires_grad:\n",
    "                print(\"  \", name)\n",
    "\n",
    "        print(\"\\nExtracting START sequence embeddings & cosine similarities...\")\n",
    "        start_embeddings, cosine_similarities, avg_cosine, std_cosine = self.compute_embeddings_cos_similariy()\n",
    "        print(f\"Average cos-similarity: {avg_cosine}, standard deviation: {std_cosine}\")\n",
    "        self.plot_embeddings_drift_cos_similarity_change(start_embeddings, start_embeddings, cosine_similarities)\n",
    "\n",
    "        for epoch in range(1, self.EPOCHS + 1):\n",
    "            train_loss = self.train_one_epoch()\n",
    "            print(f\"Epoch {epoch}: loss={train_loss:.4f}\")\n",
    "\n",
    "            embeddings, cosine_similarities, avg_cosine, std_cosine = self.compute_embeddings_cos_similariy()\n",
    "            print(f\"Average cos-similarity: {round(avg_cosine, 4)}, standard deviation: {round(std_cosine, 4)}\")\n",
    "            self.plot_embeddings_drift_cos_similarity_change(start_embeddings, embeddings, cosine_similarities)\n",
    "\n",
    "        print(\"\\nExtracting END sequence embeddings...\")\n",
    "        end_embeddings, end_cosine_similarities, avg_cosine, std_cosine = self.compute_embeddings_cos_similariy()\n",
    "\n",
    "        return start_embeddings, end_embeddings, end_cosine_similarities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "befa4e81-fa97-498e-95b2-f2c1fdf35b30",
   "metadata": {},
   "outputs": [],
   "source": [
    "# accelerator\n",
    "model, optimizer, train_dataloader = accelerator.prepare(model, optimizer, train_dataloader)\n",
    "\n",
    "training_wrapper = TrainWrapper(\n",
    "            model=model,\n",
    "            train_loader=train_dataloader,\n",
    "            optimizer=optimizer,\n",
    "            epochs=EPOCHS,\n",
    "            device=device,\n",
    ")\n",
    "torch.cuda.empty_cache()\n",
    "start_embeddings, end_embeddings, end_cosine_similarities = training_wrapper.train_model() # start training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bab5836e-35ce-4d28-a180-8840012b06ba",
   "metadata": {},
   "source": [
    "### CSSPBoostingESM model - 2 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06369fa4-e3b7-45aa-a38e-5c3e4471d9eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ESM2EncoderLoRA(nn.Module):\n",
    "    def __init__(self, padding_value=-5000.0):\n",
    "        super().__init__()\n",
    "\n",
    "        self.padding_value = padding_value\n",
    "\n",
    "        self.model = EsmModel.from_pretrained(\n",
    "            \"facebook/esm2_t33_650M_UR50D\",\n",
    "            output_hidden_states=True\n",
    "        )\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(\"facebook/esm2_t33_650M_UR50D\")\n",
    "\n",
    "        # Freeze original weights\n",
    "        for p in self.model.parameters():\n",
    "            p.requires_grad = False\n",
    "\n",
    "        # LoRA on top layers\n",
    "        lora_cfg = LoraConfig(\n",
    "            task_type=\"FEATURE_EXTRACTION\",\n",
    "            inference_mode=False,\n",
    "            r=4,\n",
    "            lora_alpha=1,\n",
    "            lora_dropout=0.1,\n",
    "            bias=\"none\",\n",
    "            # target_modules=[\"query\", \"key\", \"value\", \"dense\"],\n",
    "            target_modules=[\"query\", \"key\", \"value\", \"dense\"],\n",
    "            layers_to_transform=list(range(25, 33)),\n",
    "        )\n",
    "\n",
    "        self.model = get_peft_model(self.model, lora_cfg)\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def get_attentions(self, sequences):\n",
    "        inputs = self.tokenizer(\n",
    "            sequences, return_tensors=\"pt\", padding=True\n",
    "        ).to(self.model.device)\n",
    "\n",
    "        out = self.model(**inputs, output_attentions=True)\n",
    "        return out.attentions   # list[num_layers] → [B, num_heads, L, L]\n",
    "\n",
    "    def forward(self, sequences, glb_max_len):\n",
    "        inputs = self.tokenizer(\n",
    "            sequences, return_tensors=\"pt\", padding=True\n",
    "        ).to(self.model.device)\n",
    "\n",
    "        out = self.model(**inputs)\n",
    "        reps = out.hidden_states[-1]                  # [B, Ltok, 1280]\n",
    "        reps = reps[:, 1:-1, :]                       # remove CLS/EOS\n",
    "\n",
    "        seq_lengths = [len(s) for s in sequences]\n",
    "        Lmax = glb_max_len\n",
    "\n",
    "        B, D = reps.size(0), reps.size(-1)\n",
    "        padded = torch.full((B, Lmax, D), self.padding_value, device=reps.device)\n",
    "\n",
    "        for i, (r, real_len) in enumerate(zip(reps, seq_lengths)):\n",
    "            padded[i, :real_len] = r[:real_len]\n",
    "\n",
    "        return padded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3c335d9-6a21-4893-9216-be5178195e54",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CSSPBoostingESM(nn.Module):\n",
    "    def __init__(self, seq_embed_dim=1280, struct_embed_dim=512, padding_value=-5000):\n",
    "        super().__init__()\n",
    "        self.padding_value = padding_value\n",
    "        self.seq_encoder = ESM2EncoderLoRA()\n",
    "        self.seq_down = nn.Linear(seq_embed_dim, struct_embed_dim)\n",
    "        self.logit_scale = nn.Parameter(torch.tensor(math.log(1/0.07)))\n",
    "\n",
    "    def forward(self, sequences, struct_embed, max_len: int):\n",
    "        seq_embed = self.seq_encoder(sequences, max_len)\n",
    "        seq_embed_proj = self.seq_down(seq_embed)\n",
    "\n",
    "        seq_mask    = create_key_padding_mask(seq_embed_proj, self.padding_value)\n",
    "        struct_mask = create_key_padding_mask(struct_embed,    self.padding_value)\n",
    "\n",
    "        seq_pooled    = create_mean_of_non_masked(seq_embed_proj, seq_mask)\n",
    "        struct_pooled = create_mean_of_non_masked(struct_embed,    struct_mask)\n",
    "\n",
    "        seq_full = F.normalize(seq_pooled, dim=-1)\n",
    "        struct_full = F.normalize(struct_pooled, dim=-1)\n",
    "\n",
    "        scale = torch.exp(self.logit_scale).clamp(max=100)\n",
    "        logits_seq    = scale * seq_full @ struct_full.T\n",
    "        logits_struct = scale * struct_full @ seq_full.T\n",
    "\n",
    "        return logits_seq, logits_struct, seq_embed_proj, struct_embed, struct_mask\n",
    "\n",
    "    def training_step(self, batch, device, max_len: int):\n",
    "        sequences, struct_embed, _ = batch\n",
    "        struct_embed = struct_embed.to(device)\n",
    "\n",
    "        logits_seq, logits_struct, seq_tok, struct_tok, struct_mask = self.forward(\n",
    "            sequences, struct_embed, max_len=max_len\n",
    "        )\n",
    "\n",
    "        B = logits_seq.shape[0]\n",
    "        labels = torch.arange(B, device=device)\n",
    "\n",
    "        loss_seq    = F.cross_entropy(logits_seq, labels)\n",
    "        loss_struct = F.cross_entropy(logits_struct, labels)\n",
    "        clip_loss = (loss_seq + loss_struct) / 2\n",
    "\n",
    "        mask = (~struct_mask).unsqueeze(-1)  # [B, L, 1]\n",
    "        cos = F.cosine_similarity(seq_tok * mask, struct_tok * mask, dim=-1)\n",
    "        per_token_loss = 1 - (cos.sum(dim=1) / mask.sum(dim=1).squeeze(-1)).mean()\n",
    "\n",
    "        return clip_loss + 0.1 * per_token_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9005079d-70e8-46aa-884e-eab4a0c03b69",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = CSSPBoostingESM(\n",
    "    seq_embed_dim=1280,\n",
    "    struct_embed_dim=512,\n",
    "    padding_value=-5000,\n",
    ").to(device)\n",
    "\n",
    "# model\n",
    "\n",
    "runID = uuid.uuid4()\n",
    "learning_rate = 2e-5\n",
    "EPOCHS = 10\n",
    "# batch_size = 3\n",
    "model = CSSPBoostingESM(seq_embed_dim=1280, struct_embed_dim=512).to(\"cuda\")\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "optimizer = AdamW(model.parameters(), lr=learning_rate)\n",
    "accelerator = Accelerator()\n",
    "device = accelerator.device\n",
    "\n",
    "train_dataloader = DataLoader(train_Dataset, batch_size=10, shuffle=True)\n",
    "test_dataloader = DataLoader(test_Dataset, batch_size=10, shuffle=False)\n",
    "val_dataloader = DataLoader(meta_Dataset, batch_size=10, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39fed74b-8038-4dc0-b9bb-2743942d8c1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TrainWrapper():\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        len_test_max = 500,\n",
    "        len_train_max = 500,\n",
    "        len_val_max = 500,\n",
    "        model=model,\n",
    "        train_loader=train_dataloader,\n",
    "        test_loader=test_dataloader,\n",
    "        val_loader=val_dataloader,\n",
    "        optimizer=optimizer,\n",
    "        epochs=EPOCHS,\n",
    "        device=device\n",
    "    ):\n",
    "        self.model = model\n",
    "        self.training_loader = train_loader\n",
    "        self.testing_loader = test_loader\n",
    "        self.validation_loader = val_loader\n",
    "        self.EPOCHS = epochs\n",
    "        self.optimizer = optimizer\n",
    "        self.device = device\n",
    "        self.max_len_test = len_test_max\n",
    "        self.max_len_train = len_train_max\n",
    "        self.max_len_val = len_val_max\n",
    "\n",
    "        # projection used for diagnostics\n",
    "        self.proj = nn.Linear(1280, 512).to(self.device)\n",
    "\n",
    "    def train_one_epoch(self):\n",
    "\n",
    "        self.model.train()\n",
    "        self.model.seq_encoder.train()\n",
    "\n",
    "        running_loss = 0\n",
    "\n",
    "        for batch in tqdm(self.training_loader, total=len(self.training_loader), desc=\"Running through epoch\"):\n",
    "\n",
    "            sequences, struct_embed, labels = batch\n",
    "            struct_embed = struct_embed.to(self.device)\n",
    "            labels = labels.to(self.device)\n",
    "\n",
    "            self.optimizer.zero_grad()\n",
    "            loss = self.model.training_step((sequences, struct_embed, labels), self.device, self.max_len_train)\n",
    "\n",
    "            loss.backward()\n",
    "            self.optimizer.step()\n",
    "\n",
    "            running_loss += loss.item()\n",
    "\n",
    "        return running_loss / len(self.training_loader)\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def compute_embeddings_cos_similariy(self, Data_loader, max_len):\n",
    "\n",
    "        self.model.eval()\n",
    "        self.model.seq_encoder.eval()\n",
    "\n",
    "        all_embeds = []\n",
    "        cosine_similarities = []\n",
    "\n",
    "        for batch in tqdm(Data_loader, desc=\"Computing cosine similarity & embeddings\"):\n",
    "\n",
    "            seqs, struct_embed , __ = batch\n",
    "            seq_embed = self.model.seq_encoder(seqs, max_len)\n",
    "            seq_embed_proj = self.model.seq_down(seq_embed)\n",
    "            \n",
    "            struct_embed = struct_embed.to(self.device)\n",
    "            struct_mask = (struct_embed != -5000).any(dim=-1)\n",
    "            \n",
    "            for i in range(struct_mask.shape[0]):\n",
    "                true_len = struct_mask[i].sum()\n",
    "                seq_i = seq_embed_proj[i, :true_len, :]            # [L_true, D]\n",
    "                struct_i = struct_embed[i, :true_len, :]        # [L_true, D]\n",
    "                \n",
    "                cos = F.cosine_similarity(seq_i, struct_i, dim=-1)   # [L_true]\n",
    "                cosine_similarities.append(cos.mean().item())\n",
    "    \n",
    "                # pool seq embedding\n",
    "                pooled_embed = seq_i.mean(dim=0).cpu()\n",
    "                all_embeds.append(pooled_embed)\n",
    "    \n",
    "        all_embeds = torch.stack(all_embeds)\n",
    "        avg_cos = np.mean(cosine_similarities)\n",
    "        std_cos = np.std(cosine_similarities)\n",
    "    \n",
    "        return all_embeds, cosine_similarities, float(avg_cos), float(std_cos)\n",
    "\n",
    "    def plot_embeddings_drift_cos_similarity_change(self, start_embeddings, end_embeddings, cosine_similarities):\n",
    "\n",
    "        drift = (end_embeddings - start_embeddings).norm(dim=1).cpu().numpy()\n",
    "        cosine_similarities = np.array(cosine_similarities)\n",
    "\n",
    "        fig, ax = plt.subplots(1, 2, figsize=(8, 4))\n",
    "\n",
    "        # Drift subplot\n",
    "        ax[0].hist(drift, bins=30, color=\"steelblue\", alpha=0.8)\n",
    "        ax[0].set_title(\"Embedding Drift per Sequence\", fontsize=8)\n",
    "        ax[0].set_xlabel(\"L2 Norm Drift\", fontsize=8)\n",
    "        ax[0].set_ylabel(\"Density\", fontsize=8)\n",
    "\n",
    "        # Cosine similarity subplot\n",
    "        ax[1].hist(cosine_similarities, bins=40, color=\"darkorange\", alpha=0.7, density=True)\n",
    "        ax[1].set_title(\"Cosine Similarities (ESM-2 vs ESM-IF)\", fontsize=8)\n",
    "        ax[1].set_xlabel(\"Cosine Similarity\", fontsize=8)\n",
    "        ax[1].set_ylabel(\"Density\", fontsize=8)\n",
    "\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "    def train_model(self, save_every: int = 5):\n",
    "\n",
    "        # Base directory for this run\n",
    "        run_dir = f\"/work3/s232958/data/trained/boostingESM2wESMIF/PPint/{runID}/\"\n",
    "        os.makedirs(run_dir, exist_ok=True)\n",
    "\n",
    "        print(\"\\nTrainable parameters inside seq_encoder (LoRA layers):\")\n",
    "        for name, p in self.model.seq_encoder.named_parameters():\n",
    "            if p.requires_grad:\n",
    "                print(\"  \", name)\n",
    "\n",
    "        # ---- Save checkpoint BEFORE training (epoch 0) ----\n",
    "        save_path = os.path.join(run_dir, \"ESM2boosted_epoch_0.pt\")\n",
    "        torch.save(self.model.seq_encoder.state_dict(), save_path)\n",
    "        print(f\"Saved seq_encoder checkpoint before training -> {save_path}\")\n",
    "\n",
    "        print(\"\\nExtracting START sequence embeddings & cosine similarities...\")\n",
    "        start_embeddings, cosine_similarities, avg_cosine, std_cosine = self.compute_embeddings_cos_similariy(self.testing_loader, self.max_len_test)\n",
    "        print(f\"Average cos-similarity Test-set: {avg_cosine}, standard deviation: {std_cosine}\")\n",
    "        self.plot_embeddings_drift_cos_similarity_change(start_embeddings, start_embeddings, cosine_similarities)\n",
    "\n",
    "        start_embeddings, cosine_similarities, avg_cosine, std_cosine = self.compute_embeddings_cos_similariy(self.validation_loader, self.max_val_test)\n",
    "        print(f\"Average cos-similarity Val-set: {avg_cosine}, standard deviation: {std_cosine}\")\n",
    "        self.plot_embeddings_drift_cos_similarity_change(start_embeddings, start_embeddings, cosine_similarities)\n",
    "\n",
    "        for epoch in range(1, self.EPOCHS + 1):\n",
    "            train_loss = self.train_one_epoch()\n",
    "            print(f\"Epoch {epoch}: loss={train_loss:.4f}\")\n",
    "\n",
    "            embeddings, cosine_similarities, avg_cosine, std_cosine = self.compute_embeddings_cos_similariy(self.testing_loader, self.max_len_test)\n",
    "            print(f\"Average cos-similarity Test-set: {avg_cosine}, standard deviation: {std_cosine}\")\n",
    "            self.plot_embeddings_drift_cos_similarity_change(start_embeddings, embeddings, cosine_similarities)\n",
    "    \n",
    "            embeddings, cosine_similarities, avg_cosine, std_cosine = self.compute_embeddings_cos_similariy(self.val_dataloader, self.max_val_test)\n",
    "            print(f\"Average cos-similarity Val-set: {avg_cosine}, standard deviation: {std_cosine}\")\n",
    "            self.plot_embeddings_drift_cos_similarity_change(start_embeddings, embeddings, cosine_similarities)\n",
    "\n",
    "            # ---- Save every `save_every` epochs ----\n",
    "            if epoch % save_every == 0:\n",
    "                save_path = os.path.join(run_dir, f\"ESM2boosted_epoch_{epoch}.pt\")\n",
    "                torch.save(self.model.seq_encoder.state_dict(), save_path)\n",
    "                print(f\"Saved seq_encoder checkpoint at epoch {epoch} -> {save_path}\")\n",
    "\n",
    "        print(\"\\nExtracting END sequence embeddings...\")\n",
    "        end_embeddings, end_cosine_similarities, avg_cosine, std_cosine = self.compute_embeddings_cos_similariy(self.testing_loader, self.max_len_test)\n",
    "        print(f\"Average cos-similarity Test-set: {avg_cosine}, standard deviation: {std_cosine}\")\n",
    "        self.plot_embeddings_drift_cos_similarity_change(start_embeddings, end_embeddings, cosine_similarities)\n",
    "\n",
    "        end_embeddings, end_cosine_similarities, avg_cosine, std_cosine = self.compute_embeddings_cos_similariy(self.val_dataloader, self.max_val_test)\n",
    "        print(f\"Average cos-similarity Val-set: {avg_cosine}, standard deviation: {std_cosine}\")\n",
    "        self.plot_embeddings_drift_cos_similarity_change(start_embeddings, end_embeddings, cosine_similarities)\n",
    "        \n",
    "        return start_embeddings, end_embeddings, end_cosine_similarities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c7a15ae-5e8f-4968-93cf-652026ac9fde",
   "metadata": {},
   "outputs": [],
   "source": [
    "# accelerator\n",
    "model, optimizer, train_dataloader = accelerator.prepare(model, optimizer, train_dataloader)\n",
    "\n",
    "training_wrapper = TrainWrapper(\n",
    "    len_test_max = Df_test_LONG.seq_len.max(),\n",
    "    len_train_max = Df_train_LONG.seq_len.max(),\n",
    "    len_val_max = meta_sample_Df.seq_len.max(),\n",
    "    model=model,\n",
    "    train_loader=train_dataloader,\n",
    "    test_loader=test_dataloader,\n",
    "    val_loader=val_dataloader,\n",
    "    optimizer=optimizer,\n",
    "    epochs=EPOCHS,\n",
    "    device=device,\n",
    ")\n",
    "\n",
    "start_embeddings, end_embeddings, end_cosine_similarities = training_wrapper.train_model(save_every=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8adff4f6-7112-4212-bde7-a681e9816ecd",
   "metadata": {},
   "source": [
    "### Loading model and tetsing on meta-analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b42f4ea-1cf9-4862-9a83-e44061cc9d0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_path = \"/work3/s232958/data/trained/boostingESM2wESMIF/259f46f3-26bc-4d5b-b923-04af96da9fc9/ESM2boosted_epoch_50.pt\"\n",
    "state_dict = torch.load(checkpoint_path, map_location=device)\n",
    "\n",
    "model = ESM2EncoderLoRA()\n",
    "model.load_state_dict(state_dict)\n",
    "model.to(device)\n",
    "model.eval()  # or model.train()\n",
    "# print(\"Loaded seq_encoder weights from:\", checkpoint_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b073a368-6f8b-4bd8-9fa4-73d3e8ac7919",
   "metadata": {},
   "outputs": [],
   "source": [
    "meta_df = pd.read_csv(\"/work3/s232958/data/meta_analysis/interaction_df_metaanal_w_pbd_lens.csv\").drop(columns = [\"binder_id\", \"target_id\"]).rename(columns = {\n",
    "    \"target_id_mod\" : \"target_id\",\n",
    "    \"target_binder_ID\" : \"binder_id\",\n",
    "})\n",
    "\n",
    "# Interaction Dict\n",
    "meta_df_sample = meta_df.sample(frac=0.15, random_state=0).reset_index(drop=True)\n",
    "meta_df_sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee448b31-4641-47aa-ac61-5b93c031dab3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CLIP_PPint_w_esmIF(Dataset):\n",
    "    def __init__(\n",
    "        self,\n",
    "        dframe,\n",
    "        paths,\n",
    "        embedding_dim_struct=512,\n",
    "        embedding_dim_seq=1280,\n",
    "        embedding_pad_value=-5000.0,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.dframe = dframe.copy()\n",
    "        self.embedding_dim_seq = embedding_dim_seq\n",
    "        self.embedding_dim_struct = embedding_dim_struct\n",
    "        self.emb_pad = embedding_pad_value\n",
    "\n",
    "        # lengths\n",
    "        self.max_seq_len = self.dframe[\"seq_len_binder\"].max()\n",
    "        self.max_str_len = self.dframe[\"pdb_len_binder\"].max()\n",
    "\n",
    "        # paths\n",
    "        self.seq_encodings_path, self.struct_encodings_path = paths\n",
    "\n",
    "        # index & storage\n",
    "        self.dframe.set_index(\"binder_id\", inplace=True)\n",
    "        self.accessions = self.dframe.index.astype(str).tolist()\n",
    "        self.name_to_row = {name: i for i, name in enumerate(self.accessions)}\n",
    "        self.samples = []\n",
    "\n",
    "        for accession in tqdm(self.accessions, total=len(self.accessions), desc=\"#Loading ESM2 embeddings and contacts\"):\n",
    "\n",
    "            # laod embeddings\n",
    "            emb_seq = np.load(os.path.join(self.seq_encodings_path, f\"{accession}.npy\"))     # [Lt, D]\n",
    "            emb_struct = np.load(os.path.join(self.struct_encodings_path, f\"{accession}.npy\"))     # [Lb, D]\n",
    "            seq = str(self.dframe.loc[accession].binder_seq)\n",
    "\n",
    "            # quich check whether embedding dimmension is as it suppose to be\n",
    "            if emb_seq.shape[1] != self.embedding_dim_seq:\n",
    "                raise ValueError(\"Embedding dim mismatch with 'embedding_dim_seq'.\")\n",
    "            if emb_struct.shape[1] != self.embedding_dim_struct:\n",
    "                raise ValueError(\"Embedding dim mismatch with 'embedding_dim'.\")\n",
    "                \n",
    "            # add -5000 to all the padded target rows\n",
    "            if emb_seq.shape[0] < self.max_seq_len:\n",
    "                emb_seq = np.concatenate([emb_seq, np.full((self.max_seq_len - emb_seq.shape[0], emb_seq.shape[1]), self.emb_pad, dtype=emb_seq.dtype)], axis=0)\n",
    "            else:\n",
    "                emb_seq = emb_seq[: self.max_seq_len] # no padding was usedd\n",
    "\n",
    "            if emb_struct.shape[0] < self.max_str_len:\n",
    "                emb_struct = np.concatenate([emb_struct, np.full((self.max_str_len - emb_struct.shape[0], emb_struct.shape[1]), self.emb_pad, dtype=emb_struct.dtype)], axis=0)\n",
    "            else:\n",
    "                emb_struct = emb_struct[: self.max_str_len] # no padding was used\n",
    "\n",
    "            self.samples.append((emb_seq, seq, emb_struct))\n",
    "\n",
    "    # ---- Dataset API ----\n",
    "    def __len__(self):\n",
    "        return len(self.samples)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        emb_seq, seq, emb_struct = self.samples[idx]\n",
    "        emb_seq, emb_struct = torch.from_numpy(emb_seq).float(), torch.from_numpy(emb_struct).float()\n",
    "        # label = torch.tensor(1, dtype=torch.float32)  # single scalar labe\n",
    "        return emb_seq, seq, emb_struct\n",
    "\n",
    "    def _get_by_name(self, name):\n",
    "        # Single item -> return exactly what __getitem__ returns\n",
    "        if isinstance(name, str):\n",
    "            return self.__getitem__(self.name_to_row[name])\n",
    "        \n",
    "        # Multiple items -> fetch all\n",
    "        out = [self.__getitem__(self.name_to_row[n]) for n in list(name)]\n",
    "        # emb_seq_list, emb_struct_list, label_list = zip(*out)\n",
    "        emb_seq_list, seqs_list, emb_struct_list = zip(*out)\n",
    "    \n",
    "        # Stack embeddings\n",
    "        emb_seq_stacked  = torch.stack([torch.as_tensor(x) for x in emb_seq_list],  dim=0)  # [B, ...]        \n",
    "        emb_struct_stacked  = torch.stack([torch.as_tensor(x) for x in emb_struct_list],  dim=0)  # [B, ...]\n",
    "    \n",
    "        # Stack labels\n",
    "        # labels = torch.stack(lbl_list)  # [B]\n",
    "    \n",
    "        return emb_seq_stacked, seqs_list, emb_struct_stacked\n",
    "\n",
    "emb_seq_path = \"/work3/s232958/data/meta_analysis/embeddings_esm2_binders\"\n",
    "emb_struct_path = \"/work3/s232958/data/meta_analysis/esmif_embeddings_binders\"\n",
    "\n",
    "meta_Dataset = CLIP_PPint_w_esmIF(\n",
    "    meta_df_sample,\n",
    "    paths=[emb_seq_path, emb_struct_path],\n",
    "    embedding_dim_seq=1280,\n",
    "    embedding_dim_struct=512\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc35dd41-bccb-4234-b3ee-9f3f2aa1ffd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "_loader = DataLoader(meta_Dataset, batch_size=10)\n",
    "proj = nn.Linear(1280, 512).to(device)\n",
    "cosine_similarities_BEFORE_per_token = []\n",
    "\n",
    "for batch in _loader:\n",
    "    seqs_emeb, seqs_list, structs_emeb = batch\n",
    "    seqs_emeb, structs_emeb = seqs_emeb.to(device), structs_emeb.to(device)\n",
    "    \n",
    "    # Project both embeddings if they start at 1280 dim\n",
    "    seq_embed_proj = proj(seqs_emeb)          # shape: [B, L, 512]\n",
    "\n",
    "    struct_mask = (structs_emeb != -5000).any(dim=-1)\n",
    "\n",
    "    for i in range(struct_mask.shape[0]):\n",
    "        true_len = struct_mask[i].sum()\n",
    "        seq_i = seq_embed_proj[i, :true_len, :]            # [L_true, D]\n",
    "        struct_i = structs_emeb[i, :true_len, :]        # [L_true, D]\n",
    "        \n",
    "        cos = F.cosine_similarity(seq_i, struct_i, dim=-1)   # [L_true]\n",
    "        cosine_similarities_BEFORE_per_token.append(cos.mean().item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f44a68aa-fa70-484b-a37e-92d235e10b77",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(6, 4))\n",
    "plt.hist(cosine_similarities_BEFORE_per_token, bins=50, alpha=0.6, label=\"Cosine similarities before\", density=True)\n",
    "# plt.hist(neg_logits, bins=50, alpha=0.6, label=\"Negative pairs\", density=True)\n",
    "\n",
    "plt.xlabel(\"cos-sim\")\n",
    "plt.ylabel(\"Density\")\n",
    "plt.title(\"Cosine similarities ESM-2 and ESM-IF before training!\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa65ad63-f252-4784-aad5-44ff73354363",
   "metadata": {},
   "outputs": [],
   "source": [
    "cosine_similarities_BEFORE_pooled_per_seq = []\n",
    "\n",
    "for batch in _loader:\n",
    "    seqs_emeb, seqs_list, structs_emeb = batch\n",
    "    seqs_emeb, structs_emeb = seqs_emeb.to(device), structs_emeb.to(device)\n",
    "    \n",
    "    # Project both embeddings if they start at 1280 dim\n",
    "    seqs_emeb_proj = proj(seqs_emeb)          # shape: [B, L, 512]\n",
    "\n",
    "    # Create masks (must match projected embeddings)\n",
    "    seq_mask = create_key_padding_mask(seqs_emeb_proj)\n",
    "    struct_mask = create_key_padding_mask(structs_emeb)\n",
    "\n",
    "    # Mean-pool (returns shape: [B, 512])\n",
    "    vectors_seq = create_mean_of_non_masked(seqs_emeb_proj, seq_mask).cpu().detach().numpy()\n",
    "    vectors_struct = create_mean_of_non_masked(structs_emeb, struct_mask).cpu().detach().numpy()\n",
    "\n",
    "    # Loop through batch items\n",
    "    for i in range(vectors_seq.shape[0]):\n",
    "        cosine = np.dot(vectors_seq[i], vectors_struct[i]) / (norm(vectors_seq[i]) * norm(vectors_struct[i]))\n",
    "        cosine_similarities_BEFORE_pooled_per_seq.append(cosine)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f32170a7-396e-405c-86ca-caa3f1b68613",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(6, 4))\n",
    "plt.hist(cosine_similarities_BEFORE_pooled_per_seq, bins=50, alpha=0.6, label=\"Cosine similarities before\", density=True)\n",
    "# plt.hist(neg_logits, bins=50, alpha=0.6, label=\"Negative pairs\", density=True)\n",
    "\n",
    "plt.xlabel(\"cos-sim\")\n",
    "plt.ylabel(\"Density\")\n",
    "plt.title(\"Cosine similarities ESM-2 and ESM-IF before training!\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db4fddc2-408b-4ea9-9871-b4cb849dedf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "cosine_similarities_AFTER_per_token = []\n",
    "\n",
    "for batch in _loader:\n",
    "    ___, seqs_list, structs_emeb = batch\n",
    "    seqs_emeb = model(seqs_list, meta_df_sample.seq_len_binder.max())\n",
    "    structs_emeb, seqs_emeb = structs_emeb.to(device), seqs_emeb.to(device)\n",
    "    \n",
    "    # Project both embeddings if they start at 1280 dim\n",
    "    seq_embed_proj = proj(seqs_emeb)          # shape: [B, L, 512]\n",
    "\n",
    "    struct_mask = (structs_emeb != -5000).any(dim=-1)\n",
    "\n",
    "    for i in range(struct_mask.shape[0]):\n",
    "        true_len = struct_mask[i].sum()\n",
    "        seq_i = seq_embed_proj[i, :true_len, :]            # [L_true, D]\n",
    "        struct_i = structs_emeb[i, :true_len, :]        # [L_true, D]\n",
    "        \n",
    "        cos = F.cosine_similarity(seq_i, struct_i, dim=-1)   # [L_true]\n",
    "        cosine_similarities_AFTER_per_token.append(cos.mean().item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64add886-ca5e-405d-8499-8943d18d42a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(6, 4))\n",
    "plt.hist(cosine_similarities_AFTER_per_token, bins=50, alpha=0.6, label=\"Cosine similarities before\", density=True)\n",
    "# plt.hist(neg_logits, bins=50, alpha=0.6, label=\"Negative pairs\", density=True)\n",
    "\n",
    "plt.xlabel(\"cos-sim\")\n",
    "plt.ylabel(\"Density\")\n",
    "plt.title(\"Cosine similarities ESM-2 and ESM-IF before training!\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92dd7578-a0a9-4f14-8aba-501e551c03f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "cosine_similarities_AFTER_pooled_per_seq = []\n",
    "\n",
    "for batch in _loader:\n",
    "    ___, seqs_list, structs_emeb = batch\n",
    "    \n",
    "    # Project both embeddings if they start at 1280 dim\n",
    "    seqs_emeb = model(seqs_list, meta_df_sample.seq_len_binder.max())\n",
    "    structs_emeb, seqs_emeb = structs_emeb.to(device), seqs_emeb.to(device)\n",
    "    seqs_emeb_proj = proj(seqs_emeb)          # shape: [B, L, 512]\n",
    "\n",
    "    # Create masks (must match projected embeddings)\n",
    "    seq_mask = create_key_padding_mask(seqs_emeb_proj)\n",
    "    struct_mask = create_key_padding_mask(structs_emeb)\n",
    "\n",
    "    # Mean-pool (returns shape: [B, 512])\n",
    "    vectors_seq = create_mean_of_non_masked(seqs_emeb_proj, seq_mask).cpu().detach().numpy()\n",
    "    vectors_struct = create_mean_of_non_masked(structs_emeb, struct_mask).cpu().detach().numpy()\n",
    "\n",
    "    # Loop through batch items\n",
    "    for i in range(vectors_seq.shape[0]):\n",
    "        cosine = np.dot(vectors_seq[i], vectors_struct[i]) / (norm(vectors_seq[i]) * norm(vectors_struct[i]))\n",
    "        cosine_similarities_AFTER_pooled_per_seq.append(cosine)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "225534f6-e078-48b6-90c8-88ceacfe1e0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(6, 4))\n",
    "plt.hist(cosine_similarities_AFTER_pooled_per_seq, bins=50, alpha=0.6, label=\"Cosine similarities before\", density=True)\n",
    "# plt.hist(neg_logits, bins=50, alpha=0.6, label=\"Negative pairs\", density=True)\n",
    "\n",
    "plt.xlabel(\"cos-sim\")\n",
    "plt.ylabel(\"Density\")\n",
    "plt.title(\"Cosine similarities ESM-2 and ESM-IF AFTER training!\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1600646b-2769-4b9e-b49a-5eba19972d9b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ESM CUDA Env",
   "language": "python",
   "name": "esm_cuda"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
